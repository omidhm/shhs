{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9213f60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bpradsad\\AppData\\Local\\Temp\\ipykernel_28016\\1475073033.py:5: DtypeWarning: Columns (1263,1347) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('C:/Users/Bpradsad/Desktop/prof cetin/prof beharati/shhs1- 2.csv')\n",
      "C:\\Users\\Bpradsad\\AppData\\Local\\Temp\\ipykernel_28016\\1475073033.py:5: DtypeWarning: Columns (1263,1347) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('C:/Users/Bpradsad/Desktop/prof cetin/prof beharati/shhs1- 2.csv')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def Considered_AHI_restriction_2(LOW_AHI, High_AHI):\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_csv('C:/Users/Bpradsad/Desktop/prof cetin/prof beharati/shhs1- 2.csv')\n",
    "\n",
    "#     AHI_base = AHI\n",
    "    AHI_base = LOW_AHI\n",
    "#     filtered_df = df[(df[\"ahi_a0h3a\"] >= AHI_base) & (df[\"htnderv_s1\"] == 0) & (df[\"htnderv_s2\"].notnull())]\n",
    "    \n",
    "    filtered_df = df[((df[\"ahi_a0h3a\"] < LOW_AHI) | (df[\"ahi_a0h3a\"] > High_AHI)) & (df[\"htnderv_s1\"] == 0) & (df[\"htnderv_s2\"].notnull())]\n",
    "    selected_basedonAHI = [str(x[0]) for x in filtered_df[[\"nsrrid\"]].values]\n",
    "    AHI_considered = []\n",
    "    for patient in file_nn:\n",
    "        if patient[6:-7] in selected_basedonAHI:\n",
    "            AHI_considered.append(1)\n",
    "        else:\n",
    "            AHI_considered.append(0)\n",
    "\n",
    "    AHI_considered = np.array(AHI_considered)\n",
    "#     print(\"percentage of accepted data\", sum(AHI_considered)/len(AHI_considered))\n",
    "    Consider = [np.array(AHI_considered)==1][0]\n",
    "\n",
    "    return(Consider)\n",
    "\n",
    "file_nn = np.load(\"C:/Users/Bpradsad/file_nn.npy\")\n",
    "label_nn = np.load(\"C:/Users/Bpradsad/label_nn.npy\")\n",
    "len(file_nn[Considered_AHI_restriction_2(6, 21)])\n",
    "selected_data = [name[:12]+\".npy\" for name in file_nn[Considered_AHI_restriction_2(6, 21)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4afd3a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939ec93c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f7a767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dic = {}\n",
    "for idx, name in enumerate(file_nn):\n",
    "    label_dic[name[:12]+\".npy\"] = int(label_nn[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1be423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3f9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.signal import butter, filtfilt\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "\n",
    "# def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "#     nyq = 0.5 * fs\n",
    "#     low = lowcut / nyq\n",
    "#     high = highcut / nyq\n",
    "#     b, a = butter(order, [low, high], btype='band')\n",
    "#     return b, a\n",
    "\n",
    "# def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "#     b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "#     y = filtfilt(b, a, data)\n",
    "#     return y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def mean_removel_linear_detrended(signal, fs, lowcut, highcut):\n",
    "    \n",
    "#     from scipy.signal import butter, filtfilt\n",
    "#     import numpy as np\n",
    "#     import matplotlib.pyplot as plt\n",
    "\n",
    "#     # Baseline Correction: Mean Removal\n",
    "#     mean_removed_signal = signal - np.mean(signal)\n",
    "\n",
    "#     time = np.linspace(0, 1, len(signal))\n",
    "    \n",
    "#     # Baseline Correction: Linear Detrending\n",
    "#     trend = np.polyfit(time, mean_removed_signal, 1)\n",
    "#     linear_detrended_signal = mean_removed_signal - (trend[0] * time + trend[1])\n",
    "\n",
    "#     return linear_detrended_signal\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Define the list of folder names\n",
    "# foldernames = [\"Signal_ABDO\", \"Signal_NEWAIR\", \"Signal_H\", \"Signal_ECG\", \"Signal_EOG_L\", \"Signal_EMG\", \"Signal_POSITION\", \"Signal_EEG\", \"Signal_EOG_R\", \"Signal_THOR\", \"Signal_SaO2_OX\"]\n",
    "# fs = [10, 10, 1, 125, 50, 125, 1, 125, 50, 10, 1]\n",
    "\n",
    "# # foldernames = [\"Signal_EEG\", \"Signal_EOG_R\", \"Signal_THOR\", \"Signal_SaO2_OX\"]\n",
    "# # fs = [125, 50, 10, 1]\n",
    "\n",
    "# # Define the root directory where the folders are located\n",
    "# root_dir = 'D:/Directory_D_Desk_jup/Raw_npy/'  # Replace with your actual root directory\n",
    "\n",
    "# # Define the new directory where you want to copy the selected numpy arrays\n",
    "# new_root_dir = 'C:/Users/Bpradsad/Desktop/tempp/tmp/'  # Replace with your desired new root directory\n",
    "\n",
    "# # Define the list of selected data (file names)\n",
    "# selected_data = selected_data\n",
    "\n",
    "# # Iterate through each folder\n",
    "# for fs_idx, folder in enumerate(foldernames):\n",
    "#     folder_path = os.path.join(root_dir, folder)\n",
    "    \n",
    "#     # Check if the path is a directory\n",
    "#     if os.path.isdir(folder_path):\n",
    "#         # Create corresponding folder in the new directory\n",
    "#         new_folder_path = os.path.join(new_root_dir, folder)\n",
    "#         os.makedirs(new_folder_path, exist_ok=True)\n",
    "        \n",
    "#         # Get a list of all numpy files in the folder\n",
    "#         numpy_files = [f for f in os.listdir(folder_path) if f.endswith('.npy')]\n",
    "        \n",
    "#         # Iterate through the selected arrays\n",
    "#         for selected_array in [\"ds\"]:\n",
    "# #         for selected_array in selected_data:\n",
    "#             if selected_array in numpy_files:\n",
    "#                 source_path = os.path.join(folder_path, selected_array)\n",
    "#                 dest_path = os.path.join(new_folder_path, selected_array)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "#                 # Load the numpy array\n",
    "#                 signal = np.load(source_path)\n",
    "#                 # Define the cutoff frequencies and order of the filter\n",
    "#                 lowcut = 0.5\n",
    "#                 highcut = 30\n",
    "#                 order = 6\n",
    "                \n",
    "#                 if fs[fs_idx] == 1:\n",
    "#                     lowcut = 0.1\n",
    "#                     highcut = 0.4\n",
    "#                 else:\n",
    "#                     lowcut = 0.1\n",
    "#                     highcut = np.min([fs[fs_idx]//4, 30])\n",
    "#                 linear_detrended_signal = mean_removel_linear_detrended(signal, fs[fs_idx], lowcut, highcut)\n",
    "#                 filtered_signal = butter_bandpass_filter(linear_detrended_signal, lowcut, highcut, fs[fs_idx], order=order)\n",
    "                \n",
    "#                 # Save the filtered signal\n",
    "#                 np.save(dest_path, filtered_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46415354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958cc317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d337391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, folder_name, label_n):\n",
    "        self.data_dir = data_dir\n",
    "        self.folder_name = folder_name\n",
    "        self.label_n = label_n\n",
    "        self.file_paths = []\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "        folder_path = os.path.join(data_dir, folder_name)\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            single_data = np.load(file_path, allow_pickle=True).astype('float32')\n",
    "            self.file_paths.append((file_name, single_data))  # Store as tuple\n",
    "\n",
    "        self.calculate_mean_std()\n",
    "\n",
    "    def calculate_mean_std(self):\n",
    "        all_data = np.concatenate([data for _, data in self.file_paths])\n",
    "        self.mean = np.mean(all_data)\n",
    "        self.std = np.std(all_data)\n",
    "        self.max = np.max(all_data)\n",
    "\n",
    "    def normalize_data(self, data):\n",
    "        return data / (self.max + 0.01)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name, data = self.file_paths[idx]\n",
    "        data_lbl = self.label_n[file_name]  # Get label using file name\n",
    "        data = self.normalize_data(data)\n",
    "\n",
    "        return data, data_lbl\n",
    "\n",
    "\n",
    "###### not working #############################\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(dataset) for dataset in self.datasets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = [dataset[idx % len(dataset)] for dataset in self.datasets]\n",
    "        return tuple(data)\n",
    "  \n",
    "    \n",
    "\n",
    "foldernames = [\"Signal_ABDO\", \"Signal_NEWAIR\", \"Signal_H\", \"Signal_ECG\", \"Signal_EOG_L\", \"Signal_EMG\", \"Signal_POSITION\", \"Signal_EEG\", \"Signal_EOG_R\", \"Signal_THOR\", \"Signal_SaO2_OX\"]\n",
    "fs = [10, 10, 1, 125, 50, 125, 1, 125, 50, 10, 1]\n",
    "\n",
    "\n",
    "label_n = label_dic\n",
    "\n",
    "datasets = []\n",
    "\n",
    "for folder_name in foldernames:\n",
    "    \n",
    "    dataset = CustomDataset(\"C:/Users/Bpradsad/Desktop/tempp/Filtered/\", folder_name, label_n)\n",
    "    \n",
    "    datasets.append(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a289d758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c17661a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CNN_Block(nn.Module):\n",
    "    def __init__(self, input_channels, kernel_sizes):\n",
    "        super(CNN_Block, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        for kernel_size in kernel_sizes:\n",
    "            conv_layer = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=input_channels, out_channels=input_channels, kernel_size=kernel_size, stride=1, padding=1),\n",
    "                nn.BatchNorm1d(input_channels),  # Add BatchNorm1d\n",
    "#                 nn.ReLU(),\n",
    "                nn.AdaptiveMaxPool1d(output_size=64*64)\n",
    "            )\n",
    "            self.conv_layers.append(conv_layer)\n",
    "        \n",
    "#         self.adaptive_pool = nn.AdaptiveMaxPool1d(output_size=64*64)\n",
    "#         self.grelu = nn.ELU()\n",
    "        \n",
    "        self.converter = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=11, out_channels=3, kernel_size=3, dilation=2),\n",
    "            nn.BatchNorm2d(3),  \n",
    "#             nn.ReLU(),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.effnet = EfficientNet.from_pretrained('efficientnet-b0')  # Assuming 1000 classes\n",
    "        \n",
    "        # Add a classifier\n",
    "        self.effnet._fc = nn.Sequential(\n",
    "            nn.Linear(1280, 1),  # Assuming 1280 is the output size of the EfficientNet-B0\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        for param in self.effnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.effnet._fc.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        output = []\n",
    "        \n",
    "        for i, data_and_lbl in enumerate(inputs):\n",
    "            \n",
    "#             print(\"--------------------------------\", i)\n",
    "            x = data_and_lbl[0].unsqueeze(1).float().to('cuda')\n",
    "            \n",
    "            # Flatten the tensor to 1D\n",
    "#             x_tmp = x.view(-1).cpu().numpy()\n",
    "            \n",
    "\n",
    "#             # Plot the histogram\n",
    "#             plt.hist(x_tmp, bins=100, density=True, alpha=0.75, color='b')\n",
    "#             plt.xlabel('Value')\n",
    "#             plt.ylabel('Frequency')\n",
    "#             plt.title('Distribution of x')\n",
    "#             plt.show()\n",
    "            \n",
    "\n",
    "#             # Find maximum, minimum, and NaN values\n",
    "#             max_value = torch.max(x).item()\n",
    "#             min_value = torch.min(x).item()\n",
    "\n",
    "#             # Check for NaN values\n",
    "#             has_nan = torch.isnan(x).any().item()\n",
    "\n",
    "#             print(f\"before Maximum value: {max_value}\")\n",
    "#             print(f\"before Minimum value: {min_value}\")\n",
    "#             print(f\"before Contains NaN: {has_nan}\")\n",
    "            ####################\n",
    "            \n",
    "#             print(\"x.shape\", x.shape)\n",
    "            \n",
    "            x = self.conv_layers[i](x)\n",
    "        \n",
    "        \n",
    "            # Assuming x is a CUDA tensor\n",
    "            x_tmp = x.view(-1)\n",
    "            x_tmp = x_tmp.masked_fill(x_tmp == -float('inf'), 0)\n",
    "            x_tmp = x_tmp.masked_fill(torch.isnan(x_tmp), 0)\n",
    "            x = x_tmp.view(x.shape)\n",
    "            \n",
    "# # #             print(\"x.shape\", x.shape)\n",
    "# #             # Flatten the tensor to 1D\n",
    "#             x_tmp = x.view(-1).cpu().detach().numpy()\n",
    "            \n",
    "# #             x_tmp[x_tmp == -float('inf')] = -10\n",
    "#             # Plot the histogram\n",
    "#             plt.hist(x_tmp, bins=100, density=True, alpha=0.75, color='b')\n",
    "#             plt.xlabel('Value')\n",
    "#             plt.ylabel('Frequency')\n",
    "#             plt.title('Distribution of x')\n",
    "#             plt.show()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             # Find maximum, minimum, and NaN values\n",
    "#             max_value = torch.max(x).item()\n",
    "#             min_value = torch.min(x).item()\n",
    "\n",
    "#             # Check for NaN values\n",
    "#             has_nan = torch.isnan(x).any().item()\n",
    "\n",
    "#             print(f\"after Maximum value: {max_value}\")\n",
    "#             print(f\"after Minimum value: {min_value}\")\n",
    "#             print(f\"after Contains NaN: {has_nan}\")\n",
    "            \n",
    "            if torch.isnan(x).any():\n",
    "                print(f'XXXXXXXXXXXXXXXXXXXXXXX values {i} output contains NaN values')\n",
    "#             x = self.adaptive_pool(x)\n",
    "\n",
    "\n",
    "            x = x.view(-1, 1, 64, 64)\n",
    "            output.append(x)\n",
    "            \n",
    "#         print(output)\n",
    "#         print(len(output))\n",
    "\n",
    "        \n",
    "        combined_x = torch.cat(output, dim=1)\n",
    "                \n",
    "        if torch.isnan(combined_x).any():\n",
    "            print(f'Conv{i} output contains NaN values')\n",
    "            \n",
    "#         print(\"combined_x.shape\", combined_x.shape)\n",
    "        # Convert and process combined_x\n",
    "        x_out = self.converter(combined_x)\n",
    "#         print(x_out)\n",
    "      \n",
    "        # EfficientNet and final tuning\n",
    "        x = self.effnet(x_out)\n",
    "#         out = self.final_tuning(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "input_channels = 1\n",
    "# kernel_size = np.array(fs)+1   \n",
    "kernel_size = [5, 5, 3, 7, 7, 7, 3, 7, 7, 5, 3]\n",
    "# kernel_size = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
    "model = CNN_Block(input_channels, kernel_size)\n",
    "# torch.save(model.state_dict(), 'model_Prof_cetin.pth')\n",
    "# model = model.half()\n",
    "# Move the model to GPU\n",
    "model = model.to('cuda')\n",
    "model.load_state_dict(torch.load('model_Prof_cetin.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b59398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3981e82f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Batch loss: 0.6683\n",
      "Epoch [1/1000], Batch loss: 0.6030\n",
      "Epoch [1/1000], Batch loss: 0.6990\n",
      "Epoch [1/1000], Batch loss: 0.7653\n",
      "Epoch [1/1000], Batch loss: 0.7374\n",
      "Epoch [1/1000], Batch loss: 0.6032\n",
      "Epoch [1/1000], Batch loss: 0.5528\n",
      "Epoch [1/1000], Batch loss: 0.9133\n",
      "Epoch [1/1000], Batch loss: 0.6542\n",
      "Epoch [1/1000], Batch loss: 0.6533\n",
      "Epoch [1/1000], Batch loss: 0.6857\n",
      "Epoch [1/1000], Batch loss: 0.8309\n",
      "Epoch [1/1000], Batch loss: 0.5834\n",
      "Epoch [1/1000], Batch loss: 0.7628\n",
      "Epoch [1/1000], Batch loss: 0.8273\n",
      "Epoch [1/1000], Batch loss: 0.5965\n",
      "Epoch [1/1000], Batch loss: 0.8758\n",
      "Epoch [1/1000], Batch loss: 0.7118\n",
      "Epoch [1/1000], Batch loss: 0.8764\n",
      "Epoch [1/1000], Batch loss: 0.6921\n",
      "Epoch [1/1000], Batch loss: 0.6528\n",
      "Epoch [1/1000], Batch loss: 0.6295\n",
      "Epoch [1/1000], Batch loss: 0.8431\n",
      "Epoch [1/1000], Batch loss: 0.6874\n",
      "Epoch [1/1000], Batch loss: 0.7775\n",
      "Epoch [1/1000], Batch loss: 0.8494\n",
      "Epoch [1/1000], Batch loss: 0.7775\n",
      "Epoch [1/1000], Batch loss: 0.6063\n",
      "Epoch [1/1000], Batch loss: 0.6544\n",
      "Epoch [1/1000], Batch loss: 0.7618\n",
      "Epoch [1/1000], Batch loss: 0.9876\n",
      "Epoch [1/1000], Batch loss: 0.5976\n",
      "Epoch [1/1000], Batch loss: 0.8740\n",
      "Epoch [1/1000], Batch loss: 0.7095\n",
      "Epoch [1/1000], Batch loss: 0.7474\n",
      "Epoch [1/1000], Batch loss: 0.6751\n",
      "Epoch [1/1000], Batch loss: 0.6944\n",
      "Epoch [1/1000], Batch loss: 0.7110\n",
      "Epoch [1/1000], Batch loss: 0.6953\n",
      "Epoch [1/1000], Batch loss: 0.5831\n",
      "Epoch [1/1000], Batch loss: 0.7299\n",
      "Epoch [1/1000], Batch loss: 0.6524\n",
      "Epoch [1/1000], Batch loss: 0.8476\n",
      "Epoch [1/1000], Batch loss: 0.8276\n",
      "Epoch [1/1000], Batch loss: 0.6331\n",
      "Epoch [1/1000], Batch loss: 0.7230\n",
      "Epoch [1/1000], Batch loss: 0.9058\n",
      "Epoch [1/1000], Batch loss: 0.7527\n",
      "Epoch [1/1000], Batch loss: 0.6212\n",
      "Epoch [1/1000], Batch loss: 0.8268\n",
      "Epoch [1/1000], Batch loss: 0.6263\n",
      "Epoch [1/1000], Batch loss: 0.7515\n",
      "Epoch [1/1000], Batch loss: 0.7631\n",
      "Epoch [1/1000], Batch loss: 0.7487\n",
      "Epoch [1/1000], Batch loss: 0.7407\n",
      "Epoch [1/1000], Batch loss: 0.8360\n",
      "Epoch [1/1000], Batch loss: 0.7387\n",
      "Epoch [1/1000], Batch loss: 0.6514\n",
      "Epoch [1/1000], Batch loss: 0.6800\n",
      "Epoch [1/1000], Batch loss: 0.6016\n",
      "Epoch [1/1000], Batch loss: 0.8449\n",
      "Epoch [1/1000], Batch loss: 0.5893\n",
      "Epoch [1/1000], Batch loss: 0.7796\n",
      "Epoch [1/1000], Batch loss: 0.7141\n",
      "Epoch [1/1000], Batch loss: 0.6841\n",
      "Epoch [1/1000], Batch loss: 0.8545\n",
      "Epoch [1/1000], Batch loss: 0.6645\n",
      "Epoch [1/1000], Batch loss: 0.7132\n",
      "Epoch [1/1000], Batch loss: 0.8556\n",
      "Epoch [1/1000], Batch loss: 0.7744\n",
      "Epoch [1/1000], Batch loss: 0.7079\n",
      "Epoch [1/1000], Batch loss: 0.6766\n",
      "Epoch [1/1000], Batch loss: 0.6866\n",
      "Epoch [1/1000], Batch loss: 0.7907\n",
      "Epoch [1/1000], Batch loss: 0.7254\n",
      "Epoch [1/1000], Batch loss: 0.6977\n",
      "Train Accuracy 0.5008291873963516\n",
      "Test Accuracy 0.5147058823529411\n",
      "Epoch [2/1000], Batch loss: 0.7646\n",
      "Epoch [2/1000], Batch loss: 0.7323\n",
      "Epoch [2/1000], Batch loss: 0.7252\n",
      "Epoch [2/1000], Batch loss: 0.7298\n",
      "Epoch [2/1000], Batch loss: 0.6840\n",
      "Epoch [2/1000], Batch loss: 0.8157\n",
      "Epoch [2/1000], Batch loss: 0.6505\n",
      "Epoch [2/1000], Batch loss: 0.6767\n",
      "Epoch [2/1000], Batch loss: 0.6482\n",
      "Epoch [2/1000], Batch loss: 0.6283\n",
      "Epoch [2/1000], Batch loss: 0.8144\n",
      "Epoch [2/1000], Batch loss: 0.7211\n",
      "Epoch [2/1000], Batch loss: 0.7003\n",
      "Epoch [2/1000], Batch loss: 0.7772\n",
      "Epoch [2/1000], Batch loss: 0.6235\n",
      "Epoch [2/1000], Batch loss: 0.8872\n",
      "Epoch [2/1000], Batch loss: 0.6599\n",
      "Epoch [2/1000], Batch loss: 0.7735\n",
      "Epoch [2/1000], Batch loss: 0.6252\n",
      "Epoch [2/1000], Batch loss: 0.6907\n",
      "Epoch [2/1000], Batch loss: 0.7729\n",
      "Epoch [2/1000], Batch loss: 0.6981\n",
      "Epoch [2/1000], Batch loss: 0.7029\n",
      "Epoch [2/1000], Batch loss: 0.7410\n",
      "Epoch [2/1000], Batch loss: 0.7287\n",
      "Epoch [2/1000], Batch loss: 0.7441\n",
      "Epoch [2/1000], Batch loss: 0.7419\n",
      "Epoch [2/1000], Batch loss: 0.6676\n",
      "Epoch [2/1000], Batch loss: 0.6503\n",
      "Epoch [2/1000], Batch loss: 0.8849\n",
      "Epoch [2/1000], Batch loss: 0.6525\n",
      "Epoch [2/1000], Batch loss: 0.7366\n",
      "Epoch [2/1000], Batch loss: 0.6669\n",
      "Epoch [2/1000], Batch loss: 0.6508\n",
      "Epoch [2/1000], Batch loss: 0.9727\n",
      "Epoch [2/1000], Batch loss: 0.7338\n",
      "Epoch [2/1000], Batch loss: 0.7592\n",
      "Epoch [2/1000], Batch loss: 0.7446\n",
      "Epoch [2/1000], Batch loss: 0.7277\n",
      "Epoch [2/1000], Batch loss: 0.8237\n",
      "Epoch [2/1000], Batch loss: 0.6691\n",
      "Epoch [2/1000], Batch loss: 0.8027\n",
      "Epoch [2/1000], Batch loss: 0.6857\n",
      "Epoch [2/1000], Batch loss: 0.6233\n",
      "Epoch [2/1000], Batch loss: 0.7210\n",
      "Epoch [2/1000], Batch loss: 0.7336\n",
      "Epoch [2/1000], Batch loss: 0.7305\n",
      "Epoch [2/1000], Batch loss: 0.8162\n",
      "Epoch [2/1000], Batch loss: 0.7394\n",
      "Epoch [2/1000], Batch loss: 0.6951\n",
      "Epoch [2/1000], Batch loss: 0.5723\n",
      "Epoch [2/1000], Batch loss: 0.7506\n",
      "Epoch [2/1000], Batch loss: 0.8423\n",
      "Epoch [2/1000], Batch loss: 0.6765\n",
      "Epoch [2/1000], Batch loss: 0.9839\n",
      "Epoch [2/1000], Batch loss: 0.6146\n",
      "Epoch [2/1000], Batch loss: 0.6212\n",
      "Epoch [2/1000], Batch loss: 0.6552\n",
      "Epoch [2/1000], Batch loss: 0.6782\n",
      "Epoch [2/1000], Batch loss: 0.7452\n",
      "Epoch [2/1000], Batch loss: 0.7786\n",
      "Epoch [2/1000], Batch loss: 0.7238\n",
      "Epoch [2/1000], Batch loss: 0.7204\n",
      "Epoch [2/1000], Batch loss: 0.7545\n",
      "Epoch [2/1000], Batch loss: 0.6734\n",
      "Epoch [2/1000], Batch loss: 0.7076\n",
      "Epoch [2/1000], Batch loss: 0.6750\n",
      "Epoch [2/1000], Batch loss: 0.9053\n",
      "Epoch [2/1000], Batch loss: 0.8399\n",
      "Epoch [2/1000], Batch loss: 0.7472\n",
      "Epoch [2/1000], Batch loss: 0.7421\n",
      "Epoch [2/1000], Batch loss: 0.6166\n",
      "Epoch [2/1000], Batch loss: 0.7295\n",
      "Epoch [2/1000], Batch loss: 0.8444\n",
      "Epoch [2/1000], Batch loss: 0.6622\n",
      "Epoch [2/1000], Batch loss: 0.6946\n",
      "Train Accuracy 0.47429519071310117\n",
      "Test Accuracy 0.4852941176470588\n",
      "Epoch [3/1000], Batch loss: 0.7588\n",
      "Epoch [3/1000], Batch loss: 0.7452\n",
      "Epoch [3/1000], Batch loss: 0.7430\n",
      "Epoch [3/1000], Batch loss: 0.6581\n",
      "Epoch [3/1000], Batch loss: 0.5744\n",
      "Epoch [3/1000], Batch loss: 0.7351\n",
      "Epoch [3/1000], Batch loss: 0.7625\n",
      "Epoch [3/1000], Batch loss: 0.7086\n",
      "Epoch [3/1000], Batch loss: 0.6997\n",
      "Epoch [3/1000], Batch loss: 0.7697\n",
      "Epoch [3/1000], Batch loss: 0.8476\n",
      "Epoch [3/1000], Batch loss: 0.8514\n",
      "Epoch [3/1000], Batch loss: 0.6222\n",
      "Epoch [3/1000], Batch loss: 0.8408\n",
      "Epoch [3/1000], Batch loss: 0.6600\n",
      "Epoch [3/1000], Batch loss: 0.6088\n",
      "Epoch [3/1000], Batch loss: 0.8202\n",
      "Epoch [3/1000], Batch loss: 0.6301\n",
      "Epoch [3/1000], Batch loss: 0.7858\n",
      "Epoch [3/1000], Batch loss: 0.5381\n",
      "Epoch [3/1000], Batch loss: 0.9289\n",
      "Epoch [3/1000], Batch loss: 0.6363\n",
      "Epoch [3/1000], Batch loss: 0.7349\n",
      "Epoch [3/1000], Batch loss: 0.7912\n",
      "Epoch [3/1000], Batch loss: 0.7995\n",
      "Epoch [3/1000], Batch loss: 0.9339\n",
      "Epoch [3/1000], Batch loss: 0.8481\n",
      "Epoch [3/1000], Batch loss: 0.7857\n",
      "Epoch [3/1000], Batch loss: 0.6675\n",
      "Epoch [3/1000], Batch loss: 0.7416\n",
      "Epoch [3/1000], Batch loss: 0.8252\n",
      "Epoch [3/1000], Batch loss: 0.7410\n",
      "Epoch [3/1000], Batch loss: 0.7330\n",
      "Epoch [3/1000], Batch loss: 0.6438\n",
      "Epoch [3/1000], Batch loss: 0.9032\n",
      "Epoch [3/1000], Batch loss: 0.6759\n",
      "Epoch [3/1000], Batch loss: 0.7481\n",
      "Epoch [3/1000], Batch loss: 0.7784\n",
      "Epoch [3/1000], Batch loss: 0.7465\n",
      "Epoch [3/1000], Batch loss: 0.6400\n",
      "Epoch [3/1000], Batch loss: 0.7270\n",
      "Epoch [3/1000], Batch loss: 0.7323\n",
      "Epoch [3/1000], Batch loss: 0.7049\n",
      "Epoch [3/1000], Batch loss: 0.6775\n",
      "Epoch [3/1000], Batch loss: 0.8400\n",
      "Epoch [3/1000], Batch loss: 0.8545\n",
      "Epoch [3/1000], Batch loss: 0.7963\n",
      "Epoch [3/1000], Batch loss: 0.5653\n",
      "Epoch [3/1000], Batch loss: 0.5930\n",
      "Epoch [3/1000], Batch loss: 0.7910\n",
      "Epoch [3/1000], Batch loss: 0.7189\n",
      "Epoch [3/1000], Batch loss: 0.6776\n",
      "Epoch [3/1000], Batch loss: 0.6884\n",
      "Epoch [3/1000], Batch loss: 0.7933\n",
      "Epoch [3/1000], Batch loss: 0.5607\n",
      "Epoch [3/1000], Batch loss: 0.5849\n",
      "Epoch [3/1000], Batch loss: 0.8493\n",
      "Epoch [3/1000], Batch loss: 0.9096\n",
      "Epoch [3/1000], Batch loss: 0.5966\n",
      "Epoch [3/1000], Batch loss: 0.6163\n",
      "Epoch [3/1000], Batch loss: 0.8673\n",
      "Epoch [3/1000], Batch loss: 0.7739\n",
      "Epoch [3/1000], Batch loss: 0.6401\n",
      "Epoch [3/1000], Batch loss: 0.7500\n",
      "Epoch [3/1000], Batch loss: 0.7126\n",
      "Epoch [3/1000], Batch loss: 0.7155\n",
      "Epoch [3/1000], Batch loss: 0.4845\n",
      "Epoch [3/1000], Batch loss: 0.7017\n",
      "Epoch [3/1000], Batch loss: 0.7920\n",
      "Epoch [3/1000], Batch loss: 0.7050\n",
      "Epoch [3/1000], Batch loss: 0.8132\n",
      "Epoch [3/1000], Batch loss: 0.9365\n",
      "Epoch [3/1000], Batch loss: 0.9528\n",
      "Epoch [3/1000], Batch loss: 0.6892\n",
      "Epoch [3/1000], Batch loss: 0.7786\n",
      "Epoch [3/1000], Batch loss: 0.5555\n",
      "Train Accuracy 0.4859038142620232\n",
      "Test Accuracy 0.4264705882352941\n",
      "Epoch [4/1000], Batch loss: 0.8058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/1000], Batch loss: 0.8288\n",
      "Epoch [4/1000], Batch loss: 0.5685\n",
      "Epoch [4/1000], Batch loss: 0.7027\n",
      "Epoch [4/1000], Batch loss: 0.8179\n",
      "Epoch [4/1000], Batch loss: 0.6935\n",
      "Epoch [4/1000], Batch loss: 0.8287\n",
      "Epoch [4/1000], Batch loss: 0.7372\n",
      "Epoch [4/1000], Batch loss: 0.7908\n",
      "Epoch [4/1000], Batch loss: 0.7829\n",
      "Epoch [4/1000], Batch loss: 0.6076\n",
      "Epoch [4/1000], Batch loss: 0.7557\n",
      "Epoch [4/1000], Batch loss: 0.6380\n",
      "Epoch [4/1000], Batch loss: 0.7786\n",
      "Epoch [4/1000], Batch loss: 0.8121\n",
      "Epoch [4/1000], Batch loss: 0.7869\n",
      "Epoch [4/1000], Batch loss: 0.7374\n",
      "Epoch [4/1000], Batch loss: 0.6934\n",
      "Epoch [4/1000], Batch loss: 0.6909\n",
      "Epoch [4/1000], Batch loss: 0.7326\n",
      "Epoch [4/1000], Batch loss: 0.6887\n",
      "Epoch [4/1000], Batch loss: 0.6559\n",
      "Epoch [4/1000], Batch loss: 0.7274\n",
      "Epoch [4/1000], Batch loss: 0.7310\n",
      "Epoch [4/1000], Batch loss: 0.7587\n",
      "Epoch [4/1000], Batch loss: 0.7049\n",
      "Epoch [4/1000], Batch loss: 0.6955\n",
      "Epoch [4/1000], Batch loss: 0.7502\n",
      "Epoch [4/1000], Batch loss: 0.8187\n",
      "Epoch [4/1000], Batch loss: 0.8317\n",
      "Epoch [4/1000], Batch loss: 0.7944\n",
      "Epoch [4/1000], Batch loss: 0.7137\n",
      "Epoch [4/1000], Batch loss: 0.8576\n",
      "Epoch [4/1000], Batch loss: 0.6707\n",
      "Epoch [4/1000], Batch loss: 0.9568\n",
      "Epoch [4/1000], Batch loss: 0.7037\n",
      "Epoch [4/1000], Batch loss: 0.6315\n",
      "Epoch [4/1000], Batch loss: 0.5528\n",
      "Epoch [4/1000], Batch loss: 0.7467\n",
      "Epoch [4/1000], Batch loss: 0.9304\n",
      "Epoch [4/1000], Batch loss: 0.7008\n",
      "Epoch [4/1000], Batch loss: 0.8243\n",
      "Epoch [4/1000], Batch loss: 0.7115\n",
      "Epoch [4/1000], Batch loss: 0.8782\n",
      "Epoch [4/1000], Batch loss: 0.6552\n",
      "Epoch [4/1000], Batch loss: 0.8472\n",
      "Epoch [4/1000], Batch loss: 0.8011\n",
      "Epoch [4/1000], Batch loss: 0.6746\n",
      "Epoch [4/1000], Batch loss: 0.7234\n",
      "Epoch [4/1000], Batch loss: 0.7963\n",
      "Epoch [4/1000], Batch loss: 0.7945\n",
      "Epoch [4/1000], Batch loss: 0.7637\n",
      "Epoch [4/1000], Batch loss: 0.7450\n",
      "Epoch [4/1000], Batch loss: 0.6397\n",
      "Epoch [4/1000], Batch loss: 0.6577\n",
      "Epoch [4/1000], Batch loss: 0.7161\n",
      "Epoch [4/1000], Batch loss: 0.8148\n",
      "Epoch [4/1000], Batch loss: 0.5831\n",
      "Epoch [4/1000], Batch loss: 0.6840\n",
      "Epoch [4/1000], Batch loss: 0.6497\n",
      "Epoch [4/1000], Batch loss: 0.6335\n",
      "Epoch [4/1000], Batch loss: 0.6428\n",
      "Epoch [4/1000], Batch loss: 0.8623\n",
      "Epoch [4/1000], Batch loss: 0.7229\n",
      "Epoch [4/1000], Batch loss: 0.7014\n",
      "Epoch [4/1000], Batch loss: 0.7242\n",
      "Epoch [4/1000], Batch loss: 0.6449\n",
      "Epoch [4/1000], Batch loss: 0.6813\n",
      "Epoch [4/1000], Batch loss: 0.7202\n",
      "Epoch [4/1000], Batch loss: 0.6733\n",
      "Epoch [4/1000], Batch loss: 0.7221\n",
      "Epoch [4/1000], Batch loss: 0.8113\n",
      "Epoch [4/1000], Batch loss: 0.5850\n",
      "Epoch [4/1000], Batch loss: 0.6074\n",
      "Epoch [4/1000], Batch loss: 0.7327\n",
      "Epoch [4/1000], Batch loss: 0.6585\n",
      "Train Accuracy 0.5024875621890548\n",
      "Test Accuracy 0.6617647058823529\n",
      "Epoch [5/1000], Batch loss: 0.8305\n",
      "Epoch [5/1000], Batch loss: 0.8329\n",
      "Epoch [5/1000], Batch loss: 0.6970\n",
      "Epoch [5/1000], Batch loss: 0.7238\n",
      "Epoch [5/1000], Batch loss: 0.6561\n",
      "Epoch [5/1000], Batch loss: 0.7134\n",
      "Epoch [5/1000], Batch loss: 0.6599\n",
      "Epoch [5/1000], Batch loss: 0.5993\n",
      "Epoch [5/1000], Batch loss: 0.8475\n",
      "Epoch [5/1000], Batch loss: 0.6734\n",
      "Epoch [5/1000], Batch loss: 0.7793\n",
      "Epoch [5/1000], Batch loss: 0.6585\n",
      "Epoch [5/1000], Batch loss: 0.7100\n",
      "Epoch [5/1000], Batch loss: 0.7598\n",
      "Epoch [5/1000], Batch loss: 0.8312\n",
      "Epoch [5/1000], Batch loss: 0.9887\n",
      "Epoch [5/1000], Batch loss: 0.6555\n",
      "Epoch [5/1000], Batch loss: 0.7376\n",
      "Epoch [5/1000], Batch loss: 0.7225\n",
      "Epoch [5/1000], Batch loss: 0.7209\n",
      "Epoch [5/1000], Batch loss: 0.7420\n",
      "Epoch [5/1000], Batch loss: 0.8074\n",
      "Epoch [5/1000], Batch loss: 0.6888\n",
      "Epoch [5/1000], Batch loss: 0.6854\n",
      "Epoch [5/1000], Batch loss: 0.6563\n",
      "Epoch [5/1000], Batch loss: 0.6129\n",
      "Epoch [5/1000], Batch loss: 0.6854\n",
      "Epoch [5/1000], Batch loss: 0.6979\n",
      "Epoch [5/1000], Batch loss: 0.6667\n",
      "Epoch [5/1000], Batch loss: 0.7738\n",
      "Epoch [5/1000], Batch loss: 0.7396\n",
      "Epoch [5/1000], Batch loss: 0.5538\n",
      "Epoch [5/1000], Batch loss: 0.9221\n",
      "Epoch [5/1000], Batch loss: 0.7868\n",
      "Epoch [5/1000], Batch loss: 0.7599\n",
      "Epoch [5/1000], Batch loss: 0.6650\n",
      "Epoch [5/1000], Batch loss: 0.9163\n",
      "Epoch [5/1000], Batch loss: 0.7203\n",
      "Epoch [5/1000], Batch loss: 0.6429\n",
      "Epoch [5/1000], Batch loss: 0.9359\n",
      "Epoch [5/1000], Batch loss: 0.6638\n",
      "Epoch [5/1000], Batch loss: 0.8215\n",
      "Epoch [5/1000], Batch loss: 0.7347\n",
      "Epoch [5/1000], Batch loss: 0.7324\n",
      "Epoch [5/1000], Batch loss: 0.5784\n",
      "Epoch [5/1000], Batch loss: 0.8567\n",
      "Epoch [5/1000], Batch loss: 0.6886\n",
      "Epoch [5/1000], Batch loss: 0.8188\n",
      "Epoch [5/1000], Batch loss: 0.5927\n",
      "Epoch [5/1000], Batch loss: 0.7918\n",
      "Epoch [5/1000], Batch loss: 0.6925\n",
      "Epoch [5/1000], Batch loss: 0.7291\n",
      "Epoch [5/1000], Batch loss: 0.9708\n",
      "Epoch [5/1000], Batch loss: 0.9165\n",
      "Epoch [5/1000], Batch loss: 0.8958\n",
      "Epoch [5/1000], Batch loss: 0.7928\n",
      "Epoch [5/1000], Batch loss: 0.7586\n",
      "Epoch [5/1000], Batch loss: 0.6414\n",
      "Epoch [5/1000], Batch loss: 0.6441\n",
      "Epoch [5/1000], Batch loss: 0.6745\n",
      "Epoch [5/1000], Batch loss: 0.8151\n",
      "Epoch [5/1000], Batch loss: 0.6321\n",
      "Epoch [5/1000], Batch loss: 0.5822\n",
      "Epoch [5/1000], Batch loss: 0.9174\n",
      "Epoch [5/1000], Batch loss: 0.6558\n",
      "Epoch [5/1000], Batch loss: 0.9347\n",
      "Epoch [5/1000], Batch loss: 0.6535\n",
      "Epoch [5/1000], Batch loss: 0.7513\n",
      "Epoch [5/1000], Batch loss: 0.6740\n",
      "Epoch [5/1000], Batch loss: 0.7913\n",
      "Epoch [5/1000], Batch loss: 0.5669\n",
      "Epoch [5/1000], Batch loss: 0.8399\n",
      "Epoch [5/1000], Batch loss: 0.6442\n",
      "Epoch [5/1000], Batch loss: 0.7962\n",
      "Epoch [5/1000], Batch loss: 0.5920\n",
      "Epoch [5/1000], Batch loss: 0.6448\n",
      "Train Accuracy 0.494195688225539\n",
      "Test Accuracy 0.4411764705882353\n",
      "Epoch [6/1000], Batch loss: 0.7644\n",
      "Epoch [6/1000], Batch loss: 0.7456\n",
      "Epoch [6/1000], Batch loss: 0.7632\n",
      "Epoch [6/1000], Batch loss: 0.7357\n",
      "Epoch [6/1000], Batch loss: 0.7195\n",
      "Epoch [6/1000], Batch loss: 0.6057\n",
      "Epoch [6/1000], Batch loss: 0.7164\n",
      "Epoch [6/1000], Batch loss: 0.7420\n",
      "Epoch [6/1000], Batch loss: 0.6790\n",
      "Epoch [6/1000], Batch loss: 0.6252\n",
      "Epoch [6/1000], Batch loss: 0.6312\n",
      "Epoch [6/1000], Batch loss: 0.7214\n",
      "Epoch [6/1000], Batch loss: 0.6615\n",
      "Epoch [6/1000], Batch loss: 0.6085\n",
      "Epoch [6/1000], Batch loss: 0.9259\n",
      "Epoch [6/1000], Batch loss: 0.7397\n",
      "Epoch [6/1000], Batch loss: 0.9522\n",
      "Epoch [6/1000], Batch loss: 0.8404\n",
      "Epoch [6/1000], Batch loss: 0.9159\n",
      "Epoch [6/1000], Batch loss: 0.8990\n",
      "Epoch [6/1000], Batch loss: 0.7998\n",
      "Epoch [6/1000], Batch loss: 0.8549\n",
      "Epoch [6/1000], Batch loss: 0.5878\n",
      "Epoch [6/1000], Batch loss: 0.6227\n",
      "Epoch [6/1000], Batch loss: 0.7503\n",
      "Epoch [6/1000], Batch loss: 0.6305\n",
      "Epoch [6/1000], Batch loss: 0.6930\n",
      "Epoch [6/1000], Batch loss: 0.5644\n",
      "Epoch [6/1000], Batch loss: 0.7613\n",
      "Epoch [6/1000], Batch loss: 0.7440\n",
      "Epoch [6/1000], Batch loss: 0.6348\n",
      "Epoch [6/1000], Batch loss: 0.7142\n",
      "Epoch [6/1000], Batch loss: 0.6624\n",
      "Epoch [6/1000], Batch loss: 0.7311\n",
      "Epoch [6/1000], Batch loss: 0.8340\n",
      "Epoch [6/1000], Batch loss: 0.6622\n",
      "Epoch [6/1000], Batch loss: 0.8423\n",
      "Epoch [6/1000], Batch loss: 0.7016\n",
      "Epoch [6/1000], Batch loss: 0.5990\n",
      "Epoch [6/1000], Batch loss: 0.8167\n",
      "Epoch [6/1000], Batch loss: 0.7570\n",
      "Epoch [6/1000], Batch loss: 0.7893\n",
      "Epoch [6/1000], Batch loss: 0.5100\n",
      "Epoch [6/1000], Batch loss: 0.6012\n",
      "Epoch [6/1000], Batch loss: 0.7965\n",
      "Epoch [6/1000], Batch loss: 0.6546\n",
      "Epoch [6/1000], Batch loss: 0.8515\n",
      "Epoch [6/1000], Batch loss: 0.7439\n",
      "Epoch [6/1000], Batch loss: 0.6717\n",
      "Epoch [6/1000], Batch loss: 0.6472\n",
      "Epoch [6/1000], Batch loss: 0.8624\n",
      "Epoch [6/1000], Batch loss: 0.8042\n",
      "Epoch [6/1000], Batch loss: 0.8140\n",
      "Epoch [6/1000], Batch loss: 0.7253\n",
      "Epoch [6/1000], Batch loss: 0.8056\n",
      "Epoch [6/1000], Batch loss: 0.7252\n",
      "Epoch [6/1000], Batch loss: 0.8570\n",
      "Epoch [6/1000], Batch loss: 0.7459\n",
      "Epoch [6/1000], Batch loss: 0.7255\n",
      "Epoch [6/1000], Batch loss: 0.5025\n",
      "Epoch [6/1000], Batch loss: 0.7980\n",
      "Epoch [6/1000], Batch loss: 0.7590\n",
      "Epoch [6/1000], Batch loss: 0.6683\n",
      "Epoch [6/1000], Batch loss: 0.7695\n",
      "Epoch [6/1000], Batch loss: 0.6980\n",
      "Epoch [6/1000], Batch loss: 0.6558\n",
      "Epoch [6/1000], Batch loss: 0.8154\n",
      "Epoch [6/1000], Batch loss: 0.6506\n",
      "Epoch [6/1000], Batch loss: 0.8439\n",
      "Epoch [6/1000], Batch loss: 0.5611\n",
      "Epoch [6/1000], Batch loss: 0.9605\n",
      "Epoch [6/1000], Batch loss: 0.6387\n",
      "Epoch [6/1000], Batch loss: 0.9082\n",
      "Epoch [6/1000], Batch loss: 0.5388\n",
      "Epoch [6/1000], Batch loss: 0.8216\n",
      "Epoch [6/1000], Batch loss: 0.6485\n",
      "Train Accuracy 0.49585406301824214\n",
      "Test Accuracy 0.5\n",
      "Epoch [7/1000], Batch loss: 0.7482\n",
      "Epoch [7/1000], Batch loss: 0.7428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/1000], Batch loss: 0.5219\n",
      "Epoch [7/1000], Batch loss: 0.8644\n",
      "Epoch [7/1000], Batch loss: 0.7927\n",
      "Epoch [7/1000], Batch loss: 0.6720\n",
      "Epoch [7/1000], Batch loss: 0.7539\n",
      "Epoch [7/1000], Batch loss: 0.6817\n",
      "Epoch [7/1000], Batch loss: 0.5997\n",
      "Epoch [7/1000], Batch loss: 0.4932\n",
      "Epoch [7/1000], Batch loss: 0.7988\n",
      "Epoch [7/1000], Batch loss: 0.7178\n",
      "Epoch [7/1000], Batch loss: 0.8413\n",
      "Epoch [7/1000], Batch loss: 0.7526\n",
      "Epoch [7/1000], Batch loss: 0.6693\n",
      "Epoch [7/1000], Batch loss: 0.7424\n",
      "Epoch [7/1000], Batch loss: 0.9256\n",
      "Epoch [7/1000], Batch loss: 0.6696\n",
      "Epoch [7/1000], Batch loss: 0.7205\n",
      "Epoch [7/1000], Batch loss: 0.8039\n",
      "Epoch [7/1000], Batch loss: 0.7259\n",
      "Epoch [7/1000], Batch loss: 0.8447\n",
      "Epoch [7/1000], Batch loss: 0.6558\n",
      "Epoch [7/1000], Batch loss: 0.7143\n",
      "Epoch [7/1000], Batch loss: 0.7356\n",
      "Epoch [7/1000], Batch loss: 0.7402\n",
      "Epoch [7/1000], Batch loss: 0.7941\n",
      "Epoch [7/1000], Batch loss: 0.6310\n",
      "Epoch [7/1000], Batch loss: 0.8121\n",
      "Epoch [7/1000], Batch loss: 0.7288\n",
      "Epoch [7/1000], Batch loss: 0.5626\n",
      "Epoch [7/1000], Batch loss: 0.7708\n",
      "Epoch [7/1000], Batch loss: 0.8558\n",
      "Epoch [7/1000], Batch loss: 0.7592\n",
      "Epoch [7/1000], Batch loss: 0.6391\n",
      "Epoch [7/1000], Batch loss: 0.7232\n",
      "Epoch [7/1000], Batch loss: 0.7047\n",
      "Epoch [7/1000], Batch loss: 0.6370\n",
      "Epoch [7/1000], Batch loss: 0.6185\n",
      "Epoch [7/1000], Batch loss: 0.8616\n",
      "Epoch [7/1000], Batch loss: 0.5557\n",
      "Epoch [7/1000], Batch loss: 0.6448\n",
      "Epoch [7/1000], Batch loss: 0.7395\n",
      "Epoch [7/1000], Batch loss: 0.8714\n",
      "Epoch [7/1000], Batch loss: 0.9361\n",
      "Epoch [7/1000], Batch loss: 0.6651\n",
      "Epoch [7/1000], Batch loss: 0.4603\n",
      "Epoch [7/1000], Batch loss: 0.6596\n",
      "Epoch [7/1000], Batch loss: 0.7204\n",
      "Epoch [7/1000], Batch loss: 0.7885\n",
      "Epoch [7/1000], Batch loss: 0.9511\n",
      "Epoch [7/1000], Batch loss: 0.7919\n",
      "Epoch [7/1000], Batch loss: 0.6952\n",
      "Epoch [7/1000], Batch loss: 0.7702\n",
      "Epoch [7/1000], Batch loss: 0.6323\n",
      "Epoch [7/1000], Batch loss: 0.7207\n",
      "Epoch [7/1000], Batch loss: 0.8568\n",
      "Epoch [7/1000], Batch loss: 0.6898\n",
      "Epoch [7/1000], Batch loss: 0.6218\n",
      "Epoch [7/1000], Batch loss: 0.7336\n",
      "Epoch [7/1000], Batch loss: 0.8504\n",
      "Epoch [7/1000], Batch loss: 1.0210\n",
      "Epoch [7/1000], Batch loss: 0.5689\n",
      "Epoch [7/1000], Batch loss: 0.7358\n",
      "Epoch [7/1000], Batch loss: 0.6030\n",
      "Epoch [7/1000], Batch loss: 0.7252\n",
      "Epoch [7/1000], Batch loss: 0.6812\n",
      "Epoch [7/1000], Batch loss: 0.7224\n",
      "Epoch [7/1000], Batch loss: 0.7719\n",
      "Epoch [7/1000], Batch loss: 0.7922\n",
      "Epoch [7/1000], Batch loss: 0.8250\n",
      "Epoch [7/1000], Batch loss: 0.9330\n",
      "Epoch [7/1000], Batch loss: 0.5340\n",
      "Epoch [7/1000], Batch loss: 0.8212\n",
      "Epoch [7/1000], Batch loss: 0.8269\n",
      "Epoch [7/1000], Batch loss: 0.6770\n",
      "Train Accuracy 0.4859038142620232\n",
      "Test Accuracy 0.5294117647058824\n",
      "Epoch [8/1000], Batch loss: 0.6459\n",
      "Epoch [8/1000], Batch loss: 0.6593\n",
      "Epoch [8/1000], Batch loss: 0.6624\n",
      "Epoch [8/1000], Batch loss: 0.6229\n",
      "Epoch [8/1000], Batch loss: 0.6853\n",
      "Epoch [8/1000], Batch loss: 0.6617\n",
      "Epoch [8/1000], Batch loss: 0.8644\n",
      "Epoch [8/1000], Batch loss: 0.8072\n",
      "Epoch [8/1000], Batch loss: 0.7549\n",
      "Epoch [8/1000], Batch loss: 0.7391\n",
      "Epoch [8/1000], Batch loss: 0.6809\n",
      "Epoch [8/1000], Batch loss: 0.7148\n",
      "Epoch [8/1000], Batch loss: 0.5533\n",
      "Epoch [8/1000], Batch loss: 0.5223\n",
      "Epoch [8/1000], Batch loss: 0.7613\n",
      "Epoch [8/1000], Batch loss: 0.6719\n",
      "Epoch [8/1000], Batch loss: 0.7531\n",
      "Epoch [8/1000], Batch loss: 0.6475\n",
      "Epoch [8/1000], Batch loss: 0.7409\n",
      "Epoch [8/1000], Batch loss: 0.9226\n",
      "Epoch [8/1000], Batch loss: 0.7181\n",
      "Epoch [8/1000], Batch loss: 0.8173\n",
      "Epoch [8/1000], Batch loss: 0.6533\n",
      "Epoch [8/1000], Batch loss: 0.5709\n",
      "Epoch [8/1000], Batch loss: 0.7248\n",
      "Epoch [8/1000], Batch loss: 0.8188\n",
      "Epoch [8/1000], Batch loss: 0.8524\n",
      "Epoch [8/1000], Batch loss: 0.6355\n",
      "Epoch [8/1000], Batch loss: 0.7076\n",
      "Epoch [8/1000], Batch loss: 0.6682\n",
      "Epoch [8/1000], Batch loss: 0.7549\n",
      "Epoch [8/1000], Batch loss: 0.5607\n",
      "Epoch [8/1000], Batch loss: 0.7010\n",
      "Epoch [8/1000], Batch loss: 0.6956\n",
      "Epoch [8/1000], Batch loss: 0.7806\n",
      "Epoch [8/1000], Batch loss: 0.7538\n",
      "Epoch [8/1000], Batch loss: 0.8502\n",
      "Epoch [8/1000], Batch loss: 0.7681\n",
      "Epoch [8/1000], Batch loss: 0.8922\n",
      "Epoch [8/1000], Batch loss: 0.6549\n",
      "Epoch [8/1000], Batch loss: 0.7271\n",
      "Epoch [8/1000], Batch loss: 0.7521\n",
      "Epoch [8/1000], Batch loss: 0.7200\n",
      "Epoch [8/1000], Batch loss: 0.6540\n",
      "Epoch [8/1000], Batch loss: 0.7821\n",
      "Epoch [8/1000], Batch loss: 0.6473\n",
      "Epoch [8/1000], Batch loss: 0.6469\n",
      "Epoch [8/1000], Batch loss: 0.5868\n",
      "Epoch [8/1000], Batch loss: 0.7635\n",
      "Epoch [8/1000], Batch loss: 0.9141\n",
      "Epoch [8/1000], Batch loss: 0.6752\n",
      "Epoch [8/1000], Batch loss: 0.8084\n",
      "Epoch [8/1000], Batch loss: 0.6917\n",
      "Epoch [8/1000], Batch loss: 0.7520\n",
      "Epoch [8/1000], Batch loss: 0.6875\n",
      "Epoch [8/1000], Batch loss: 0.6730\n",
      "Epoch [8/1000], Batch loss: 0.7435\n",
      "Epoch [8/1000], Batch loss: 0.7773\n",
      "Epoch [8/1000], Batch loss: 0.6948\n",
      "Epoch [8/1000], Batch loss: 0.6081\n",
      "Epoch [8/1000], Batch loss: 0.7078\n",
      "Epoch [8/1000], Batch loss: 0.7326\n",
      "Epoch [8/1000], Batch loss: 0.6316\n",
      "Epoch [8/1000], Batch loss: 0.7410\n",
      "Epoch [8/1000], Batch loss: 0.6813\n",
      "Epoch [8/1000], Batch loss: 0.7539\n",
      "Epoch [8/1000], Batch loss: 0.7124\n",
      "Epoch [8/1000], Batch loss: 0.7989\n",
      "Epoch [8/1000], Batch loss: 0.6973\n",
      "Epoch [8/1000], Batch loss: 0.6690\n",
      "Epoch [8/1000], Batch loss: 0.6493\n",
      "Epoch [8/1000], Batch loss: 0.7873\n",
      "Epoch [8/1000], Batch loss: 0.7945\n",
      "Epoch [8/1000], Batch loss: 0.6680\n",
      "Epoch [8/1000], Batch loss: 0.6239\n",
      "Epoch [8/1000], Batch loss: 0.6022\n",
      "Train Accuracy 0.5373134328358209\n",
      "Test Accuracy 0.5147058823529411\n",
      "Epoch [9/1000], Batch loss: 0.7528\n",
      "Epoch [9/1000], Batch loss: 0.5597\n",
      "Epoch [9/1000], Batch loss: 0.8994\n",
      "Epoch [9/1000], Batch loss: 0.7651\n",
      "Epoch [9/1000], Batch loss: 0.8618\n",
      "Epoch [9/1000], Batch loss: 0.6037\n",
      "Epoch [9/1000], Batch loss: 0.7773\n",
      "Epoch [9/1000], Batch loss: 0.6055\n",
      "Epoch [9/1000], Batch loss: 0.5918\n",
      "Epoch [9/1000], Batch loss: 0.4714\n",
      "Epoch [9/1000], Batch loss: 0.8365\n",
      "Epoch [9/1000], Batch loss: 0.6167\n",
      "Epoch [9/1000], Batch loss: 0.7386\n",
      "Epoch [9/1000], Batch loss: 0.7356\n",
      "Epoch [9/1000], Batch loss: 0.7764\n",
      "Epoch [9/1000], Batch loss: 0.6531\n",
      "Epoch [9/1000], Batch loss: 0.8034\n",
      "Epoch [9/1000], Batch loss: 0.7925\n",
      "Epoch [9/1000], Batch loss: 0.7394\n",
      "Epoch [9/1000], Batch loss: 0.8015\n",
      "Epoch [9/1000], Batch loss: 0.5889\n",
      "Epoch [9/1000], Batch loss: 0.7476\n",
      "Epoch [9/1000], Batch loss: 0.6004\n",
      "Epoch [9/1000], Batch loss: 0.6060\n",
      "Epoch [9/1000], Batch loss: 0.4649\n",
      "Epoch [9/1000], Batch loss: 0.6752\n",
      "Epoch [9/1000], Batch loss: 0.9045\n",
      "Epoch [9/1000], Batch loss: 0.7397\n",
      "Epoch [9/1000], Batch loss: 0.8147\n",
      "Epoch [9/1000], Batch loss: 0.7297\n",
      "Epoch [9/1000], Batch loss: 0.5533\n",
      "Epoch [9/1000], Batch loss: 0.6802\n",
      "Epoch [9/1000], Batch loss: 0.7467\n",
      "Epoch [9/1000], Batch loss: 0.7333\n",
      "Epoch [9/1000], Batch loss: 0.6214\n",
      "Epoch [9/1000], Batch loss: 0.4920\n",
      "Epoch [9/1000], Batch loss: 0.8167\n",
      "Epoch [9/1000], Batch loss: 0.7451\n",
      "Epoch [9/1000], Batch loss: 0.5931\n",
      "Epoch [9/1000], Batch loss: 0.8626\n",
      "Epoch [9/1000], Batch loss: 0.6474\n",
      "Epoch [9/1000], Batch loss: 0.6996\n",
      "Epoch [9/1000], Batch loss: 0.5744\n",
      "Epoch [9/1000], Batch loss: 0.5916\n",
      "Epoch [9/1000], Batch loss: 0.7232\n",
      "Epoch [9/1000], Batch loss: 0.7883\n",
      "Epoch [9/1000], Batch loss: 0.7378\n",
      "Epoch [9/1000], Batch loss: 0.8811\n",
      "Epoch [9/1000], Batch loss: 0.7553\n",
      "Epoch [9/1000], Batch loss: 0.7972\n",
      "Epoch [9/1000], Batch loss: 0.7400\n",
      "Epoch [9/1000], Batch loss: 0.6394\n",
      "Epoch [9/1000], Batch loss: 0.7404\n",
      "Epoch [9/1000], Batch loss: 0.6322\n",
      "Epoch [9/1000], Batch loss: 0.9130\n",
      "Epoch [9/1000], Batch loss: 0.7490\n",
      "Epoch [9/1000], Batch loss: 0.7700\n",
      "Epoch [9/1000], Batch loss: 0.7486\n",
      "Epoch [9/1000], Batch loss: 0.7624\n",
      "Epoch [9/1000], Batch loss: 0.5714\n",
      "Epoch [9/1000], Batch loss: 0.7883\n",
      "Epoch [9/1000], Batch loss: 0.8809\n",
      "Epoch [9/1000], Batch loss: 0.7522\n",
      "Epoch [9/1000], Batch loss: 0.7161\n",
      "Epoch [9/1000], Batch loss: 0.7407\n",
      "Epoch [9/1000], Batch loss: 0.8370\n",
      "Epoch [9/1000], Batch loss: 0.5366\n",
      "Epoch [9/1000], Batch loss: 0.5218\n",
      "Epoch [9/1000], Batch loss: 0.7689\n",
      "Epoch [9/1000], Batch loss: 0.8287\n",
      "Epoch [9/1000], Batch loss: 0.7978\n",
      "Epoch [9/1000], Batch loss: 0.5733\n",
      "Epoch [9/1000], Batch loss: 0.7698\n",
      "Epoch [9/1000], Batch loss: 0.7969\n",
      "Epoch [9/1000], Batch loss: 0.7230\n",
      "Epoch [9/1000], Batch loss: 0.8390\n",
      "Train Accuracy 0.5223880597014925\n",
      "Test Accuracy 0.5294117647058824\n",
      "Epoch [10/1000], Batch loss: 0.6613\n",
      "Epoch [10/1000], Batch loss: 0.6837\n",
      "Epoch [10/1000], Batch loss: 0.7270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Batch loss: 0.8371\n",
      "Epoch [10/1000], Batch loss: 0.7490\n",
      "Epoch [10/1000], Batch loss: 0.6167\n",
      "Epoch [10/1000], Batch loss: 0.6910\n",
      "Epoch [10/1000], Batch loss: 0.7362\n",
      "Epoch [10/1000], Batch loss: 0.7458\n",
      "Epoch [10/1000], Batch loss: 0.5794\n",
      "Epoch [10/1000], Batch loss: 0.7158\n",
      "Epoch [10/1000], Batch loss: 0.7443\n",
      "Epoch [10/1000], Batch loss: 0.9559\n",
      "Epoch [10/1000], Batch loss: 0.7525\n",
      "Epoch [10/1000], Batch loss: 0.6886\n",
      "Epoch [10/1000], Batch loss: 0.6457\n",
      "Epoch [10/1000], Batch loss: 0.7117\n",
      "Epoch [10/1000], Batch loss: 0.9254\n",
      "Epoch [10/1000], Batch loss: 0.7281\n",
      "Epoch [10/1000], Batch loss: 0.6898\n",
      "Epoch [10/1000], Batch loss: 0.6492\n",
      "Epoch [10/1000], Batch loss: 0.7583\n",
      "Epoch [10/1000], Batch loss: 0.7286\n",
      "Epoch [10/1000], Batch loss: 0.6487\n",
      "Epoch [10/1000], Batch loss: 0.6046\n",
      "Epoch [10/1000], Batch loss: 0.7124\n",
      "Epoch [10/1000], Batch loss: 0.5627\n",
      "Epoch [10/1000], Batch loss: 0.6903\n",
      "Epoch [10/1000], Batch loss: 0.7702\n",
      "Epoch [10/1000], Batch loss: 0.8436\n",
      "Epoch [10/1000], Batch loss: 0.6906\n",
      "Epoch [10/1000], Batch loss: 0.4637\n",
      "Epoch [10/1000], Batch loss: 0.6283\n",
      "Epoch [10/1000], Batch loss: 0.8903\n",
      "Epoch [10/1000], Batch loss: 0.7044\n",
      "Epoch [10/1000], Batch loss: 0.5967\n",
      "Epoch [10/1000], Batch loss: 0.6090\n",
      "Epoch [10/1000], Batch loss: 0.7434\n",
      "Epoch [10/1000], Batch loss: 0.6510\n",
      "Epoch [10/1000], Batch loss: 0.6419\n",
      "Epoch [10/1000], Batch loss: 0.7458\n",
      "Epoch [10/1000], Batch loss: 0.7426\n",
      "Epoch [10/1000], Batch loss: 0.8038\n",
      "Epoch [10/1000], Batch loss: 0.6017\n",
      "Epoch [10/1000], Batch loss: 0.5363\n",
      "Epoch [10/1000], Batch loss: 0.6649\n",
      "Epoch [10/1000], Batch loss: 0.4994\n",
      "Epoch [10/1000], Batch loss: 0.6167\n",
      "Epoch [10/1000], Batch loss: 0.8813\n",
      "Epoch [10/1000], Batch loss: 0.7107\n",
      "Epoch [10/1000], Batch loss: 0.6929\n",
      "Epoch [10/1000], Batch loss: 0.8965\n",
      "Epoch [10/1000], Batch loss: 0.6081\n",
      "Epoch [10/1000], Batch loss: 0.7235\n",
      "Epoch [10/1000], Batch loss: 0.7543\n",
      "Epoch [10/1000], Batch loss: 0.6153\n",
      "Epoch [10/1000], Batch loss: 0.6883\n",
      "Epoch [10/1000], Batch loss: 0.6282\n",
      "Epoch [10/1000], Batch loss: 0.8214\n",
      "Epoch [10/1000], Batch loss: 0.9204\n",
      "Epoch [10/1000], Batch loss: 0.7206\n",
      "Epoch [10/1000], Batch loss: 0.6831\n",
      "Epoch [10/1000], Batch loss: 0.8085\n",
      "Epoch [10/1000], Batch loss: 0.5968\n",
      "Epoch [10/1000], Batch loss: 0.7802\n",
      "Epoch [10/1000], Batch loss: 0.6906\n",
      "Epoch [10/1000], Batch loss: 0.7804\n",
      "Epoch [10/1000], Batch loss: 0.8272\n",
      "Epoch [10/1000], Batch loss: 0.6757\n",
      "Epoch [10/1000], Batch loss: 0.6623\n",
      "Epoch [10/1000], Batch loss: 0.7310\n",
      "Epoch [10/1000], Batch loss: 0.8563\n",
      "Epoch [10/1000], Batch loss: 0.7877\n",
      "Epoch [10/1000], Batch loss: 0.7712\n",
      "Epoch [10/1000], Batch loss: 1.0273\n",
      "Epoch [10/1000], Batch loss: 0.8006\n",
      "Train Accuracy 0.538971807628524\n",
      "Test Accuracy 0.6029411764705882\n",
      "Epoch [11/1000], Batch loss: 0.7704\n",
      "Epoch [11/1000], Batch loss: 0.8113\n",
      "Epoch [11/1000], Batch loss: 0.6967\n",
      "Epoch [11/1000], Batch loss: 0.8681\n",
      "Epoch [11/1000], Batch loss: 0.5858\n",
      "Epoch [11/1000], Batch loss: 0.7617\n",
      "Epoch [11/1000], Batch loss: 0.6879\n",
      "Epoch [11/1000], Batch loss: 0.5795\n",
      "Epoch [11/1000], Batch loss: 0.8082\n",
      "Epoch [11/1000], Batch loss: 0.6443\n",
      "Epoch [11/1000], Batch loss: 0.7183\n",
      "Epoch [11/1000], Batch loss: 0.5816\n",
      "Epoch [11/1000], Batch loss: 0.7145\n",
      "Epoch [11/1000], Batch loss: 0.7909\n",
      "Epoch [11/1000], Batch loss: 0.7192\n",
      "Epoch [11/1000], Batch loss: 0.6819\n",
      "Epoch [11/1000], Batch loss: 0.8045\n",
      "Epoch [11/1000], Batch loss: 0.7953\n",
      "Epoch [11/1000], Batch loss: 0.6187\n",
      "Epoch [11/1000], Batch loss: 0.6056\n",
      "Epoch [11/1000], Batch loss: 0.7614\n",
      "Epoch [11/1000], Batch loss: 0.8198\n",
      "Epoch [11/1000], Batch loss: 0.5673\n",
      "Epoch [11/1000], Batch loss: 0.5413\n",
      "Epoch [11/1000], Batch loss: 0.7580\n",
      "Epoch [11/1000], Batch loss: 0.7919\n",
      "Epoch [11/1000], Batch loss: 0.7856\n",
      "Epoch [11/1000], Batch loss: 0.7084\n",
      "Epoch [11/1000], Batch loss: 0.9056\n",
      "Epoch [11/1000], Batch loss: 0.5588\n",
      "Epoch [11/1000], Batch loss: 0.6199\n",
      "Epoch [11/1000], Batch loss: 0.6561\n",
      "Epoch [11/1000], Batch loss: 0.6721\n",
      "Epoch [11/1000], Batch loss: 0.9509\n",
      "Epoch [11/1000], Batch loss: 0.5455\n",
      "Epoch [11/1000], Batch loss: 0.6770\n",
      "Epoch [11/1000], Batch loss: 0.4800\n",
      "Epoch [11/1000], Batch loss: 0.8248\n",
      "Epoch [11/1000], Batch loss: 0.8073\n",
      "Epoch [11/1000], Batch loss: 0.6673\n",
      "Epoch [11/1000], Batch loss: 1.0009\n",
      "Epoch [11/1000], Batch loss: 0.7916\n",
      "Epoch [11/1000], Batch loss: 0.7403\n",
      "Epoch [11/1000], Batch loss: 0.8217\n",
      "Epoch [11/1000], Batch loss: 0.7319\n",
      "Epoch [11/1000], Batch loss: 0.6934\n",
      "Epoch [11/1000], Batch loss: 0.7896\n",
      "Epoch [11/1000], Batch loss: 0.6999\n",
      "Epoch [11/1000], Batch loss: 0.6531\n",
      "Epoch [11/1000], Batch loss: 0.6337\n",
      "Epoch [11/1000], Batch loss: 0.6840\n",
      "Epoch [11/1000], Batch loss: 0.6786\n",
      "Epoch [11/1000], Batch loss: 0.6098\n",
      "Epoch [11/1000], Batch loss: 0.6176\n",
      "Epoch [11/1000], Batch loss: 0.5492\n",
      "Epoch [11/1000], Batch loss: 0.7088\n",
      "Epoch [11/1000], Batch loss: 0.5270\n",
      "Epoch [11/1000], Batch loss: 0.7817\n",
      "Epoch [11/1000], Batch loss: 0.7249\n",
      "Epoch [11/1000], Batch loss: 0.7873\n",
      "Epoch [11/1000], Batch loss: 0.7537\n",
      "Epoch [11/1000], Batch loss: 0.8574\n",
      "Epoch [11/1000], Batch loss: 0.6936\n",
      "Epoch [11/1000], Batch loss: 0.5314\n",
      "Epoch [11/1000], Batch loss: 0.5184\n",
      "Epoch [11/1000], Batch loss: 0.7392\n",
      "Epoch [11/1000], Batch loss: 0.7747\n",
      "Epoch [11/1000], Batch loss: 0.7838\n",
      "Epoch [11/1000], Batch loss: 0.8162\n",
      "Epoch [11/1000], Batch loss: 0.5541\n",
      "Epoch [11/1000], Batch loss: 0.6962\n",
      "Epoch [11/1000], Batch loss: 0.7088\n",
      "Epoch [11/1000], Batch loss: 0.6046\n",
      "Epoch [11/1000], Batch loss: 0.6294\n",
      "Epoch [11/1000], Batch loss: 0.5999\n",
      "Epoch [11/1000], Batch loss: 0.4743\n",
      "Train Accuracy 0.5456053067993366\n",
      "Test Accuracy 0.6911764705882353\n",
      "Epoch [12/1000], Batch loss: 0.9100\n",
      "Epoch [12/1000], Batch loss: 0.7836\n",
      "Epoch [12/1000], Batch loss: 0.7274\n",
      "Epoch [12/1000], Batch loss: 0.6239\n",
      "Epoch [12/1000], Batch loss: 0.7840\n",
      "Epoch [12/1000], Batch loss: 0.6538\n",
      "Epoch [12/1000], Batch loss: 0.8767\n",
      "Epoch [12/1000], Batch loss: 0.5714\n",
      "Epoch [12/1000], Batch loss: 0.7576\n",
      "Epoch [12/1000], Batch loss: 0.7983\n",
      "Epoch [12/1000], Batch loss: 0.4996\n",
      "Epoch [12/1000], Batch loss: 0.5369\n",
      "Epoch [12/1000], Batch loss: 0.7339\n",
      "Epoch [12/1000], Batch loss: 0.7196\n",
      "Epoch [12/1000], Batch loss: 0.6459\n",
      "Epoch [12/1000], Batch loss: 0.8076\n",
      "Epoch [12/1000], Batch loss: 0.7787\n",
      "Epoch [12/1000], Batch loss: 0.6095\n",
      "Epoch [12/1000], Batch loss: 0.9087\n",
      "Epoch [12/1000], Batch loss: 0.8466\n",
      "Epoch [12/1000], Batch loss: 0.7874\n",
      "Epoch [12/1000], Batch loss: 0.6202\n",
      "Epoch [12/1000], Batch loss: 0.6950\n",
      "Epoch [12/1000], Batch loss: 0.6779\n",
      "Epoch [12/1000], Batch loss: 0.5403\n",
      "Epoch [12/1000], Batch loss: 0.5060\n",
      "Epoch [12/1000], Batch loss: 0.8684\n",
      "Epoch [12/1000], Batch loss: 0.8813\n",
      "Epoch [12/1000], Batch loss: 0.7566\n",
      "Epoch [12/1000], Batch loss: 0.6570\n",
      "Epoch [12/1000], Batch loss: 0.6104\n",
      "Epoch [12/1000], Batch loss: 0.6601\n",
      "Epoch [12/1000], Batch loss: 0.7654\n",
      "Epoch [12/1000], Batch loss: 0.7245\n",
      "Epoch [12/1000], Batch loss: 0.6591\n",
      "Epoch [12/1000], Batch loss: 0.6651\n",
      "Epoch [12/1000], Batch loss: 0.6650\n",
      "Epoch [12/1000], Batch loss: 0.6347\n",
      "Epoch [12/1000], Batch loss: 0.7214\n",
      "Epoch [12/1000], Batch loss: 0.5830\n",
      "Epoch [12/1000], Batch loss: 0.6841\n",
      "Epoch [12/1000], Batch loss: 0.8394\n",
      "Epoch [12/1000], Batch loss: 0.6377\n",
      "Epoch [12/1000], Batch loss: 1.0272\n",
      "Epoch [12/1000], Batch loss: 0.6673\n",
      "Epoch [12/1000], Batch loss: 0.6604\n",
      "Epoch [12/1000], Batch loss: 0.8583\n",
      "Epoch [12/1000], Batch loss: 0.6929\n",
      "Epoch [12/1000], Batch loss: 0.7157\n",
      "Epoch [12/1000], Batch loss: 0.7067\n",
      "Epoch [12/1000], Batch loss: 0.8042\n",
      "Epoch [12/1000], Batch loss: 0.8156\n",
      "Epoch [12/1000], Batch loss: 0.5514\n",
      "Epoch [12/1000], Batch loss: 0.7778\n",
      "Epoch [12/1000], Batch loss: 0.7294\n",
      "Epoch [12/1000], Batch loss: 0.6707\n",
      "Epoch [12/1000], Batch loss: 0.7566\n",
      "Epoch [12/1000], Batch loss: 0.6970\n",
      "Epoch [12/1000], Batch loss: 0.7957\n",
      "Epoch [12/1000], Batch loss: 0.6189\n",
      "Epoch [12/1000], Batch loss: 0.5956\n",
      "Epoch [12/1000], Batch loss: 0.7034\n",
      "Epoch [12/1000], Batch loss: 1.0534\n",
      "Epoch [12/1000], Batch loss: 0.7584\n",
      "Epoch [12/1000], Batch loss: 0.7665\n",
      "Epoch [12/1000], Batch loss: 0.8281\n",
      "Epoch [12/1000], Batch loss: 0.5609\n",
      "Epoch [12/1000], Batch loss: 0.8385\n",
      "Epoch [12/1000], Batch loss: 0.6675\n",
      "Epoch [12/1000], Batch loss: 0.7279\n",
      "Epoch [12/1000], Batch loss: 0.7199\n",
      "Epoch [12/1000], Batch loss: 0.5961\n",
      "Epoch [12/1000], Batch loss: 0.5969\n",
      "Epoch [12/1000], Batch loss: 0.6892\n",
      "Epoch [12/1000], Batch loss: 0.6533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/1000], Batch loss: 0.8056\n",
      "Train Accuracy 0.5223880597014925\n",
      "Test Accuracy 0.6176470588235294\n",
      "Epoch [13/1000], Batch loss: 0.7711\n",
      "Epoch [13/1000], Batch loss: 0.7370\n",
      "Epoch [13/1000], Batch loss: 0.6574\n",
      "Epoch [13/1000], Batch loss: 0.7675\n",
      "Epoch [13/1000], Batch loss: 0.7182\n",
      "Epoch [13/1000], Batch loss: 0.6820\n",
      "Epoch [13/1000], Batch loss: 0.8308\n",
      "Epoch [13/1000], Batch loss: 0.7730\n",
      "Epoch [13/1000], Batch loss: 0.6984\n",
      "Epoch [13/1000], Batch loss: 0.7651\n",
      "Epoch [13/1000], Batch loss: 0.7218\n",
      "Epoch [13/1000], Batch loss: 0.6403\n",
      "Epoch [13/1000], Batch loss: 0.8418\n",
      "Epoch [13/1000], Batch loss: 0.6459\n",
      "Epoch [13/1000], Batch loss: 0.5014\n",
      "Epoch [13/1000], Batch loss: 0.6306\n",
      "Epoch [13/1000], Batch loss: 0.6899\n",
      "Epoch [13/1000], Batch loss: 0.7534\n",
      "Epoch [13/1000], Batch loss: 0.8155\n",
      "Epoch [13/1000], Batch loss: 0.9126\n",
      "Epoch [13/1000], Batch loss: 0.8136\n",
      "Epoch [13/1000], Batch loss: 0.9228\n",
      "Epoch [13/1000], Batch loss: 0.5921\n",
      "Epoch [13/1000], Batch loss: 0.8106\n",
      "Epoch [13/1000], Batch loss: 0.8619\n",
      "Epoch [13/1000], Batch loss: 0.6780\n",
      "Epoch [13/1000], Batch loss: 0.6466\n",
      "Epoch [13/1000], Batch loss: 0.7597\n",
      "Epoch [13/1000], Batch loss: 0.8999\n",
      "Epoch [13/1000], Batch loss: 0.7445\n",
      "Epoch [13/1000], Batch loss: 0.5832\n",
      "Epoch [13/1000], Batch loss: 0.7913\n",
      "Epoch [13/1000], Batch loss: 0.5853\n",
      "Epoch [13/1000], Batch loss: 0.6749\n",
      "Epoch [13/1000], Batch loss: 0.6642\n",
      "Epoch [13/1000], Batch loss: 0.5621\n",
      "Epoch [13/1000], Batch loss: 0.7067\n",
      "Epoch [13/1000], Batch loss: 0.7171\n",
      "Epoch [13/1000], Batch loss: 0.5994\n",
      "Epoch [13/1000], Batch loss: 0.7686\n",
      "Epoch [13/1000], Batch loss: 0.8431\n",
      "Epoch [13/1000], Batch loss: 0.6743\n",
      "Epoch [13/1000], Batch loss: 0.7110\n",
      "Epoch [13/1000], Batch loss: 0.7294\n",
      "Epoch [13/1000], Batch loss: 0.5771\n",
      "Epoch [13/1000], Batch loss: 0.8208\n",
      "Epoch [13/1000], Batch loss: 0.7666\n",
      "Epoch [13/1000], Batch loss: 0.6645\n",
      "Epoch [13/1000], Batch loss: 0.8683\n",
      "Epoch [13/1000], Batch loss: 0.7616\n",
      "Epoch [13/1000], Batch loss: 0.7402\n",
      "Epoch [13/1000], Batch loss: 0.6412\n",
      "Epoch [13/1000], Batch loss: 0.6509\n",
      "Epoch [13/1000], Batch loss: 0.8653\n",
      "Epoch [13/1000], Batch loss: 0.7665\n",
      "Epoch [13/1000], Batch loss: 0.7892\n",
      "Epoch [13/1000], Batch loss: 0.6741\n",
      "Epoch [13/1000], Batch loss: 0.6295\n",
      "Epoch [13/1000], Batch loss: 0.7340\n",
      "Epoch [13/1000], Batch loss: 0.8955\n",
      "Epoch [13/1000], Batch loss: 0.7562\n",
      "Epoch [13/1000], Batch loss: 0.8001\n",
      "Epoch [13/1000], Batch loss: 0.7502\n",
      "Epoch [13/1000], Batch loss: 0.7598\n",
      "Epoch [13/1000], Batch loss: 0.8358\n",
      "Epoch [13/1000], Batch loss: 0.7344\n",
      "Epoch [13/1000], Batch loss: 0.7736\n",
      "Epoch [13/1000], Batch loss: 0.6076\n",
      "Epoch [13/1000], Batch loss: 0.6593\n",
      "Epoch [13/1000], Batch loss: 0.7147\n",
      "Epoch [13/1000], Batch loss: 0.7837\n",
      "Epoch [13/1000], Batch loss: 0.7903\n",
      "Epoch [13/1000], Batch loss: 0.7132\n",
      "Epoch [13/1000], Batch loss: 0.6553\n",
      "Epoch [13/1000], Batch loss: 0.5240\n",
      "Epoch [13/1000], Batch loss: 0.7626\n",
      "Train Accuracy 0.5124378109452736\n",
      "Test Accuracy 0.5882352941176471\n",
      "Epoch [14/1000], Batch loss: 0.5218\n",
      "Epoch [14/1000], Batch loss: 0.6233\n",
      "Epoch [14/1000], Batch loss: 0.6683\n",
      "Epoch [14/1000], Batch loss: 0.6902\n",
      "Epoch [14/1000], Batch loss: 0.9044\n",
      "Epoch [14/1000], Batch loss: 0.7452\n",
      "Epoch [14/1000], Batch loss: 0.5552\n",
      "Epoch [14/1000], Batch loss: 0.7841\n",
      "Epoch [14/1000], Batch loss: 0.7385\n",
      "Epoch [14/1000], Batch loss: 0.8705\n",
      "Epoch [14/1000], Batch loss: 0.7307\n",
      "Epoch [14/1000], Batch loss: 0.8078\n",
      "Epoch [14/1000], Batch loss: 0.5556\n",
      "Epoch [14/1000], Batch loss: 1.0414\n",
      "Epoch [14/1000], Batch loss: 0.6763\n",
      "Epoch [14/1000], Batch loss: 0.7350\n",
      "Epoch [14/1000], Batch loss: 0.7919\n",
      "Epoch [14/1000], Batch loss: 0.8930\n",
      "Epoch [14/1000], Batch loss: 0.7145\n",
      "Epoch [14/1000], Batch loss: 0.9050\n",
      "Epoch [14/1000], Batch loss: 0.9033\n",
      "Epoch [14/1000], Batch loss: 0.6183\n",
      "Epoch [14/1000], Batch loss: 0.6639\n",
      "Epoch [14/1000], Batch loss: 0.9176\n",
      "Epoch [14/1000], Batch loss: 0.6931\n",
      "Epoch [14/1000], Batch loss: 0.7872\n",
      "Epoch [14/1000], Batch loss: 0.7596\n",
      "Epoch [14/1000], Batch loss: 0.5383\n",
      "Epoch [14/1000], Batch loss: 0.7653\n",
      "Epoch [14/1000], Batch loss: 0.8095\n",
      "Epoch [14/1000], Batch loss: 0.9401\n",
      "Epoch [14/1000], Batch loss: 0.7824\n",
      "Epoch [14/1000], Batch loss: 0.6844\n",
      "Epoch [14/1000], Batch loss: 0.8023\n",
      "Epoch [14/1000], Batch loss: 0.7669\n",
      "Epoch [14/1000], Batch loss: 0.8058\n",
      "Epoch [14/1000], Batch loss: 0.6451\n",
      "Epoch [14/1000], Batch loss: 0.7674\n",
      "Epoch [14/1000], Batch loss: 0.6219\n",
      "Epoch [14/1000], Batch loss: 0.7578\n",
      "Epoch [14/1000], Batch loss: 0.7940\n",
      "Epoch [14/1000], Batch loss: 0.8501\n",
      "Epoch [14/1000], Batch loss: 0.7188\n",
      "Epoch [14/1000], Batch loss: 0.8452\n",
      "Epoch [14/1000], Batch loss: 0.5480\n",
      "Epoch [14/1000], Batch loss: 0.7996\n",
      "Epoch [14/1000], Batch loss: 0.7981\n",
      "Epoch [14/1000], Batch loss: 0.7240\n",
      "Epoch [14/1000], Batch loss: 0.5437\n",
      "Epoch [14/1000], Batch loss: 0.7664\n",
      "Epoch [14/1000], Batch loss: 0.8590\n",
      "Epoch [14/1000], Batch loss: 0.6033\n",
      "Epoch [14/1000], Batch loss: 0.7014\n",
      "Epoch [14/1000], Batch loss: 0.8202\n",
      "Epoch [14/1000], Batch loss: 0.8076\n",
      "Epoch [14/1000], Batch loss: 0.6906\n",
      "Epoch [14/1000], Batch loss: 0.7227\n",
      "Epoch [14/1000], Batch loss: 0.7622\n",
      "Epoch [14/1000], Batch loss: 0.8914\n",
      "Epoch [14/1000], Batch loss: 0.6589\n",
      "Epoch [14/1000], Batch loss: 0.8140\n",
      "Epoch [14/1000], Batch loss: 0.6841\n",
      "Epoch [14/1000], Batch loss: 0.8164\n",
      "Epoch [14/1000], Batch loss: 0.7431\n",
      "Epoch [14/1000], Batch loss: 0.7650\n",
      "Epoch [14/1000], Batch loss: 0.5340\n",
      "Epoch [14/1000], Batch loss: 0.7917\n",
      "Epoch [14/1000], Batch loss: 0.7553\n",
      "Epoch [14/1000], Batch loss: 0.8540\n",
      "Epoch [14/1000], Batch loss: 0.7123\n",
      "Epoch [14/1000], Batch loss: 0.7613\n",
      "Epoch [14/1000], Batch loss: 0.6907\n",
      "Epoch [14/1000], Batch loss: 0.6367\n",
      "Epoch [14/1000], Batch loss: 0.6993\n",
      "Epoch [14/1000], Batch loss: 0.6457\n",
      "Epoch [14/1000], Batch loss: 0.6433\n",
      "Train Accuracy 0.5257048092868989\n",
      "Test Accuracy 0.5882352941176471\n",
      "Epoch [15/1000], Batch loss: 0.9162\n",
      "Epoch [15/1000], Batch loss: 0.6958\n",
      "Epoch [15/1000], Batch loss: 0.7805\n",
      "Epoch [15/1000], Batch loss: 0.5677\n",
      "Epoch [15/1000], Batch loss: 0.6234\n",
      "Epoch [15/1000], Batch loss: 0.6105\n",
      "Epoch [15/1000], Batch loss: 0.9082\n",
      "Epoch [15/1000], Batch loss: 0.8739\n",
      "Epoch [15/1000], Batch loss: 0.7424\n",
      "Epoch [15/1000], Batch loss: 0.6728\n",
      "Epoch [15/1000], Batch loss: 0.5795\n",
      "Epoch [15/1000], Batch loss: 0.7708\n",
      "Epoch [15/1000], Batch loss: 0.5957\n",
      "Epoch [15/1000], Batch loss: 0.6781\n",
      "Epoch [15/1000], Batch loss: 0.7123\n",
      "Epoch [15/1000], Batch loss: 0.6453\n",
      "Epoch [15/1000], Batch loss: 0.6517\n",
      "Epoch [15/1000], Batch loss: 0.7275\n",
      "Epoch [15/1000], Batch loss: 0.6242\n",
      "Epoch [15/1000], Batch loss: 0.6207\n",
      "Epoch [15/1000], Batch loss: 0.8592\n",
      "Epoch [15/1000], Batch loss: 0.6586\n",
      "Epoch [15/1000], Batch loss: 0.7945\n",
      "Epoch [15/1000], Batch loss: 0.8314\n",
      "Epoch [15/1000], Batch loss: 0.7052\n",
      "Epoch [15/1000], Batch loss: 0.7409\n",
      "Epoch [15/1000], Batch loss: 0.6713\n",
      "Epoch [15/1000], Batch loss: 0.8578\n",
      "Epoch [15/1000], Batch loss: 0.8501\n",
      "Epoch [15/1000], Batch loss: 0.5913\n",
      "Epoch [15/1000], Batch loss: 0.6712\n",
      "Epoch [15/1000], Batch loss: 0.7189\n",
      "Epoch [15/1000], Batch loss: 0.8134\n",
      "Epoch [15/1000], Batch loss: 0.7879\n",
      "Epoch [15/1000], Batch loss: 0.6560\n",
      "Epoch [15/1000], Batch loss: 0.7866\n",
      "Epoch [15/1000], Batch loss: 0.7563\n",
      "Epoch [15/1000], Batch loss: 0.6880\n",
      "Epoch [15/1000], Batch loss: 0.5474\n",
      "Epoch [15/1000], Batch loss: 0.8173\n",
      "Epoch [15/1000], Batch loss: 0.6085\n",
      "Epoch [15/1000], Batch loss: 0.7219\n",
      "Epoch [15/1000], Batch loss: 0.5490\n",
      "Epoch [15/1000], Batch loss: 0.7000\n",
      "Epoch [15/1000], Batch loss: 0.7480\n",
      "Epoch [15/1000], Batch loss: 0.6917\n",
      "Epoch [15/1000], Batch loss: 0.8222\n",
      "Epoch [15/1000], Batch loss: 0.9196\n",
      "Epoch [15/1000], Batch loss: 0.6494\n",
      "Epoch [15/1000], Batch loss: 0.5417\n",
      "Epoch [15/1000], Batch loss: 0.8672\n",
      "Epoch [15/1000], Batch loss: 0.7587\n",
      "Epoch [15/1000], Batch loss: 1.0189\n",
      "Epoch [15/1000], Batch loss: 0.6801\n",
      "Epoch [15/1000], Batch loss: 0.8137\n",
      "Epoch [15/1000], Batch loss: 0.5789\n",
      "Epoch [15/1000], Batch loss: 0.6077\n",
      "Epoch [15/1000], Batch loss: 0.8162\n",
      "Epoch [15/1000], Batch loss: 0.8003\n",
      "Epoch [15/1000], Batch loss: 0.7156\n",
      "Epoch [15/1000], Batch loss: 0.5998\n",
      "Epoch [15/1000], Batch loss: 0.8462\n",
      "Epoch [15/1000], Batch loss: 0.5986\n",
      "Epoch [15/1000], Batch loss: 0.8552\n",
      "Epoch [15/1000], Batch loss: 0.5511\n",
      "Epoch [15/1000], Batch loss: 0.8118\n",
      "Epoch [15/1000], Batch loss: 0.7274\n",
      "Epoch [15/1000], Batch loss: 0.6970\n",
      "Epoch [15/1000], Batch loss: 0.7888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/1000], Batch loss: 0.7030\n",
      "Epoch [15/1000], Batch loss: 0.9288\n",
      "Epoch [15/1000], Batch loss: 0.8883\n",
      "Epoch [15/1000], Batch loss: 0.8990\n",
      "Epoch [15/1000], Batch loss: 0.8377\n",
      "Epoch [15/1000], Batch loss: 0.7723\n",
      "Epoch [15/1000], Batch loss: 0.6899\n",
      "Train Accuracy 0.5157545605306799\n",
      "Test Accuracy 0.5588235294117647\n",
      "Epoch [16/1000], Batch loss: 0.8700\n",
      "Epoch [16/1000], Batch loss: 0.7801\n",
      "Epoch [16/1000], Batch loss: 0.8547\n",
      "Epoch [16/1000], Batch loss: 0.7538\n",
      "Epoch [16/1000], Batch loss: 0.8979\n",
      "Epoch [16/1000], Batch loss: 0.7497\n",
      "Epoch [16/1000], Batch loss: 0.6252\n",
      "Epoch [16/1000], Batch loss: 0.5943\n",
      "Epoch [16/1000], Batch loss: 0.8124\n",
      "Epoch [16/1000], Batch loss: 0.5025\n",
      "Epoch [16/1000], Batch loss: 0.5786\n",
      "Epoch [16/1000], Batch loss: 0.8659\n",
      "Epoch [16/1000], Batch loss: 0.8114\n",
      "Epoch [16/1000], Batch loss: 0.8420\n",
      "Epoch [16/1000], Batch loss: 0.7497\n",
      "Epoch [16/1000], Batch loss: 0.7212\n",
      "Epoch [16/1000], Batch loss: 0.7506\n",
      "Epoch [16/1000], Batch loss: 0.7532\n",
      "Epoch [16/1000], Batch loss: 0.7126\n",
      "Epoch [16/1000], Batch loss: 0.8326\n",
      "Epoch [16/1000], Batch loss: 0.4235\n",
      "Epoch [16/1000], Batch loss: 0.8890\n",
      "Epoch [16/1000], Batch loss: 0.6259\n",
      "Epoch [16/1000], Batch loss: 0.7603\n",
      "Epoch [16/1000], Batch loss: 0.8116\n",
      "Epoch [16/1000], Batch loss: 0.9056\n",
      "Epoch [16/1000], Batch loss: 0.9657\n",
      "Epoch [16/1000], Batch loss: 0.7960\n",
      "Epoch [16/1000], Batch loss: 0.5085\n",
      "Epoch [16/1000], Batch loss: 0.8107\n",
      "Epoch [16/1000], Batch loss: 0.7704\n",
      "Epoch [16/1000], Batch loss: 0.6593\n",
      "Epoch [16/1000], Batch loss: 0.5521\n",
      "Epoch [16/1000], Batch loss: 0.7291\n",
      "Epoch [16/1000], Batch loss: 0.7035\n",
      "Epoch [16/1000], Batch loss: 0.8125\n",
      "Epoch [16/1000], Batch loss: 0.6077\n",
      "Epoch [16/1000], Batch loss: 0.5752\n",
      "Epoch [16/1000], Batch loss: 0.7547\n",
      "Epoch [16/1000], Batch loss: 0.5488\n",
      "Epoch [16/1000], Batch loss: 0.6984\n",
      "Epoch [16/1000], Batch loss: 1.0298\n",
      "Epoch [16/1000], Batch loss: 0.8726\n",
      "Epoch [16/1000], Batch loss: 0.6037\n",
      "Epoch [16/1000], Batch loss: 0.8679\n",
      "Epoch [16/1000], Batch loss: 0.6213\n",
      "Epoch [16/1000], Batch loss: 0.6667\n",
      "Epoch [16/1000], Batch loss: 0.6822\n",
      "Epoch [16/1000], Batch loss: 0.6600\n",
      "Epoch [16/1000], Batch loss: 0.6399\n",
      "Epoch [16/1000], Batch loss: 0.5794\n",
      "Epoch [16/1000], Batch loss: 0.9002\n",
      "Epoch [16/1000], Batch loss: 0.6361\n",
      "Epoch [16/1000], Batch loss: 0.6451\n",
      "Epoch [16/1000], Batch loss: 0.7719\n",
      "Epoch [16/1000], Batch loss: 0.5753\n",
      "Epoch [16/1000], Batch loss: 0.7456\n",
      "Epoch [16/1000], Batch loss: 0.7183\n",
      "Epoch [16/1000], Batch loss: 0.6618\n",
      "Epoch [16/1000], Batch loss: 0.7225\n",
      "Epoch [16/1000], Batch loss: 0.5270\n",
      "Epoch [16/1000], Batch loss: 1.2936\n",
      "Epoch [16/1000], Batch loss: 0.7613\n",
      "Epoch [16/1000], Batch loss: 0.6945\n",
      "Epoch [16/1000], Batch loss: 0.4744\n",
      "Epoch [16/1000], Batch loss: 0.7004\n",
      "Epoch [16/1000], Batch loss: 0.5852\n",
      "Epoch [16/1000], Batch loss: 0.6952\n",
      "Epoch [16/1000], Batch loss: 0.5451\n",
      "Epoch [16/1000], Batch loss: 0.6954\n",
      "Epoch [16/1000], Batch loss: 0.6546\n",
      "Epoch [16/1000], Batch loss: 0.6786\n",
      "Epoch [16/1000], Batch loss: 0.7570\n",
      "Epoch [16/1000], Batch loss: 0.7540\n",
      "Epoch [16/1000], Batch loss: 0.6737\n",
      "Epoch [16/1000], Batch loss: 0.7830\n",
      "Train Accuracy 0.5290215588723052\n",
      "Test Accuracy 0.5735294117647058\n",
      "Epoch [17/1000], Batch loss: 0.7633\n",
      "Epoch [17/1000], Batch loss: 0.9226\n",
      "Epoch [17/1000], Batch loss: 0.5959\n",
      "Epoch [17/1000], Batch loss: 0.7645\n",
      "Epoch [17/1000], Batch loss: 0.5651\n",
      "Epoch [17/1000], Batch loss: 0.9128\n",
      "Epoch [17/1000], Batch loss: 0.6803\n",
      "Epoch [17/1000], Batch loss: 0.5538\n",
      "Epoch [17/1000], Batch loss: 0.7521\n",
      "Epoch [17/1000], Batch loss: 0.6381\n",
      "Epoch [17/1000], Batch loss: 0.7350\n",
      "Epoch [17/1000], Batch loss: 0.7760\n",
      "Epoch [17/1000], Batch loss: 0.8208\n",
      "Epoch [17/1000], Batch loss: 0.7396\n",
      "Epoch [17/1000], Batch loss: 0.6643\n",
      "Epoch [17/1000], Batch loss: 0.8651\n",
      "Epoch [17/1000], Batch loss: 0.7626\n",
      "Epoch [17/1000], Batch loss: 0.6729\n",
      "Epoch [17/1000], Batch loss: 0.6702\n",
      "Epoch [17/1000], Batch loss: 0.9428\n",
      "Epoch [17/1000], Batch loss: 0.6451\n",
      "Epoch [17/1000], Batch loss: 0.7430\n",
      "Epoch [17/1000], Batch loss: 0.5826\n",
      "Epoch [17/1000], Batch loss: 0.6906\n",
      "Epoch [17/1000], Batch loss: 0.5910\n",
      "Epoch [17/1000], Batch loss: 0.7463\n",
      "Epoch [17/1000], Batch loss: 0.7233\n",
      "Epoch [17/1000], Batch loss: 0.8691\n",
      "Epoch [17/1000], Batch loss: 0.7434\n",
      "Epoch [17/1000], Batch loss: 0.6431\n",
      "Epoch [17/1000], Batch loss: 0.8049\n",
      "Epoch [17/1000], Batch loss: 0.6482\n",
      "Epoch [17/1000], Batch loss: 0.7785\n",
      "Epoch [17/1000], Batch loss: 0.7093\n",
      "Epoch [17/1000], Batch loss: 0.7889\n",
      "Epoch [17/1000], Batch loss: 0.6778\n",
      "Epoch [17/1000], Batch loss: 0.6416\n",
      "Epoch [17/1000], Batch loss: 0.8047\n",
      "Epoch [17/1000], Batch loss: 0.6122\n",
      "Epoch [17/1000], Batch loss: 0.8791\n",
      "Epoch [17/1000], Batch loss: 0.6802\n",
      "Epoch [17/1000], Batch loss: 0.6689\n",
      "Epoch [17/1000], Batch loss: 0.6304\n",
      "Epoch [17/1000], Batch loss: 0.5976\n",
      "Epoch [17/1000], Batch loss: 0.6306\n",
      "Epoch [17/1000], Batch loss: 0.4950\n",
      "Epoch [17/1000], Batch loss: 0.7892\n",
      "Epoch [17/1000], Batch loss: 0.8663\n",
      "Epoch [17/1000], Batch loss: 0.7610\n",
      "Epoch [17/1000], Batch loss: 0.7083\n",
      "Epoch [17/1000], Batch loss: 0.7773\n",
      "Epoch [17/1000], Batch loss: 0.6520\n",
      "Epoch [17/1000], Batch loss: 0.7501\n",
      "Epoch [17/1000], Batch loss: 1.0284\n",
      "Epoch [17/1000], Batch loss: 0.7989\n",
      "Epoch [17/1000], Batch loss: 0.8595\n",
      "Epoch [17/1000], Batch loss: 0.8198\n",
      "Epoch [17/1000], Batch loss: 0.6620\n",
      "Epoch [17/1000], Batch loss: 0.7317\n",
      "Epoch [17/1000], Batch loss: 0.7200\n",
      "Epoch [17/1000], Batch loss: 0.5898\n",
      "Epoch [17/1000], Batch loss: 0.7082\n",
      "Epoch [17/1000], Batch loss: 0.7248\n",
      "Epoch [17/1000], Batch loss: 0.6692\n",
      "Epoch [17/1000], Batch loss: 0.7609\n",
      "Epoch [17/1000], Batch loss: 0.7569\n",
      "Epoch [17/1000], Batch loss: 0.7241\n",
      "Epoch [17/1000], Batch loss: 0.8844\n",
      "Epoch [17/1000], Batch loss: 0.5910\n",
      "Epoch [17/1000], Batch loss: 0.7436\n",
      "Epoch [17/1000], Batch loss: 0.6897\n",
      "Epoch [17/1000], Batch loss: 0.8366\n",
      "Epoch [17/1000], Batch loss: 0.6398\n",
      "Epoch [17/1000], Batch loss: 0.6371\n",
      "Epoch [17/1000], Batch loss: 0.6530\n",
      "Epoch [17/1000], Batch loss: 0.6183\n",
      "Train Accuracy 0.5140961857379768\n",
      "Test Accuracy 0.4411764705882353\n",
      "Epoch [18/1000], Batch loss: 0.7327\n",
      "Epoch [18/1000], Batch loss: 0.7159\n",
      "Epoch [18/1000], Batch loss: 0.8748\n",
      "Epoch [18/1000], Batch loss: 0.7539\n",
      "Epoch [18/1000], Batch loss: 0.8270\n",
      "Epoch [18/1000], Batch loss: 0.6012\n",
      "Epoch [18/1000], Batch loss: 0.6949\n",
      "Epoch [18/1000], Batch loss: 0.6611\n",
      "Epoch [18/1000], Batch loss: 0.6659\n",
      "Epoch [18/1000], Batch loss: 0.7116\n",
      "Epoch [18/1000], Batch loss: 0.6503\n",
      "Epoch [18/1000], Batch loss: 0.7219\n",
      "Epoch [18/1000], Batch loss: 0.5802\n",
      "Epoch [18/1000], Batch loss: 0.6222\n",
      "Epoch [18/1000], Batch loss: 0.8059\n",
      "Epoch [18/1000], Batch loss: 0.5996\n",
      "Epoch [18/1000], Batch loss: 0.9015\n",
      "Epoch [18/1000], Batch loss: 0.7455\n",
      "Epoch [18/1000], Batch loss: 0.6614\n",
      "Epoch [18/1000], Batch loss: 0.6332\n",
      "Epoch [18/1000], Batch loss: 0.6220\n",
      "Epoch [18/1000], Batch loss: 0.7702\n",
      "Epoch [18/1000], Batch loss: 0.6602\n",
      "Epoch [18/1000], Batch loss: 0.9369\n",
      "Epoch [18/1000], Batch loss: 0.8013\n",
      "Epoch [18/1000], Batch loss: 0.6315\n",
      "Epoch [18/1000], Batch loss: 0.7532\n",
      "Epoch [18/1000], Batch loss: 0.8259\n",
      "Epoch [18/1000], Batch loss: 0.6690\n",
      "Epoch [18/1000], Batch loss: 0.7235\n",
      "Epoch [18/1000], Batch loss: 0.6546\n",
      "Epoch [18/1000], Batch loss: 0.8326\n",
      "Epoch [18/1000], Batch loss: 0.7057\n",
      "Epoch [18/1000], Batch loss: 0.7722\n",
      "Epoch [18/1000], Batch loss: 0.5223\n",
      "Epoch [18/1000], Batch loss: 0.7053\n",
      "Epoch [18/1000], Batch loss: 0.7238\n",
      "Epoch [18/1000], Batch loss: 0.8827\n",
      "Epoch [18/1000], Batch loss: 0.7374\n",
      "Epoch [18/1000], Batch loss: 0.7419\n",
      "Epoch [18/1000], Batch loss: 0.6442\n",
      "Epoch [18/1000], Batch loss: 0.7348\n",
      "Epoch [18/1000], Batch loss: 0.5845\n",
      "Epoch [18/1000], Batch loss: 0.9175\n",
      "Epoch [18/1000], Batch loss: 0.8143\n",
      "Epoch [18/1000], Batch loss: 0.9299\n",
      "Epoch [18/1000], Batch loss: 0.7917\n",
      "Epoch [18/1000], Batch loss: 0.6528\n",
      "Epoch [18/1000], Batch loss: 0.5814\n",
      "Epoch [18/1000], Batch loss: 0.8511\n",
      "Epoch [18/1000], Batch loss: 0.7272\n",
      "Epoch [18/1000], Batch loss: 0.9907\n",
      "Epoch [18/1000], Batch loss: 0.7004\n",
      "Epoch [18/1000], Batch loss: 0.5414\n",
      "Epoch [18/1000], Batch loss: 0.7021\n",
      "Epoch [18/1000], Batch loss: 0.8280\n",
      "Epoch [18/1000], Batch loss: 0.8674\n",
      "Epoch [18/1000], Batch loss: 0.6376\n",
      "Epoch [18/1000], Batch loss: 0.8267\n",
      "Epoch [18/1000], Batch loss: 0.7982\n",
      "Epoch [18/1000], Batch loss: 0.7074\n",
      "Epoch [18/1000], Batch loss: 0.7510\n",
      "Epoch [18/1000], Batch loss: 0.5273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/1000], Batch loss: 0.5999\n",
      "Epoch [18/1000], Batch loss: 0.9889\n",
      "Epoch [18/1000], Batch loss: 0.7446\n",
      "Epoch [18/1000], Batch loss: 0.7324\n",
      "Epoch [18/1000], Batch loss: 0.5614\n",
      "Epoch [18/1000], Batch loss: 0.7250\n",
      "Epoch [18/1000], Batch loss: 0.6967\n",
      "Epoch [18/1000], Batch loss: 0.7404\n",
      "Epoch [18/1000], Batch loss: 0.8310\n",
      "Epoch [18/1000], Batch loss: 0.7766\n",
      "Epoch [18/1000], Batch loss: 0.7153\n",
      "Epoch [18/1000], Batch loss: 0.7628\n",
      "Epoch [18/1000], Batch loss: 0.8372\n",
      "Train Accuracy 0.5107794361525705\n",
      "Test Accuracy 0.47058823529411764\n",
      "Epoch [19/1000], Batch loss: 0.8065\n",
      "Epoch [19/1000], Batch loss: 0.9054\n",
      "Epoch [19/1000], Batch loss: 0.7654\n",
      "Epoch [19/1000], Batch loss: 0.7399\n",
      "Epoch [19/1000], Batch loss: 0.6689\n",
      "Epoch [19/1000], Batch loss: 0.8954\n",
      "Epoch [19/1000], Batch loss: 0.7902\n",
      "Epoch [19/1000], Batch loss: 0.8139\n",
      "Epoch [19/1000], Batch loss: 0.7695\n",
      "Epoch [19/1000], Batch loss: 0.9450\n",
      "Epoch [19/1000], Batch loss: 0.7120\n",
      "Epoch [19/1000], Batch loss: 0.9743\n",
      "Epoch [19/1000], Batch loss: 0.6658\n",
      "Epoch [19/1000], Batch loss: 0.6231\n",
      "Epoch [19/1000], Batch loss: 0.6589\n",
      "Epoch [19/1000], Batch loss: 0.6066\n",
      "Epoch [19/1000], Batch loss: 0.6729\n",
      "Epoch [19/1000], Batch loss: 0.6074\n",
      "Epoch [19/1000], Batch loss: 0.7499\n",
      "Epoch [19/1000], Batch loss: 0.7712\n",
      "Epoch [19/1000], Batch loss: 0.6795\n",
      "Epoch [19/1000], Batch loss: 1.0356\n",
      "Epoch [19/1000], Batch loss: 0.7911\n",
      "Epoch [19/1000], Batch loss: 0.9407\n",
      "Epoch [19/1000], Batch loss: 0.8221\n",
      "Epoch [19/1000], Batch loss: 0.5612\n",
      "Epoch [19/1000], Batch loss: 0.7073\n",
      "Epoch [19/1000], Batch loss: 0.6065\n",
      "Epoch [19/1000], Batch loss: 0.6805\n",
      "Epoch [19/1000], Batch loss: 0.5329\n",
      "Epoch [19/1000], Batch loss: 0.7338\n",
      "Epoch [19/1000], Batch loss: 0.6830\n",
      "Epoch [19/1000], Batch loss: 0.7552\n",
      "Epoch [19/1000], Batch loss: 0.7489\n",
      "Epoch [19/1000], Batch loss: 0.9579\n",
      "Epoch [19/1000], Batch loss: 0.6531\n",
      "Epoch [19/1000], Batch loss: 0.8936\n",
      "Epoch [19/1000], Batch loss: 0.6256\n",
      "Epoch [19/1000], Batch loss: 0.9129\n",
      "Epoch [19/1000], Batch loss: 0.7667\n",
      "Epoch [19/1000], Batch loss: 0.7078\n",
      "Epoch [19/1000], Batch loss: 0.7285\n",
      "Epoch [19/1000], Batch loss: 0.6769\n",
      "Epoch [19/1000], Batch loss: 0.5720\n",
      "Epoch [19/1000], Batch loss: 0.6583\n",
      "Epoch [19/1000], Batch loss: 0.5416\n",
      "Epoch [19/1000], Batch loss: 0.7134\n",
      "Epoch [19/1000], Batch loss: 0.7137\n",
      "Epoch [19/1000], Batch loss: 0.7739\n",
      "Epoch [19/1000], Batch loss: 0.7526\n",
      "Epoch [19/1000], Batch loss: 0.7457\n",
      "Epoch [19/1000], Batch loss: 0.8175\n",
      "Epoch [19/1000], Batch loss: 0.7390\n",
      "Epoch [19/1000], Batch loss: 0.6810\n",
      "Epoch [19/1000], Batch loss: 0.7041\n",
      "Epoch [19/1000], Batch loss: 0.7793\n",
      "Epoch [19/1000], Batch loss: 0.9059\n",
      "Epoch [19/1000], Batch loss: 0.6627\n",
      "Epoch [19/1000], Batch loss: 0.8219\n",
      "Epoch [19/1000], Batch loss: 0.7696\n",
      "Epoch [19/1000], Batch loss: 0.6325\n",
      "Epoch [19/1000], Batch loss: 0.5642\n",
      "Epoch [19/1000], Batch loss: 0.6266\n",
      "Epoch [19/1000], Batch loss: 0.6368\n",
      "Epoch [19/1000], Batch loss: 0.7084\n",
      "Epoch [19/1000], Batch loss: 0.7922\n",
      "Epoch [19/1000], Batch loss: 0.7353\n",
      "Epoch [19/1000], Batch loss: 0.5801\n",
      "Epoch [19/1000], Batch loss: 0.8189\n",
      "Epoch [19/1000], Batch loss: 0.6517\n",
      "Epoch [19/1000], Batch loss: 0.7567\n",
      "Epoch [19/1000], Batch loss: 0.6662\n",
      "Epoch [19/1000], Batch loss: 0.6708\n",
      "Epoch [19/1000], Batch loss: 0.4113\n",
      "Epoch [19/1000], Batch loss: 0.8569\n",
      "Epoch [19/1000], Batch loss: 0.6332\n",
      "Train Accuracy 0.5207296849087893\n",
      "Test Accuracy 0.45588235294117646\n",
      "Epoch [20/1000], Batch loss: 0.6457\n",
      "Epoch [20/1000], Batch loss: 0.7616\n",
      "Epoch [20/1000], Batch loss: 0.9078\n",
      "Epoch [20/1000], Batch loss: 0.6732\n",
      "Epoch [20/1000], Batch loss: 0.6313\n",
      "Epoch [20/1000], Batch loss: 0.6224\n",
      "Epoch [20/1000], Batch loss: 0.8027\n",
      "Epoch [20/1000], Batch loss: 0.9662\n",
      "Epoch [20/1000], Batch loss: 0.6516\n",
      "Epoch [20/1000], Batch loss: 0.6085\n",
      "Epoch [20/1000], Batch loss: 0.5461\n",
      "Epoch [20/1000], Batch loss: 0.7662\n",
      "Epoch [20/1000], Batch loss: 0.5438\n",
      "Epoch [20/1000], Batch loss: 0.6766\n",
      "Epoch [20/1000], Batch loss: 0.8413\n",
      "Epoch [20/1000], Batch loss: 0.5285\n",
      "Epoch [20/1000], Batch loss: 0.7201\n",
      "Epoch [20/1000], Batch loss: 0.7479\n",
      "Epoch [20/1000], Batch loss: 0.6846\n",
      "Epoch [20/1000], Batch loss: 0.6969\n",
      "Epoch [20/1000], Batch loss: 0.8022\n",
      "Epoch [20/1000], Batch loss: 0.7319\n",
      "Epoch [20/1000], Batch loss: 0.9544\n",
      "Epoch [20/1000], Batch loss: 0.6730\n",
      "Epoch [20/1000], Batch loss: 0.8237\n",
      "Epoch [20/1000], Batch loss: 0.6308\n",
      "Epoch [20/1000], Batch loss: 0.6411\n",
      "Epoch [20/1000], Batch loss: 0.7089\n",
      "Epoch [20/1000], Batch loss: 0.6577\n",
      "Epoch [20/1000], Batch loss: 0.8320\n",
      "Epoch [20/1000], Batch loss: 0.8004\n",
      "Epoch [20/1000], Batch loss: 0.7353\n",
      "Epoch [20/1000], Batch loss: 0.6253\n",
      "Epoch [20/1000], Batch loss: 0.7105\n",
      "Epoch [20/1000], Batch loss: 0.8140\n",
      "Epoch [20/1000], Batch loss: 0.7059\n",
      "Epoch [20/1000], Batch loss: 0.8264\n",
      "Epoch [20/1000], Batch loss: 0.6646\n",
      "Epoch [20/1000], Batch loss: 0.7087\n",
      "Epoch [20/1000], Batch loss: 0.7238\n",
      "Epoch [20/1000], Batch loss: 0.7721\n",
      "Epoch [20/1000], Batch loss: 0.8224\n",
      "Epoch [20/1000], Batch loss: 0.5751\n",
      "Epoch [20/1000], Batch loss: 0.5430\n",
      "Epoch [20/1000], Batch loss: 0.7236\n",
      "Epoch [20/1000], Batch loss: 0.8697\n",
      "Epoch [20/1000], Batch loss: 0.7108\n",
      "Epoch [20/1000], Batch loss: 0.7478\n",
      "Epoch [20/1000], Batch loss: 0.6262\n",
      "Epoch [20/1000], Batch loss: 0.8250\n",
      "Epoch [20/1000], Batch loss: 0.8355\n",
      "Epoch [20/1000], Batch loss: 0.6797\n",
      "Epoch [20/1000], Batch loss: 0.8480\n",
      "Epoch [20/1000], Batch loss: 0.6826\n",
      "Epoch [20/1000], Batch loss: 0.8291\n",
      "Epoch [20/1000], Batch loss: 0.7388\n",
      "Epoch [20/1000], Batch loss: 0.7294\n",
      "Epoch [20/1000], Batch loss: 0.6454\n",
      "Epoch [20/1000], Batch loss: 0.8855\n",
      "Epoch [20/1000], Batch loss: 0.8895\n",
      "Epoch [20/1000], Batch loss: 0.7318\n",
      "Epoch [20/1000], Batch loss: 0.6628\n",
      "Epoch [20/1000], Batch loss: 0.6476\n",
      "Epoch [20/1000], Batch loss: 0.5877\n",
      "Epoch [20/1000], Batch loss: 0.7105\n",
      "Epoch [20/1000], Batch loss: 0.6941\n",
      "Epoch [20/1000], Batch loss: 0.5097\n",
      "Epoch [20/1000], Batch loss: 0.7285\n",
      "Epoch [20/1000], Batch loss: 0.6370\n",
      "Epoch [20/1000], Batch loss: 0.9903\n",
      "Epoch [20/1000], Batch loss: 0.9217\n",
      "Epoch [20/1000], Batch loss: 0.7982\n",
      "Epoch [20/1000], Batch loss: 0.8550\n",
      "Epoch [20/1000], Batch loss: 0.8106\n",
      "Epoch [20/1000], Batch loss: 0.6735\n",
      "Epoch [20/1000], Batch loss: 0.5219\n",
      "Train Accuracy 0.494195688225539\n",
      "Test Accuracy 0.5147058823529411\n",
      "Epoch [21/1000], Batch loss: 0.6595\n",
      "Epoch [21/1000], Batch loss: 0.6497\n",
      "Epoch [21/1000], Batch loss: 0.7198\n",
      "Epoch [21/1000], Batch loss: 0.6001\n",
      "Epoch [21/1000], Batch loss: 0.6360\n",
      "Epoch [21/1000], Batch loss: 0.6362\n",
      "Epoch [21/1000], Batch loss: 0.6871\n",
      "Epoch [21/1000], Batch loss: 0.7407\n",
      "Epoch [21/1000], Batch loss: 0.6680\n",
      "Epoch [21/1000], Batch loss: 0.4915\n",
      "Epoch [21/1000], Batch loss: 0.6616\n",
      "Epoch [21/1000], Batch loss: 0.8418\n",
      "Epoch [21/1000], Batch loss: 0.7560\n",
      "Epoch [21/1000], Batch loss: 0.5268\n",
      "Epoch [21/1000], Batch loss: 0.6838\n",
      "Epoch [21/1000], Batch loss: 0.8000\n",
      "Epoch [21/1000], Batch loss: 0.6053\n",
      "Epoch [21/1000], Batch loss: 0.5506\n",
      "Epoch [21/1000], Batch loss: 0.7144\n",
      "Epoch [21/1000], Batch loss: 0.9248\n",
      "Epoch [21/1000], Batch loss: 0.6851\n",
      "Epoch [21/1000], Batch loss: 0.7624\n",
      "Epoch [21/1000], Batch loss: 0.8477\n",
      "Epoch [21/1000], Batch loss: 0.6697\n",
      "Epoch [21/1000], Batch loss: 0.6904\n",
      "Epoch [21/1000], Batch loss: 0.8237\n",
      "Epoch [21/1000], Batch loss: 0.7449\n",
      "Epoch [21/1000], Batch loss: 0.8378\n",
      "Epoch [21/1000], Batch loss: 0.8229\n",
      "Epoch [21/1000], Batch loss: 0.8728\n",
      "Epoch [21/1000], Batch loss: 0.6673\n",
      "Epoch [21/1000], Batch loss: 0.8791\n",
      "Epoch [21/1000], Batch loss: 0.7523\n",
      "Epoch [21/1000], Batch loss: 1.0224\n",
      "Epoch [21/1000], Batch loss: 0.6231\n",
      "Epoch [21/1000], Batch loss: 0.7566\n",
      "Epoch [21/1000], Batch loss: 0.6700\n",
      "Epoch [21/1000], Batch loss: 0.6436\n",
      "Epoch [21/1000], Batch loss: 0.7165\n",
      "Epoch [21/1000], Batch loss: 0.6739\n",
      "Epoch [21/1000], Batch loss: 0.7421\n",
      "Epoch [21/1000], Batch loss: 0.7193\n",
      "Epoch [21/1000], Batch loss: 0.5491\n",
      "Epoch [21/1000], Batch loss: 0.8537\n",
      "Epoch [21/1000], Batch loss: 0.8012\n",
      "Epoch [21/1000], Batch loss: 0.8107\n",
      "Epoch [21/1000], Batch loss: 0.6440\n",
      "Epoch [21/1000], Batch loss: 0.7506\n",
      "Epoch [21/1000], Batch loss: 0.7093\n",
      "Epoch [21/1000], Batch loss: 0.7588\n",
      "Epoch [21/1000], Batch loss: 0.7560\n",
      "Epoch [21/1000], Batch loss: 0.7110\n",
      "Epoch [21/1000], Batch loss: 0.7116\n",
      "Epoch [21/1000], Batch loss: 0.8556\n",
      "Epoch [21/1000], Batch loss: 0.6736\n",
      "Epoch [21/1000], Batch loss: 0.4987\n",
      "Epoch [21/1000], Batch loss: 0.7270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/1000], Batch loss: 1.0079\n",
      "Epoch [21/1000], Batch loss: 0.7535\n",
      "Epoch [21/1000], Batch loss: 0.3865\n",
      "Epoch [21/1000], Batch loss: 0.7703\n",
      "Epoch [21/1000], Batch loss: 0.7116\n",
      "Epoch [21/1000], Batch loss: 0.6788\n",
      "Epoch [21/1000], Batch loss: 0.6416\n",
      "Epoch [21/1000], Batch loss: 0.9220\n",
      "Epoch [21/1000], Batch loss: 0.6221\n",
      "Epoch [21/1000], Batch loss: 0.3985\n",
      "Epoch [21/1000], Batch loss: 0.7583\n",
      "Epoch [21/1000], Batch loss: 0.7640\n",
      "Epoch [21/1000], Batch loss: 0.7077\n",
      "Epoch [21/1000], Batch loss: 0.8654\n",
      "Epoch [21/1000], Batch loss: 0.7693\n",
      "Epoch [21/1000], Batch loss: 0.7015\n",
      "Epoch [21/1000], Batch loss: 0.8170\n",
      "Epoch [21/1000], Batch loss: 0.8033\n",
      "Epoch [21/1000], Batch loss: 0.8443\n",
      "Train Accuracy 0.4975124378109453\n",
      "Test Accuracy 0.4852941176470588\n",
      "Epoch [22/1000], Batch loss: 0.7574\n",
      "Epoch [22/1000], Batch loss: 0.7481\n",
      "Epoch [22/1000], Batch loss: 0.7413\n",
      "Epoch [22/1000], Batch loss: 0.5962\n",
      "Epoch [22/1000], Batch loss: 1.0277\n",
      "Epoch [22/1000], Batch loss: 0.7774\n",
      "Epoch [22/1000], Batch loss: 0.6786\n",
      "Epoch [22/1000], Batch loss: 0.8300\n",
      "Epoch [22/1000], Batch loss: 0.8050\n",
      "Epoch [22/1000], Batch loss: 0.6533\n",
      "Epoch [22/1000], Batch loss: 0.6000\n",
      "Epoch [22/1000], Batch loss: 0.7446\n",
      "Epoch [22/1000], Batch loss: 0.6988\n",
      "Epoch [22/1000], Batch loss: 0.6960\n",
      "Epoch [22/1000], Batch loss: 0.6369\n",
      "Epoch [22/1000], Batch loss: 0.6813\n",
      "Epoch [22/1000], Batch loss: 0.5756\n",
      "Epoch [22/1000], Batch loss: 0.7243\n",
      "Epoch [22/1000], Batch loss: 0.8710\n",
      "Epoch [22/1000], Batch loss: 0.9041\n",
      "Epoch [22/1000], Batch loss: 0.8337\n",
      "Epoch [22/1000], Batch loss: 0.7919\n",
      "Epoch [22/1000], Batch loss: 0.7342\n",
      "Epoch [22/1000], Batch loss: 0.7811\n",
      "Epoch [22/1000], Batch loss: 0.5241\n",
      "Epoch [22/1000], Batch loss: 0.8019\n",
      "Epoch [22/1000], Batch loss: 0.5946\n",
      "Epoch [22/1000], Batch loss: 0.6819\n",
      "Epoch [22/1000], Batch loss: 0.8258\n",
      "Epoch [22/1000], Batch loss: 0.6789\n",
      "Epoch [22/1000], Batch loss: 0.5691\n",
      "Epoch [22/1000], Batch loss: 0.6024\n",
      "Epoch [22/1000], Batch loss: 0.7811\n",
      "Epoch [22/1000], Batch loss: 0.8052\n",
      "Epoch [22/1000], Batch loss: 0.6288\n",
      "Epoch [22/1000], Batch loss: 0.9843\n",
      "Epoch [22/1000], Batch loss: 0.7070\n",
      "Epoch [22/1000], Batch loss: 0.7359\n",
      "Epoch [22/1000], Batch loss: 0.8329\n",
      "Epoch [22/1000], Batch loss: 0.6836\n",
      "Epoch [22/1000], Batch loss: 0.6710\n",
      "Epoch [22/1000], Batch loss: 0.7212\n",
      "Epoch [22/1000], Batch loss: 0.8855\n",
      "Epoch [22/1000], Batch loss: 0.6464\n",
      "Epoch [22/1000], Batch loss: 0.5921\n",
      "Epoch [22/1000], Batch loss: 0.5499\n",
      "Epoch [22/1000], Batch loss: 0.7409\n",
      "Epoch [22/1000], Batch loss: 0.7456\n",
      "Epoch [22/1000], Batch loss: 0.7492\n",
      "Epoch [22/1000], Batch loss: 0.7750\n",
      "Epoch [22/1000], Batch loss: 0.5357\n",
      "Epoch [22/1000], Batch loss: 0.5016\n",
      "Epoch [22/1000], Batch loss: 0.6913\n",
      "Epoch [22/1000], Batch loss: 0.7986\n",
      "Epoch [22/1000], Batch loss: 0.7054\n",
      "Epoch [22/1000], Batch loss: 0.7321\n",
      "Epoch [22/1000], Batch loss: 0.5922\n",
      "Epoch [22/1000], Batch loss: 0.6333\n",
      "Epoch [22/1000], Batch loss: 0.7452\n",
      "Epoch [22/1000], Batch loss: 0.8095\n",
      "Epoch [22/1000], Batch loss: 0.8092\n",
      "Epoch [22/1000], Batch loss: 0.5954\n",
      "Epoch [22/1000], Batch loss: 0.5855\n",
      "Epoch [22/1000], Batch loss: 0.8981\n",
      "Epoch [22/1000], Batch loss: 0.9615\n",
      "Epoch [22/1000], Batch loss: 0.6282\n",
      "Epoch [22/1000], Batch loss: 0.5826\n",
      "Epoch [22/1000], Batch loss: 0.8481\n",
      "Epoch [22/1000], Batch loss: 0.7229\n",
      "Epoch [22/1000], Batch loss: 0.8282\n",
      "Epoch [22/1000], Batch loss: 0.5543\n",
      "Epoch [22/1000], Batch loss: 0.6085\n",
      "Epoch [22/1000], Batch loss: 0.8465\n",
      "Epoch [22/1000], Batch loss: 0.6816\n",
      "Epoch [22/1000], Batch loss: 0.7094\n",
      "Epoch [22/1000], Batch loss: 0.5692\n",
      "Train Accuracy 0.5290215588723052\n",
      "Test Accuracy 0.5294117647058824\n",
      "Epoch [23/1000], Batch loss: 0.9337\n",
      "Epoch [23/1000], Batch loss: 0.8379\n",
      "Epoch [23/1000], Batch loss: 0.6388\n",
      "Epoch [23/1000], Batch loss: 0.5787\n",
      "Epoch [23/1000], Batch loss: 0.7574\n",
      "Epoch [23/1000], Batch loss: 0.8460\n",
      "Epoch [23/1000], Batch loss: 0.8308\n",
      "Epoch [23/1000], Batch loss: 0.5802\n",
      "Epoch [23/1000], Batch loss: 0.7631\n",
      "Epoch [23/1000], Batch loss: 0.5053\n",
      "Epoch [23/1000], Batch loss: 0.6156\n",
      "Epoch [23/1000], Batch loss: 0.7316\n",
      "Epoch [23/1000], Batch loss: 0.7642\n",
      "Epoch [23/1000], Batch loss: 0.6409\n",
      "Epoch [23/1000], Batch loss: 0.5858\n",
      "Epoch [23/1000], Batch loss: 0.5872\n",
      "Epoch [23/1000], Batch loss: 0.7591\n",
      "Epoch [23/1000], Batch loss: 0.5511\n",
      "Epoch [23/1000], Batch loss: 0.8776\n",
      "Epoch [23/1000], Batch loss: 0.7720\n",
      "Epoch [23/1000], Batch loss: 0.7367\n",
      "Epoch [23/1000], Batch loss: 0.5987\n",
      "Epoch [23/1000], Batch loss: 0.5315\n",
      "Epoch [23/1000], Batch loss: 0.7676\n",
      "Epoch [23/1000], Batch loss: 0.7366\n",
      "Epoch [23/1000], Batch loss: 0.7018\n",
      "Epoch [23/1000], Batch loss: 0.8074\n",
      "Epoch [23/1000], Batch loss: 0.5200\n",
      "Epoch [23/1000], Batch loss: 0.9510\n",
      "Epoch [23/1000], Batch loss: 0.7692\n",
      "Epoch [23/1000], Batch loss: 0.7752\n",
      "Epoch [23/1000], Batch loss: 0.5867\n",
      "Epoch [23/1000], Batch loss: 0.8263\n",
      "Epoch [23/1000], Batch loss: 0.8777\n",
      "Epoch [23/1000], Batch loss: 0.8597\n",
      "Epoch [23/1000], Batch loss: 0.8814\n",
      "Epoch [23/1000], Batch loss: 0.6680\n",
      "Epoch [23/1000], Batch loss: 0.6567\n",
      "Epoch [23/1000], Batch loss: 0.7012\n",
      "Epoch [23/1000], Batch loss: 0.7948\n",
      "Epoch [23/1000], Batch loss: 0.5941\n",
      "Epoch [23/1000], Batch loss: 0.6748\n",
      "Epoch [23/1000], Batch loss: 0.5424\n",
      "Epoch [23/1000], Batch loss: 0.5623\n",
      "Epoch [23/1000], Batch loss: 0.7646\n",
      "Epoch [23/1000], Batch loss: 0.6545\n",
      "Epoch [23/1000], Batch loss: 0.6692\n",
      "Epoch [23/1000], Batch loss: 0.7443\n",
      "Epoch [23/1000], Batch loss: 0.5907\n",
      "Epoch [23/1000], Batch loss: 0.6108\n",
      "Epoch [23/1000], Batch loss: 0.6118\n",
      "Epoch [23/1000], Batch loss: 0.8423\n",
      "Epoch [23/1000], Batch loss: 0.7038\n",
      "Epoch [23/1000], Batch loss: 0.8438\n",
      "Epoch [23/1000], Batch loss: 0.6514\n",
      "Epoch [23/1000], Batch loss: 0.6603\n",
      "Epoch [23/1000], Batch loss: 0.6869\n",
      "Epoch [23/1000], Batch loss: 0.5372\n",
      "Epoch [23/1000], Batch loss: 0.7287\n",
      "Epoch [23/1000], Batch loss: 0.6738\n",
      "Epoch [23/1000], Batch loss: 0.6608\n",
      "Epoch [23/1000], Batch loss: 0.7938\n",
      "Epoch [23/1000], Batch loss: 0.8366\n",
      "Epoch [23/1000], Batch loss: 0.8170\n",
      "Epoch [23/1000], Batch loss: 0.9634\n",
      "Epoch [23/1000], Batch loss: 0.6779\n",
      "Epoch [23/1000], Batch loss: 0.7535\n",
      "Epoch [23/1000], Batch loss: 0.8095\n",
      "Epoch [23/1000], Batch loss: 0.8071\n",
      "Epoch [23/1000], Batch loss: 0.8534\n",
      "Epoch [23/1000], Batch loss: 0.6759\n",
      "Epoch [23/1000], Batch loss: 0.7890\n",
      "Epoch [23/1000], Batch loss: 0.9237\n",
      "Epoch [23/1000], Batch loss: 0.5615\n",
      "Epoch [23/1000], Batch loss: 0.9726\n",
      "Epoch [23/1000], Batch loss: 0.6619\n",
      "Train Accuracy 0.538971807628524\n",
      "Test Accuracy 0.4264705882352941\n",
      "Epoch [24/1000], Batch loss: 0.7461\n",
      "Epoch [24/1000], Batch loss: 0.6857\n",
      "Epoch [24/1000], Batch loss: 0.6604\n",
      "Epoch [24/1000], Batch loss: 0.9047\n",
      "Epoch [24/1000], Batch loss: 0.7353\n",
      "Epoch [24/1000], Batch loss: 0.8269\n",
      "Epoch [24/1000], Batch loss: 0.5208\n",
      "Epoch [24/1000], Batch loss: 0.6219\n",
      "Epoch [24/1000], Batch loss: 0.6392\n",
      "Epoch [24/1000], Batch loss: 0.8614\n",
      "Epoch [24/1000], Batch loss: 0.6909\n",
      "Epoch [24/1000], Batch loss: 0.6347\n",
      "Epoch [24/1000], Batch loss: 0.6396\n",
      "Epoch [24/1000], Batch loss: 0.6685\n",
      "Epoch [24/1000], Batch loss: 0.6641\n",
      "Epoch [24/1000], Batch loss: 0.7008\n",
      "Epoch [24/1000], Batch loss: 0.9796\n",
      "Epoch [24/1000], Batch loss: 0.7278\n",
      "Epoch [24/1000], Batch loss: 0.7734\n",
      "Epoch [24/1000], Batch loss: 0.7460\n",
      "Epoch [24/1000], Batch loss: 0.6916\n",
      "Epoch [24/1000], Batch loss: 0.6859\n",
      "Epoch [24/1000], Batch loss: 0.7071\n",
      "Epoch [24/1000], Batch loss: 0.8619\n",
      "Epoch [24/1000], Batch loss: 0.9050\n",
      "Epoch [24/1000], Batch loss: 0.9285\n",
      "Epoch [24/1000], Batch loss: 0.6864\n",
      "Epoch [24/1000], Batch loss: 0.8341\n",
      "Epoch [24/1000], Batch loss: 0.8093\n",
      "Epoch [24/1000], Batch loss: 1.0332\n",
      "Epoch [24/1000], Batch loss: 0.6403\n",
      "Epoch [24/1000], Batch loss: 0.6641\n",
      "Epoch [24/1000], Batch loss: 0.7203\n",
      "Epoch [24/1000], Batch loss: 0.5412\n",
      "Epoch [24/1000], Batch loss: 1.0564\n",
      "Epoch [24/1000], Batch loss: 0.6361\n",
      "Epoch [24/1000], Batch loss: 0.8071\n",
      "Epoch [24/1000], Batch loss: 0.6749\n",
      "Epoch [24/1000], Batch loss: 0.6796\n",
      "Epoch [24/1000], Batch loss: 0.7382\n",
      "Epoch [24/1000], Batch loss: 0.6988\n",
      "Epoch [24/1000], Batch loss: 0.6427\n",
      "Epoch [24/1000], Batch loss: 0.5726\n",
      "Epoch [24/1000], Batch loss: 0.8885\n",
      "Epoch [24/1000], Batch loss: 0.4845\n",
      "Epoch [24/1000], Batch loss: 0.6611\n",
      "Epoch [24/1000], Batch loss: 0.7091\n",
      "Epoch [24/1000], Batch loss: 0.7125\n",
      "Epoch [24/1000], Batch loss: 0.8783\n",
      "Epoch [24/1000], Batch loss: 0.8993\n",
      "Epoch [24/1000], Batch loss: 0.6643\n",
      "Epoch [24/1000], Batch loss: 0.6726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/1000], Batch loss: 0.8005\n",
      "Epoch [24/1000], Batch loss: 0.7418\n",
      "Epoch [24/1000], Batch loss: 0.8171\n",
      "Epoch [24/1000], Batch loss: 0.8618\n",
      "Epoch [24/1000], Batch loss: 0.6696\n",
      "Epoch [24/1000], Batch loss: 0.6305\n",
      "Epoch [24/1000], Batch loss: 0.6582\n",
      "Epoch [24/1000], Batch loss: 0.7166\n",
      "Epoch [24/1000], Batch loss: 0.7720\n",
      "Epoch [24/1000], Batch loss: 0.8896\n",
      "Epoch [24/1000], Batch loss: 0.5957\n",
      "Epoch [24/1000], Batch loss: 0.8589\n",
      "Epoch [24/1000], Batch loss: 0.8998\n",
      "Epoch [24/1000], Batch loss: 0.8086\n",
      "Epoch [24/1000], Batch loss: 0.6873\n",
      "Epoch [24/1000], Batch loss: 0.7860\n",
      "Epoch [24/1000], Batch loss: 0.7127\n",
      "Epoch [24/1000], Batch loss: 0.7585\n",
      "Epoch [24/1000], Batch loss: 0.6735\n",
      "Epoch [24/1000], Batch loss: 0.6662\n",
      "Epoch [24/1000], Batch loss: 0.6048\n",
      "Epoch [24/1000], Batch loss: 0.5184\n",
      "Epoch [24/1000], Batch loss: 0.7643\n",
      "Epoch [24/1000], Batch loss: 0.7438\n",
      "Train Accuracy 0.5074626865671642\n",
      "Test Accuracy 0.45588235294117646\n",
      "Epoch [25/1000], Batch loss: 0.8724\n",
      "Epoch [25/1000], Batch loss: 0.7338\n",
      "Epoch [25/1000], Batch loss: 0.6620\n",
      "Epoch [25/1000], Batch loss: 0.8108\n",
      "Epoch [25/1000], Batch loss: 0.7549\n",
      "Epoch [25/1000], Batch loss: 0.8085\n",
      "Epoch [25/1000], Batch loss: 0.7852\n",
      "Epoch [25/1000], Batch loss: 0.6220\n",
      "Epoch [25/1000], Batch loss: 0.7811\n",
      "Epoch [25/1000], Batch loss: 0.6619\n",
      "Epoch [25/1000], Batch loss: 0.8882\n",
      "Epoch [25/1000], Batch loss: 0.7870\n",
      "Epoch [25/1000], Batch loss: 0.9048\n",
      "Epoch [25/1000], Batch loss: 0.6118\n",
      "Epoch [25/1000], Batch loss: 0.5412\n",
      "Epoch [25/1000], Batch loss: 0.8703\n",
      "Epoch [25/1000], Batch loss: 0.7156\n",
      "Epoch [25/1000], Batch loss: 0.6297\n",
      "Epoch [25/1000], Batch loss: 0.6765\n",
      "Epoch [25/1000], Batch loss: 0.6523\n",
      "Epoch [25/1000], Batch loss: 1.1497\n",
      "Epoch [25/1000], Batch loss: 0.8544\n",
      "Epoch [25/1000], Batch loss: 0.8458\n",
      "Epoch [25/1000], Batch loss: 0.8428\n",
      "Epoch [25/1000], Batch loss: 0.6486\n",
      "Epoch [25/1000], Batch loss: 0.6834\n",
      "Epoch [25/1000], Batch loss: 0.7453\n",
      "Epoch [25/1000], Batch loss: 0.6068\n",
      "Epoch [25/1000], Batch loss: 0.7966\n",
      "Epoch [25/1000], Batch loss: 0.8958\n",
      "Epoch [25/1000], Batch loss: 0.8245\n",
      "Epoch [25/1000], Batch loss: 0.8631\n",
      "Epoch [25/1000], Batch loss: 0.7526\n",
      "Epoch [25/1000], Batch loss: 0.7258\n",
      "Epoch [25/1000], Batch loss: 0.7314\n",
      "Epoch [25/1000], Batch loss: 0.7721\n",
      "Epoch [25/1000], Batch loss: 0.9465\n",
      "Epoch [25/1000], Batch loss: 0.7955\n",
      "Epoch [25/1000], Batch loss: 0.6625\n",
      "Epoch [25/1000], Batch loss: 0.5425\n",
      "Epoch [25/1000], Batch loss: 0.6763\n",
      "Epoch [25/1000], Batch loss: 0.8116\n",
      "Epoch [25/1000], Batch loss: 0.6425\n",
      "Epoch [25/1000], Batch loss: 0.8198\n",
      "Epoch [25/1000], Batch loss: 0.6589\n",
      "Epoch [25/1000], Batch loss: 0.9394\n",
      "Epoch [25/1000], Batch loss: 0.7417\n",
      "Epoch [25/1000], Batch loss: 0.7305\n",
      "Epoch [25/1000], Batch loss: 0.6097\n",
      "Epoch [25/1000], Batch loss: 0.7096\n",
      "Epoch [25/1000], Batch loss: 0.8140\n",
      "Epoch [25/1000], Batch loss: 0.7234\n",
      "Epoch [25/1000], Batch loss: 0.7254\n",
      "Epoch [25/1000], Batch loss: 0.7750\n",
      "Epoch [25/1000], Batch loss: 0.6848\n",
      "Epoch [25/1000], Batch loss: 0.5781\n",
      "Epoch [25/1000], Batch loss: 0.7011\n",
      "Epoch [25/1000], Batch loss: 0.7386\n",
      "Epoch [25/1000], Batch loss: 0.8068\n",
      "Epoch [25/1000], Batch loss: 0.6238\n",
      "Epoch [25/1000], Batch loss: 0.6968\n",
      "Epoch [25/1000], Batch loss: 0.7608\n",
      "Epoch [25/1000], Batch loss: 0.6795\n",
      "Epoch [25/1000], Batch loss: 0.7629\n",
      "Epoch [25/1000], Batch loss: 0.8296\n",
      "Epoch [25/1000], Batch loss: 0.6666\n",
      "Epoch [25/1000], Batch loss: 0.5705\n",
      "Epoch [25/1000], Batch loss: 0.6493\n",
      "Epoch [25/1000], Batch loss: 0.6862\n",
      "Epoch [25/1000], Batch loss: 0.6956\n",
      "Epoch [25/1000], Batch loss: 0.7799\n",
      "Epoch [25/1000], Batch loss: 0.7240\n",
      "Epoch [25/1000], Batch loss: 0.9404\n",
      "Epoch [25/1000], Batch loss: 0.6214\n",
      "Epoch [25/1000], Batch loss: 0.7270\n",
      "Epoch [25/1000], Batch loss: 0.6138\n",
      "Train Accuracy 0.5041459369817579\n",
      "Test Accuracy 0.5735294117647058\n",
      "Epoch [26/1000], Batch loss: 0.8013\n",
      "Epoch [26/1000], Batch loss: 0.5401\n",
      "Epoch [26/1000], Batch loss: 0.8160\n",
      "Epoch [26/1000], Batch loss: 0.6795\n",
      "Epoch [26/1000], Batch loss: 0.6954\n",
      "Epoch [26/1000], Batch loss: 0.5898\n",
      "Epoch [26/1000], Batch loss: 0.5817\n",
      "Epoch [26/1000], Batch loss: 0.7976\n",
      "Epoch [26/1000], Batch loss: 0.5435\n",
      "Epoch [26/1000], Batch loss: 0.7608\n",
      "Epoch [26/1000], Batch loss: 0.7837\n",
      "Epoch [26/1000], Batch loss: 0.6513\n",
      "Epoch [26/1000], Batch loss: 0.7480\n",
      "Epoch [26/1000], Batch loss: 0.5264\n",
      "Epoch [26/1000], Batch loss: 0.6428\n",
      "Epoch [26/1000], Batch loss: 0.8943\n",
      "Epoch [26/1000], Batch loss: 0.8351\n",
      "Epoch [26/1000], Batch loss: 0.6990\n",
      "Epoch [26/1000], Batch loss: 0.6334\n",
      "Epoch [26/1000], Batch loss: 0.5541\n",
      "Epoch [26/1000], Batch loss: 0.6700\n",
      "Epoch [26/1000], Batch loss: 0.5762\n",
      "Epoch [26/1000], Batch loss: 0.8823\n",
      "Epoch [26/1000], Batch loss: 1.1290\n",
      "Epoch [26/1000], Batch loss: 0.6712\n",
      "Epoch [26/1000], Batch loss: 0.6315\n",
      "Epoch [26/1000], Batch loss: 0.6358\n",
      "Epoch [26/1000], Batch loss: 0.6379\n",
      "Epoch [26/1000], Batch loss: 0.6066\n",
      "Epoch [26/1000], Batch loss: 0.6748\n",
      "Epoch [26/1000], Batch loss: 0.8249\n",
      "Epoch [26/1000], Batch loss: 0.6791\n",
      "Epoch [26/1000], Batch loss: 0.7153\n",
      "Epoch [26/1000], Batch loss: 0.7283\n",
      "Epoch [26/1000], Batch loss: 0.8370\n",
      "Epoch [26/1000], Batch loss: 0.7783\n",
      "Epoch [26/1000], Batch loss: 0.5557\n",
      "Epoch [26/1000], Batch loss: 0.7132\n",
      "Epoch [26/1000], Batch loss: 0.7600\n",
      "Epoch [26/1000], Batch loss: 0.7020\n",
      "Epoch [26/1000], Batch loss: 0.4481\n",
      "Epoch [26/1000], Batch loss: 0.7820\n",
      "Epoch [26/1000], Batch loss: 0.6982\n",
      "Epoch [26/1000], Batch loss: 0.8949\n",
      "Epoch [26/1000], Batch loss: 0.6429\n",
      "Epoch [26/1000], Batch loss: 0.6433\n",
      "Epoch [26/1000], Batch loss: 0.6639\n",
      "Epoch [26/1000], Batch loss: 0.7410\n",
      "Epoch [26/1000], Batch loss: 0.8580\n",
      "Epoch [26/1000], Batch loss: 0.7752\n",
      "Epoch [26/1000], Batch loss: 0.6618\n",
      "Epoch [26/1000], Batch loss: 0.7480\n",
      "Epoch [26/1000], Batch loss: 0.7596\n",
      "Epoch [26/1000], Batch loss: 0.8671\n",
      "Epoch [26/1000], Batch loss: 0.7165\n",
      "Epoch [26/1000], Batch loss: 0.6998\n",
      "Epoch [26/1000], Batch loss: 0.6606\n",
      "Epoch [26/1000], Batch loss: 0.8672\n",
      "Epoch [26/1000], Batch loss: 0.7890\n",
      "Epoch [26/1000], Batch loss: 0.8010\n",
      "Epoch [26/1000], Batch loss: 0.5745\n",
      "Epoch [26/1000], Batch loss: 0.5338\n",
      "Epoch [26/1000], Batch loss: 0.6487\n",
      "Epoch [26/1000], Batch loss: 0.7710\n",
      "Epoch [26/1000], Batch loss: 0.6825\n",
      "Epoch [26/1000], Batch loss: 0.5500\n",
      "Epoch [26/1000], Batch loss: 0.8827\n",
      "Epoch [26/1000], Batch loss: 0.8849\n",
      "Epoch [26/1000], Batch loss: 0.6717\n",
      "Epoch [26/1000], Batch loss: 0.6093\n",
      "Epoch [26/1000], Batch loss: 0.6942\n",
      "Epoch [26/1000], Batch loss: 0.7799\n",
      "Epoch [26/1000], Batch loss: 0.6423\n",
      "Epoch [26/1000], Batch loss: 1.0057\n",
      "Epoch [26/1000], Batch loss: 0.7238\n",
      "Epoch [26/1000], Batch loss: 0.5959\n",
      "Train Accuracy 0.5538971807628524\n",
      "Test Accuracy 0.4852941176470588\n",
      "Epoch [27/1000], Batch loss: 0.7416\n",
      "Epoch [27/1000], Batch loss: 0.7055\n",
      "Epoch [27/1000], Batch loss: 0.8766\n",
      "Epoch [27/1000], Batch loss: 0.8380\n",
      "Epoch [27/1000], Batch loss: 0.8870\n",
      "Epoch [27/1000], Batch loss: 0.7119\n",
      "Epoch [27/1000], Batch loss: 0.5435\n",
      "Epoch [27/1000], Batch loss: 0.7951\n",
      "Epoch [27/1000], Batch loss: 0.7740\n",
      "Epoch [27/1000], Batch loss: 0.6992\n",
      "Epoch [27/1000], Batch loss: 0.5541\n",
      "Epoch [27/1000], Batch loss: 0.8572\n",
      "Epoch [27/1000], Batch loss: 0.7055\n",
      "Epoch [27/1000], Batch loss: 0.6937\n",
      "Epoch [27/1000], Batch loss: 0.8564\n",
      "Epoch [27/1000], Batch loss: 0.9443\n",
      "Epoch [27/1000], Batch loss: 0.8195\n",
      "Epoch [27/1000], Batch loss: 0.7318\n",
      "Epoch [27/1000], Batch loss: 0.9464\n",
      "Epoch [27/1000], Batch loss: 0.7627\n",
      "Epoch [27/1000], Batch loss: 0.9276\n",
      "Epoch [27/1000], Batch loss: 0.6538\n",
      "Epoch [27/1000], Batch loss: 0.8327\n",
      "Epoch [27/1000], Batch loss: 0.9714\n",
      "Epoch [27/1000], Batch loss: 0.8519\n",
      "Epoch [27/1000], Batch loss: 0.5961\n",
      "Epoch [27/1000], Batch loss: 0.8129\n",
      "Epoch [27/1000], Batch loss: 0.6326\n",
      "Epoch [27/1000], Batch loss: 0.6192\n",
      "Epoch [27/1000], Batch loss: 0.5916\n",
      "Epoch [27/1000], Batch loss: 0.6571\n",
      "Epoch [27/1000], Batch loss: 0.8732\n",
      "Epoch [27/1000], Batch loss: 0.5967\n",
      "Epoch [27/1000], Batch loss: 0.5345\n",
      "Epoch [27/1000], Batch loss: 0.9685\n",
      "Epoch [27/1000], Batch loss: 0.7213\n",
      "Epoch [27/1000], Batch loss: 0.6909\n",
      "Epoch [27/1000], Batch loss: 0.6570\n",
      "Epoch [27/1000], Batch loss: 0.8591\n",
      "Epoch [27/1000], Batch loss: 0.7264\n",
      "Epoch [27/1000], Batch loss: 0.6774\n",
      "Epoch [27/1000], Batch loss: 0.7002\n",
      "Epoch [27/1000], Batch loss: 0.8936\n",
      "Epoch [27/1000], Batch loss: 0.7501\n",
      "Epoch [27/1000], Batch loss: 0.6893\n",
      "Epoch [27/1000], Batch loss: 0.9787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/1000], Batch loss: 0.6444\n",
      "Epoch [27/1000], Batch loss: 0.6675\n",
      "Epoch [27/1000], Batch loss: 0.5823\n",
      "Epoch [27/1000], Batch loss: 0.7040\n",
      "Epoch [27/1000], Batch loss: 0.8145\n",
      "Epoch [27/1000], Batch loss: 0.6870\n",
      "Epoch [27/1000], Batch loss: 0.6291\n",
      "Epoch [27/1000], Batch loss: 0.8609\n",
      "Epoch [27/1000], Batch loss: 0.5494\n",
      "Epoch [27/1000], Batch loss: 0.8927\n",
      "Epoch [27/1000], Batch loss: 0.6629\n",
      "Epoch [27/1000], Batch loss: 0.9696\n",
      "Epoch [27/1000], Batch loss: 0.7731\n",
      "Epoch [27/1000], Batch loss: 0.6186\n",
      "Epoch [27/1000], Batch loss: 0.5671\n",
      "Epoch [27/1000], Batch loss: 0.8130\n",
      "Epoch [27/1000], Batch loss: 0.8417\n",
      "Epoch [27/1000], Batch loss: 0.8298\n",
      "Epoch [27/1000], Batch loss: 0.7658\n",
      "Epoch [27/1000], Batch loss: 0.8944\n",
      "Epoch [27/1000], Batch loss: 0.5706\n",
      "Epoch [27/1000], Batch loss: 0.7297\n",
      "Epoch [27/1000], Batch loss: 0.6755\n",
      "Epoch [27/1000], Batch loss: 0.7344\n",
      "Epoch [27/1000], Batch loss: 0.5420\n",
      "Epoch [27/1000], Batch loss: 0.6965\n",
      "Epoch [27/1000], Batch loss: 0.8174\n",
      "Epoch [27/1000], Batch loss: 0.5738\n",
      "Epoch [27/1000], Batch loss: 0.9607\n",
      "Epoch [27/1000], Batch loss: 0.8499\n",
      "Train Accuracy 0.49087893864013266\n",
      "Test Accuracy 0.6470588235294118\n",
      "Epoch [28/1000], Batch loss: 0.6303\n",
      "Epoch [28/1000], Batch loss: 0.9253\n",
      "Epoch [28/1000], Batch loss: 0.6517\n",
      "Epoch [28/1000], Batch loss: 0.6432\n",
      "Epoch [28/1000], Batch loss: 0.9833\n",
      "Epoch [28/1000], Batch loss: 0.6778\n",
      "Epoch [28/1000], Batch loss: 0.7615\n",
      "Epoch [28/1000], Batch loss: 0.7180\n",
      "Epoch [28/1000], Batch loss: 0.7491\n",
      "Epoch [28/1000], Batch loss: 0.7794\n",
      "Epoch [28/1000], Batch loss: 0.7173\n",
      "Epoch [28/1000], Batch loss: 0.5793\n",
      "Epoch [28/1000], Batch loss: 0.8013\n",
      "Epoch [28/1000], Batch loss: 1.0699\n",
      "Epoch [28/1000], Batch loss: 0.5903\n",
      "Epoch [28/1000], Batch loss: 0.4387\n",
      "Epoch [28/1000], Batch loss: 0.9669\n",
      "Epoch [28/1000], Batch loss: 0.8133\n",
      "Epoch [28/1000], Batch loss: 0.8898\n",
      "Epoch [28/1000], Batch loss: 0.7529\n",
      "Epoch [28/1000], Batch loss: 0.8943\n",
      "Epoch [28/1000], Batch loss: 0.6315\n",
      "Epoch [28/1000], Batch loss: 0.7710\n",
      "Epoch [28/1000], Batch loss: 0.6945\n",
      "Epoch [28/1000], Batch loss: 0.8395\n",
      "Epoch [28/1000], Batch loss: 0.7294\n",
      "Epoch [28/1000], Batch loss: 0.7468\n",
      "Epoch [28/1000], Batch loss: 0.6902\n",
      "Epoch [28/1000], Batch loss: 0.6825\n",
      "Epoch [28/1000], Batch loss: 0.6680\n",
      "Epoch [28/1000], Batch loss: 0.5799\n",
      "Epoch [28/1000], Batch loss: 0.7905\n",
      "Epoch [28/1000], Batch loss: 0.7561\n",
      "Epoch [28/1000], Batch loss: 0.7611\n",
      "Epoch [28/1000], Batch loss: 0.6546\n",
      "Epoch [28/1000], Batch loss: 0.8374\n",
      "Epoch [28/1000], Batch loss: 0.7373\n",
      "Epoch [28/1000], Batch loss: 0.7673\n",
      "Epoch [28/1000], Batch loss: 0.5942\n",
      "Epoch [28/1000], Batch loss: 0.6443\n",
      "Epoch [28/1000], Batch loss: 0.7287\n",
      "Epoch [28/1000], Batch loss: 0.9690\n",
      "Epoch [28/1000], Batch loss: 0.5317\n",
      "Epoch [28/1000], Batch loss: 0.5963\n",
      "Epoch [28/1000], Batch loss: 0.7046\n",
      "Epoch [28/1000], Batch loss: 0.7328\n",
      "Epoch [28/1000], Batch loss: 0.6066\n",
      "Epoch [28/1000], Batch loss: 0.6754\n",
      "Epoch [28/1000], Batch loss: 0.7782\n",
      "Epoch [28/1000], Batch loss: 0.6746\n",
      "Epoch [28/1000], Batch loss: 0.7973\n",
      "Epoch [28/1000], Batch loss: 0.7415\n",
      "Epoch [28/1000], Batch loss: 0.6839\n",
      "Epoch [28/1000], Batch loss: 0.6341\n",
      "Epoch [28/1000], Batch loss: 0.9683\n",
      "Epoch [28/1000], Batch loss: 0.8611\n",
      "Epoch [28/1000], Batch loss: 0.6277\n",
      "Epoch [28/1000], Batch loss: 0.8077\n",
      "Epoch [28/1000], Batch loss: 0.8356\n",
      "Epoch [28/1000], Batch loss: 0.7177\n",
      "Epoch [28/1000], Batch loss: 0.6177\n",
      "Epoch [28/1000], Batch loss: 0.7984\n",
      "Epoch [28/1000], Batch loss: 0.7774\n",
      "Epoch [28/1000], Batch loss: 0.8267\n",
      "Epoch [28/1000], Batch loss: 1.0097\n",
      "Epoch [28/1000], Batch loss: 0.8094\n",
      "Epoch [28/1000], Batch loss: 0.8542\n",
      "Epoch [28/1000], Batch loss: 0.7531\n",
      "Epoch [28/1000], Batch loss: 0.6216\n",
      "Epoch [28/1000], Batch loss: 0.8974\n",
      "Epoch [28/1000], Batch loss: 0.8074\n",
      "Epoch [28/1000], Batch loss: 0.4827\n",
      "Epoch [28/1000], Batch loss: 0.6334\n",
      "Epoch [28/1000], Batch loss: 0.7439\n",
      "Epoch [28/1000], Batch loss: 0.5769\n",
      "Epoch [28/1000], Batch loss: 0.6027\n",
      "Train Accuracy 0.5074626865671642\n",
      "Test Accuracy 0.5294117647058824\n",
      "Epoch [29/1000], Batch loss: 0.8238\n",
      "Epoch [29/1000], Batch loss: 0.7191\n",
      "Epoch [29/1000], Batch loss: 0.7613\n",
      "Epoch [29/1000], Batch loss: 0.7085\n",
      "Epoch [29/1000], Batch loss: 0.8354\n",
      "Epoch [29/1000], Batch loss: 1.0329\n",
      "Epoch [29/1000], Batch loss: 0.8094\n",
      "Epoch [29/1000], Batch loss: 0.5796\n",
      "Epoch [29/1000], Batch loss: 0.5570\n",
      "Epoch [29/1000], Batch loss: 0.9186\n",
      "Epoch [29/1000], Batch loss: 0.7701\n",
      "Epoch [29/1000], Batch loss: 0.7418\n",
      "Epoch [29/1000], Batch loss: 0.6833\n",
      "Epoch [29/1000], Batch loss: 0.7203\n",
      "Epoch [29/1000], Batch loss: 0.7184\n",
      "Epoch [29/1000], Batch loss: 0.8919\n",
      "Epoch [29/1000], Batch loss: 0.8654\n",
      "Epoch [29/1000], Batch loss: 0.7628\n",
      "Epoch [29/1000], Batch loss: 0.5842\n",
      "Epoch [29/1000], Batch loss: 0.8355\n",
      "Epoch [29/1000], Batch loss: 0.6731\n",
      "Epoch [29/1000], Batch loss: 0.6754\n",
      "Epoch [29/1000], Batch loss: 0.7382\n",
      "Epoch [29/1000], Batch loss: 0.7771\n",
      "Epoch [29/1000], Batch loss: 0.6537\n",
      "Epoch [29/1000], Batch loss: 0.6739\n",
      "Epoch [29/1000], Batch loss: 0.6409\n",
      "Epoch [29/1000], Batch loss: 1.0766\n",
      "Epoch [29/1000], Batch loss: 0.8619\n",
      "Epoch [29/1000], Batch loss: 0.7522\n",
      "Epoch [29/1000], Batch loss: 0.7475\n",
      "Epoch [29/1000], Batch loss: 0.7451\n",
      "Epoch [29/1000], Batch loss: 0.5683\n",
      "Epoch [29/1000], Batch loss: 0.7502\n",
      "Epoch [29/1000], Batch loss: 0.6120\n",
      "Epoch [29/1000], Batch loss: 0.7400\n",
      "Epoch [29/1000], Batch loss: 0.6076\n",
      "Epoch [29/1000], Batch loss: 0.5870\n",
      "Epoch [29/1000], Batch loss: 0.5833\n",
      "Epoch [29/1000], Batch loss: 0.6128\n",
      "Epoch [29/1000], Batch loss: 0.6944\n",
      "Epoch [29/1000], Batch loss: 0.5738\n",
      "Epoch [29/1000], Batch loss: 0.7004\n",
      "Epoch [29/1000], Batch loss: 0.7325\n",
      "Epoch [29/1000], Batch loss: 0.6142\n",
      "Epoch [29/1000], Batch loss: 0.7701\n",
      "Epoch [29/1000], Batch loss: 0.6631\n",
      "Epoch [29/1000], Batch loss: 0.4341\n",
      "Epoch [29/1000], Batch loss: 0.7571\n",
      "Epoch [29/1000], Batch loss: 0.8321\n",
      "Epoch [29/1000], Batch loss: 0.6388\n",
      "Epoch [29/1000], Batch loss: 0.7108\n",
      "Epoch [29/1000], Batch loss: 0.7895\n",
      "Epoch [29/1000], Batch loss: 0.7295\n",
      "Epoch [29/1000], Batch loss: 0.6570\n",
      "Epoch [29/1000], Batch loss: 0.5589\n",
      "Epoch [29/1000], Batch loss: 0.9526\n",
      "Epoch [29/1000], Batch loss: 0.7673\n",
      "Epoch [29/1000], Batch loss: 0.7187\n",
      "Epoch [29/1000], Batch loss: 0.7005\n",
      "Epoch [29/1000], Batch loss: 0.7496\n",
      "Epoch [29/1000], Batch loss: 0.8852\n",
      "Epoch [29/1000], Batch loss: 0.5758\n",
      "Epoch [29/1000], Batch loss: 0.8648\n",
      "Epoch [29/1000], Batch loss: 0.6814\n",
      "Epoch [29/1000], Batch loss: 0.6059\n",
      "Epoch [29/1000], Batch loss: 1.0065\n",
      "Epoch [29/1000], Batch loss: 0.5831\n",
      "Epoch [29/1000], Batch loss: 0.8087\n",
      "Epoch [29/1000], Batch loss: 0.8983\n",
      "Epoch [29/1000], Batch loss: 0.8865\n",
      "Epoch [29/1000], Batch loss: 0.5862\n",
      "Epoch [29/1000], Batch loss: 0.6345\n",
      "Epoch [29/1000], Batch loss: 0.6751\n",
      "Epoch [29/1000], Batch loss: 0.5518\n",
      "Epoch [29/1000], Batch loss: 0.6127\n",
      "Train Accuracy 0.5223880597014925\n",
      "Test Accuracy 0.5147058823529411\n",
      "Epoch [30/1000], Batch loss: 0.5878\n",
      "Epoch [30/1000], Batch loss: 0.6468\n",
      "Epoch [30/1000], Batch loss: 0.8733\n",
      "Epoch [30/1000], Batch loss: 0.5778\n",
      "Epoch [30/1000], Batch loss: 0.9122\n",
      "Epoch [30/1000], Batch loss: 0.6578\n",
      "Epoch [30/1000], Batch loss: 0.4297\n",
      "Epoch [30/1000], Batch loss: 0.8190\n",
      "Epoch [30/1000], Batch loss: 0.8196\n",
      "Epoch [30/1000], Batch loss: 0.9485\n",
      "Epoch [30/1000], Batch loss: 0.8488\n",
      "Epoch [30/1000], Batch loss: 0.5301\n",
      "Epoch [30/1000], Batch loss: 0.5550\n",
      "Epoch [30/1000], Batch loss: 0.9365\n",
      "Epoch [30/1000], Batch loss: 0.8245\n",
      "Epoch [30/1000], Batch loss: 0.7576\n",
      "Epoch [30/1000], Batch loss: 0.9538\n",
      "Epoch [30/1000], Batch loss: 0.6576\n",
      "Epoch [30/1000], Batch loss: 0.6404\n",
      "Epoch [30/1000], Batch loss: 0.4838\n",
      "Epoch [30/1000], Batch loss: 0.7703\n",
      "Epoch [30/1000], Batch loss: 0.7716\n",
      "Epoch [30/1000], Batch loss: 0.9150\n",
      "Epoch [30/1000], Batch loss: 0.6859\n",
      "Epoch [30/1000], Batch loss: 0.5098\n",
      "Epoch [30/1000], Batch loss: 1.0203\n",
      "Epoch [30/1000], Batch loss: 0.8587\n",
      "Epoch [30/1000], Batch loss: 0.6601\n",
      "Epoch [30/1000], Batch loss: 0.9172\n",
      "Epoch [30/1000], Batch loss: 0.6842\n",
      "Epoch [30/1000], Batch loss: 0.6180\n",
      "Epoch [30/1000], Batch loss: 0.7720\n",
      "Epoch [30/1000], Batch loss: 0.5751\n",
      "Epoch [30/1000], Batch loss: 0.6570\n",
      "Epoch [30/1000], Batch loss: 0.5651\n",
      "Epoch [30/1000], Batch loss: 0.6251\n",
      "Epoch [30/1000], Batch loss: 0.7155\n",
      "Epoch [30/1000], Batch loss: 0.7240\n",
      "Epoch [30/1000], Batch loss: 0.8258\n",
      "Epoch [30/1000], Batch loss: 0.6274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000], Batch loss: 0.5843\n",
      "Epoch [30/1000], Batch loss: 0.6332\n",
      "Epoch [30/1000], Batch loss: 0.6839\n",
      "Epoch [30/1000], Batch loss: 0.8032\n",
      "Epoch [30/1000], Batch loss: 0.9641\n",
      "Epoch [30/1000], Batch loss: 0.5648\n",
      "Epoch [30/1000], Batch loss: 0.7385\n",
      "Epoch [30/1000], Batch loss: 0.9866\n",
      "Epoch [30/1000], Batch loss: 0.8816\n",
      "Epoch [30/1000], Batch loss: 0.7612\n",
      "Epoch [30/1000], Batch loss: 0.6928\n",
      "Epoch [30/1000], Batch loss: 0.8086\n",
      "Epoch [30/1000], Batch loss: 1.1017\n",
      "Epoch [30/1000], Batch loss: 0.5733\n",
      "Epoch [30/1000], Batch loss: 0.6405\n",
      "Epoch [30/1000], Batch loss: 0.7289\n",
      "Epoch [30/1000], Batch loss: 0.6727\n",
      "Epoch [30/1000], Batch loss: 0.7362\n",
      "Epoch [30/1000], Batch loss: 0.8849\n",
      "Epoch [30/1000], Batch loss: 0.5088\n",
      "Epoch [30/1000], Batch loss: 0.8332\n",
      "Epoch [30/1000], Batch loss: 0.6969\n",
      "Epoch [30/1000], Batch loss: 0.7141\n",
      "Epoch [30/1000], Batch loss: 0.8706\n",
      "Epoch [30/1000], Batch loss: 0.7870\n",
      "Epoch [30/1000], Batch loss: 0.7804\n",
      "Epoch [30/1000], Batch loss: 0.8816\n",
      "Epoch [30/1000], Batch loss: 0.6600\n",
      "Epoch [30/1000], Batch loss: 0.6505\n",
      "Epoch [30/1000], Batch loss: 0.8504\n",
      "Epoch [30/1000], Batch loss: 0.6540\n",
      "Epoch [30/1000], Batch loss: 0.6649\n",
      "Epoch [30/1000], Batch loss: 0.7308\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR  # Add this import\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have a model defined\n",
    "# model = YourModel()\n",
    "\n",
    "# Set up loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# Add a scheduler\n",
    "scheduler = MultiStepLR(optimizer, milestones=[200, 300, 400], gamma=0.3)  # Adjust milestones and gamma as needed\n",
    "\n",
    "epoch_numbers = 1000\n",
    "k = 10\n",
    "seed = 42\n",
    "\n",
    "# Combine all datasets\n",
    "combined_dataset = CombinedDataset(datasets)\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "average_accuracies = []\n",
    "ress = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(combined_dataset):\n",
    "    model.load_state_dict(torch.load('model_Prof_cetin.pth'))\n",
    "    best = -1\n",
    "    # Split the data\n",
    "    train_dataset = torch.utils.data.Subset(combined_dataset, train_idx)\n",
    "    test_dataset = torch.utils.data.Subset(combined_dataset, test_idx)\n",
    "\n",
    "    # Create data loaders for train and test sets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    for epoch in range(epoch_numbers):\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        y_probs = []\n",
    "        model.train()  # Set the model to training mode\n",
    "        for dataANDlabels in train_loader:\n",
    "            out = model(dataANDlabels)\n",
    "            lbl = dataANDlabels[0][1].float().to('cuda')\n",
    "\n",
    "            loss = criterion(out.squeeze(), lbl)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{epoch_numbers}], Batch loss: {loss.item():.4f}')\n",
    "\n",
    "            \n",
    "            predicted = (out >= 0.5).float()\n",
    "            y_probs.extend(out.data.cpu().numpy()) \n",
    "            y_true.extend(lbl.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        print(\"Train Accuracy\", accuracy)\n",
    "            \n",
    "            \n",
    "            \n",
    "        scheduler.step()  # Update learning rate\n",
    "\n",
    "        # Test the model\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        y_probs = []\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for dataANDlabels in test_loader:\n",
    "                out = model(dataANDlabels)\n",
    "                lbl = dataANDlabels[0][1].float().to('cuda')\n",
    "\n",
    "                # Apply threshold\n",
    "                predicted = (out >= 0.5).float()\n",
    "                y_probs.extend(out.data.cpu().numpy()) \n",
    "                y_true.extend(lbl.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            print(\"Test Accuracy\", accuracy)\n",
    "\n",
    "            if best < accuracy:\n",
    "                best = accuracy\n",
    "\n",
    "    ress.append(best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef83e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9544cf01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
