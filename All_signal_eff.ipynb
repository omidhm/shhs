{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ca83ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bpradsad\\AppData\\Local\\Temp\\ipykernel_15788\\2731409793.py:4: DtypeWarning: Columns (1263,1347) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('C:/Users/Bpradsad/Desktop/prof cetin/prof beharati/shhs1- 2.csv')\n",
      "C:\\Users\\Bpradsad\\AppData\\Local\\Temp\\ipykernel_15788\\2731409793.py:4: DtypeWarning: Columns (1263,1347) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('C:/Users/Bpradsad/Desktop/prof cetin/prof beharati/shhs1- 2.csv')\n"
     ]
    }
   ],
   "source": [
    "def Considered_AHI_restriction_2(LOW_AHI, High_AHI):\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_csv('C:/Users/Bpradsad/Desktop/prof cetin/prof beharati/shhs1- 2.csv')\n",
    "\n",
    "#     AHI_base = AHI\n",
    "    AHI_base = LOW_AHI\n",
    "#     filtered_df = df[(df[\"ahi_a0h3a\"] >= AHI_base) & (df[\"htnderv_s1\"] == 0) & (df[\"htnderv_s2\"].notnull())]\n",
    "    \n",
    "    filtered_df = df[((df[\"ahi_a0h3a\"] < LOW_AHI) | (df[\"ahi_a0h3a\"] > High_AHI)) & (df[\"htnderv_s1\"] == 0) & (df[\"htnderv_s2\"].notnull())]\n",
    "    selected_basedonAHI = [str(x[0]) for x in filtered_df[[\"nsrrid\"]].values]\n",
    "    AHI_considered = []\n",
    "    for patient in file_nn:\n",
    "        if patient[6:-7] in selected_basedonAHI:\n",
    "            AHI_considered.append(1)\n",
    "        else:\n",
    "            AHI_considered.append(0)\n",
    "\n",
    "    AHI_considered = np.array(AHI_considered)\n",
    "#     print(\"percentage of accepted data\", sum(AHI_considered)/len(AHI_considered))\n",
    "    Consider = [np.array(AHI_considered)==1][0]\n",
    "\n",
    "    return(Consider)\n",
    "\n",
    "file_nn = np.load(\"C:/Users/Bpradsad/file_nn.npy\")\n",
    "label_nn = np.load(\"C:/Users/Bpradsad/label_nn.npy\")\n",
    "len(file_nn[Considered_AHI_restriction_2(6, 21)])\n",
    "selected_data = [name[:12]+\".npy\" for name in file_nn[Considered_AHI_restriction_2(6, 21)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4afd3a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf674b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f3b5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b8b21e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "\n",
    "# # Define the list of folder names\n",
    "# foldernames = [\"Signal_ABDO\", \"Signal_NEWAIR\", \"Signal_H\", \"Signal_ECG\", \"Signal_EOG_L\", \n",
    "#                \"Signal_EMG\", \"Signal_POSITION\", \"Signal_EEG\", \"Signal_EOG_R\", \"Signal_THOR\", \n",
    "#                \"Signal_SaO2_OX\"]\n",
    "\n",
    "# # Define the root directory where the folders are located\n",
    "# root_dir = 'D:/Directory_D_Desk_jup/Raw_npy/'  # Replace with your actual root directory\n",
    "\n",
    "# # Iterate through each folder\n",
    "# for folder in foldernames:\n",
    "#     folder_path = os.path.join(root_dir, folder)\n",
    "    \n",
    "#     # Check if the path is a directory\n",
    "#     if os.path.isdir(folder_path):\n",
    "#         # Iterate through the files in the folder\n",
    "#         for file_name in os.listdir(folder_path):\n",
    "#             file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "#             # Check if the file is a numpy array (you may want to adjust this check)\n",
    "#             if file_name.endswith('.npy'):\n",
    "#                 # Load the numpy array\n",
    "#                 data = np.load(file_path)\n",
    "                \n",
    "#                 # Check for NaN values\n",
    "#                 has_nan = np.isnan(data).any()\n",
    "                \n",
    "#                 if has_nan:\n",
    "#                     print(f\"File {file_name} in Folder {folder} contains NaN values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e143e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# import numpy as np\n",
    "\n",
    "# # Define the list of folder names\n",
    "# foldernames = [\"Signal_ABDO\", \"Signal_NEWAIR\", \"Signal_H\", \"Signal_ECG\", \"Signal_EOG_L\", \n",
    "#                \"Signal_EMG\", \"Signal_POSITION\", \"Signal_EEG\", \"Signal_EOG_R\", \"Signal_THOR\", \n",
    "#                \"Signal_SaO2_OX\"]\n",
    "\n",
    "\n",
    "\n",
    "# # Define the root directory where the folders are located\n",
    "# root_dir = 'D:/Directory_D_Desk_jup/Raw_npy/'  # Replace with your actual root directory\n",
    "\n",
    "# # Define the new directory where you want to copy the selected numpy arrays\n",
    "# new_root_dir = 'C:/Users/Bpradsad/Desktop/tempp/'  # Replace with your desired new root directory\n",
    "\n",
    "# # Define the number of arrays to select from each folder\n",
    "# num_arrays_to_select = 811\n",
    "\n",
    "# # Iterate through each folder\n",
    "# for folder in foldernames:\n",
    "#     folder_path = os.path.join(root_dir, folder)\n",
    "    \n",
    "#     # Check if the path is a directory\n",
    "#     if os.path.isdir(folder_path):\n",
    "#         # Create corresponding folder in the new directory\n",
    "#         new_folder_path = os.path.join(new_root_dir, folder)\n",
    "#         os.makedirs(new_folder_path, exist_ok=True)\n",
    "        \n",
    "#         # Get a list of all numpy files in the folder\n",
    "#         numpy_files = [f for f in os.listdir(folder_path) if f.endswith('.npy')]\n",
    "        \n",
    "#         # Select the first 50 numpy arrays\n",
    "#         selected_arrays = numpy_files[:num_arrays_to_select]\n",
    "        \n",
    "#         # Iterate through the selected arrays\n",
    "#         for selected_array in selected_arrays:\n",
    "#             source_path = os.path.join(folder_path, selected_array)\n",
    "#             dest_path = os.path.join(new_folder_path, selected_array)\n",
    "            \n",
    "#             # Copy the selected array to the new directory\n",
    "#             shutil.copy2(source_path, dest_path)\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the list of folder names\n",
    "foldernames = [\"Signal_ABDO\", \"Signal_NEWAIR\", \"Signal_H\", \"Signal_ECG\", \"Signal_EOG_L\", \n",
    "               \"Signal_EMG\", \"Signal_POSITION\", \"Signal_EEG\", \"Signal_EOG_R\", \"Signal_THOR\", \n",
    "               \"Signal_SaO2_OX\"]\n",
    "\n",
    "# Define the root directory where the folders are located\n",
    "root_dir = 'D:/Directory_D_Desk_jup/Raw_npy/'  # Replace with your actual root directory\n",
    "\n",
    "# Define the new directory where you want to copy the selected numpy arrays\n",
    "new_root_dir = 'C:/Users/Bpradsad/Desktop/tempp/'  # Replace with your desired new root directory\n",
    "\n",
    "# Define the list of selected data (file names)\n",
    "selected_data = selected_data\n",
    "\n",
    "# Iterate through each folder\n",
    "for folder in foldernames:\n",
    "    folder_path = os.path.join(root_dir, folder)\n",
    "    \n",
    "    # Check if the path is a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Create corresponding folder in the new directory\n",
    "        new_folder_path = os.path.join(new_root_dir, folder)\n",
    "        os.makedirs(new_folder_path, exist_ok=True)\n",
    "        \n",
    "        # Get a list of all numpy files in the folder\n",
    "        numpy_files = [f for f in os.listdir(folder_path) if f.endswith('.npy')]\n",
    "        \n",
    "        # Iterate through the selected arrays\n",
    "        for selected_array in selected_data:\n",
    "            if selected_array in numpy_files:\n",
    "                source_path = os.path.join(folder_path, selected_array)\n",
    "                dest_path = os.path.join(new_folder_path, selected_array)\n",
    "                \n",
    "                # Copy the selected array to the new directory\n",
    "                shutil.copy2(source_path, dest_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f7a767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dic = {}\n",
    "for idx, name in enumerate(file_nn):\n",
    "    label_dic[name[:12]+\".npy\"] = int(label_nn[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b619452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d337391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, data_dir, folder_name, label_n):\n",
    "#         self.data_dir = data_dir\n",
    "#         self.folder_name = folder_name\n",
    "#         self.label_n = label_n\n",
    "#         self.file_paths = []\n",
    "\n",
    "#         folder_path = os.path.join(data_dir, folder_name)\n",
    "#         for file_name in os.listdir(folder_path):\n",
    "#             file_path = os.path.join(folder_path, file_name)\n",
    "# #             single_data = np.load(file_path, allow_pickle=True).astype('float16')  # Change data type to float32\n",
    "#             single_data = np.load(file_path, allow_pickle=True)#.astype('float32')  # Change data type to float32\n",
    "#             self.file_paths.append(single_data)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.file_paths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         data = self.file_paths[idx]\n",
    "#         data_lbl = self.label_n[idx]\n",
    "        \n",
    "#         return data, data_lbl\n",
    "\n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, data_dir, folder_name, label_n):\n",
    "#         self.data_dir = data_dir\n",
    "#         self.folder_name = folder_name\n",
    "#         self.label_n = label_n\n",
    "#         self.file_paths = []\n",
    "#         self.mean = None\n",
    "#         self.std = None\n",
    "\n",
    "#         folder_path = os.path.join(data_dir, folder_name)\n",
    "#         for file_name in os.listdir(folder_path):\n",
    "#             file_path = os.path.join(folder_path, file_name)\n",
    "#             single_data = np.load(file_path, allow_pickle=True).astype('float32')\n",
    "#             self.file_paths.append(single_data)\n",
    "\n",
    "#         self.calculate_mean_std()\n",
    "\n",
    "#     def calculate_mean_std(self):\n",
    "#         all_data = np.concatenate(self.file_paths)\n",
    "#         self.mean = np.mean(all_data)\n",
    "#         self.std = np.std(all_data)\n",
    "#         self.max = np.max(all_data)\n",
    "\n",
    "#     def normalize_data(self, data):\n",
    "# #         return (data - self.mean) / self.std\n",
    "#         return data /(self.max +.01)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.file_paths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         data = self.file_paths[idx]\n",
    "#         data_lbl = self.label_n[idx]\n",
    "#         data = self.normalize_data(data)  # Normalize data\n",
    "\n",
    "#         return data, data_lbl\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, folder_name, label_n):\n",
    "        self.data_dir = data_dir\n",
    "        self.folder_name = folder_name\n",
    "        self.label_n = label_n\n",
    "        self.file_paths = []\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "        folder_path = os.path.join(data_dir, folder_name)\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            single_data = np.load(file_path, allow_pickle=True).astype('float32')\n",
    "            self.file_paths.append((file_name, single_data))  # Store as tuple\n",
    "\n",
    "        self.calculate_mean_std()\n",
    "\n",
    "    def calculate_mean_std(self):\n",
    "        all_data = np.concatenate([data for _, data in self.file_paths])\n",
    "        self.mean = np.mean(all_data)\n",
    "        self.std = np.std(all_data)\n",
    "        self.max = np.max(all_data)\n",
    "\n",
    "    def normalize_data(self, data):\n",
    "        return data / (self.max + 0.01)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name, data = self.file_paths[idx]\n",
    "        data_lbl = self.label_n[file_name]  # Get label using file name\n",
    "        data = self.normalize_data(data)\n",
    "\n",
    "        return data, data_lbl\n",
    "\n",
    "\n",
    "###### not working #############################\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(dataset) for dataset in self.datasets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = [dataset[idx % len(dataset)] for dataset in self.datasets]\n",
    "        return tuple(data)\n",
    "  \n",
    "    \n",
    "\n",
    "foldernames = [\"Signal_ABDO\", \"Signal_NEWAIR\", \"Signal_H\", \"Signal_ECG\", \"Signal_EOG_L\", \"Signal_EMG\", \"Signal_POSITION\", \"Signal_EEG\", \"Signal_EOG_R\", \"Signal_THOR\", \"Signal_SaO2_OX\"]\n",
    "fs = [10, 10, 1, 125, 50, 125, 1, 125, 50, 10, 1]\n",
    "\n",
    "\n",
    "label_n = label_dic\n",
    "\n",
    "datasets = []\n",
    "\n",
    "for folder_name in foldernames:\n",
    "    \n",
    "#     dataset = CustomDataset(\"D:/Directory_D_Desk_jup/Raw_npy/\", folder_name, label_n)\n",
    "    dataset = CustomDataset(\"C:/Users/Bpradsad/Desktop/tempp/\", folder_name, label_n)\n",
    "    \n",
    "    datasets.append(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a289d758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c17661a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CNN_Block(nn.Module):\n",
    "    def __init__(self, input_channels, kernel_sizes):\n",
    "        super(CNN_Block, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        for kernel_size in kernel_sizes:\n",
    "            conv_layer = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=input_channels, out_channels=input_channels, kernel_size=kernel_size, stride=1, padding=1),\n",
    "                nn.BatchNorm1d(input_channels),  # Add BatchNorm1d\n",
    "#                 nn.ReLU(),\n",
    "                nn.AdaptiveMaxPool1d(output_size=64*64)\n",
    "            )\n",
    "            self.conv_layers.append(conv_layer)\n",
    "        \n",
    "#         self.adaptive_pool = nn.AdaptiveMaxPool1d(output_size=64*64)\n",
    "#         self.grelu = nn.ELU()\n",
    "        \n",
    "        self.converter = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=11, out_channels=3, kernel_size=3, dilation=2),\n",
    "            nn.BatchNorm2d(3),  \n",
    "#             nn.ReLU(),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.effnet = EfficientNet.from_pretrained('efficientnet-b0')  # Assuming 1000 classes\n",
    "        \n",
    "        # Add a classifier\n",
    "        self.effnet._fc = nn.Sequential(\n",
    "            nn.Linear(1280, 1),  # Assuming 1280 is the output size of the EfficientNet-B0\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        for param in self.effnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.effnet._fc.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        output = []\n",
    "        \n",
    "        for i, data_and_lbl in enumerate(inputs):\n",
    "#             print(\"--------------------------------\", i)\n",
    "            x = data_and_lbl[0].unsqueeze(1).float().to('cuda')\n",
    "            \n",
    "            # Flatten the tensor to 1D\n",
    "#             x_tmp = x.view(-1).cpu().numpy()\n",
    "            \n",
    "\n",
    "#             # Plot the histogram\n",
    "#             plt.hist(x_tmp, bins=100, density=True, alpha=0.75, color='b')\n",
    "#             plt.xlabel('Value')\n",
    "#             plt.ylabel('Frequency')\n",
    "#             plt.title('Distribution of x')\n",
    "#             plt.show()\n",
    "            \n",
    "\n",
    "#             # Find maximum, minimum, and NaN values\n",
    "#             max_value = torch.max(x).item()\n",
    "#             min_value = torch.min(x).item()\n",
    "\n",
    "#             # Check for NaN values\n",
    "#             has_nan = torch.isnan(x).any().item()\n",
    "\n",
    "#             print(f\"before Maximum value: {max_value}\")\n",
    "#             print(f\"before Minimum value: {min_value}\")\n",
    "#             print(f\"before Contains NaN: {has_nan}\")\n",
    "            ####################\n",
    "            \n",
    "#             print(\"x.shape\", x.shape)\n",
    "            \n",
    "            x = self.conv_layers[i](x)\n",
    "        \n",
    "        \n",
    "            # Assuming x is a CUDA tensor\n",
    "            x_tmp = x.view(-1)\n",
    "            x_tmp = x_tmp.masked_fill(x_tmp == -float('inf'), 0)\n",
    "            x_tmp = x_tmp.masked_fill(torch.isnan(x_tmp), 0)\n",
    "            x = x_tmp.view(x.shape)\n",
    "            \n",
    "# # #             print(\"x.shape\", x.shape)\n",
    "# #             # Flatten the tensor to 1D\n",
    "#             x_tmp = x.view(-1).cpu().detach().numpy()\n",
    "            \n",
    "# #             x_tmp[x_tmp == -float('inf')] = -10\n",
    "#             # Plot the histogram\n",
    "#             plt.hist(x_tmp, bins=100, density=True, alpha=0.75, color='b')\n",
    "#             plt.xlabel('Value')\n",
    "#             plt.ylabel('Frequency')\n",
    "#             plt.title('Distribution of x')\n",
    "#             plt.show()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             # Find maximum, minimum, and NaN values\n",
    "#             max_value = torch.max(x).item()\n",
    "#             min_value = torch.min(x).item()\n",
    "\n",
    "#             # Check for NaN values\n",
    "#             has_nan = torch.isnan(x).any().item()\n",
    "\n",
    "#             print(f\"after Maximum value: {max_value}\")\n",
    "#             print(f\"after Minimum value: {min_value}\")\n",
    "#             print(f\"after Contains NaN: {has_nan}\")\n",
    "            \n",
    "            if torch.isnan(x).any():\n",
    "                print(f'XXXXXXXXXXXXXXXXXXXXXXX values {i} output contains NaN values')\n",
    "#             x = self.adaptive_pool(x)\n",
    "\n",
    "\n",
    "            x = x.view(-1, 1, 64, 64)\n",
    "            output.append(x)\n",
    "            \n",
    "#         print(output)\n",
    "#         print(len(output))\n",
    "\n",
    "        \n",
    "        combined_x = torch.cat(output, dim=1)\n",
    "                \n",
    "        if torch.isnan(combined_x).any():\n",
    "            print(f'Conv{i} output contains NaN values')\n",
    "            \n",
    "#         print(\"combined_x.shape\", combined_x.shape)\n",
    "        # Convert and process combined_x\n",
    "        x_out = self.converter(combined_x)\n",
    "#         print(x_out)\n",
    "      \n",
    "        # EfficientNet and final tuning\n",
    "        x = self.effnet(x_out)\n",
    "#         out = self.final_tuning(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "input_channels = 1\n",
    "# kernel_size = np.array(fs)+1   \n",
    "kernel_size = [5, 5, 3, 7, 7, 7, 3, 7, 7, 5, 3]\n",
    "# kernel_size = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
    "model = CNN_Block(input_channels, kernel_size)\n",
    "# torch.save(model.state_dict(), 'model_Prof_cetin.pth')\n",
    "# model = model.half()\n",
    "# Move the model to GPU\n",
    "model = model.to('cuda')\n",
    "model.load_state_dict(torch.load('model_Prof_cetin.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4397a074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_Block(\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv1d(1, 1, kernel_size=(5,), stride=(1,), padding=(1,))\n",
       "      (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): AdaptiveMaxPool1d(output_size=4096)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv1d(1, 1, kernel_size=(5,), stride=(1,), padding=(1,))\n",
       "      (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): AdaptiveMaxPool1d(output_size=4096)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): AdaptiveMaxPool1d(output_size=4096)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv1d(1, 1, kernel_size=(7,), stride=(1,), padding=(1,))\n",
       "      (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): AdaptiveMaxPool1d(output_size=4096)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv1d(1, 1, kernel_size=(7,), stride=(1,), padding=(1,))\n",
       "      (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): AdaptiveMaxPool1d(output_size=4096)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Conv1d(1, 1, kernel_size=(7,), stride=(1,), padding=(1,))\n",
       "      (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): AdaptiveMaxPool1d(output_size=4096)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): AdaptiveMaxPool1d(output_size=4096)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Conv1d(1, 1, kernel_size=(7,), stride=(1,), padding=(1,))\n",
       "      (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): AdaptiveMaxPool1d(output_size=4096)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Conv1d(1, 1, kernel_size=(7,), stride=(1,), padding=(1,))\n",
       "      (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): AdaptiveMaxPool1d(output_size=4096)\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Conv1d(1, 1, kernel_size=(5,), stride=(1,), padding=(1,))\n",
       "      (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): AdaptiveMaxPool1d(output_size=4096)\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): AdaptiveMaxPool1d(output_size=4096)\n",
       "    )\n",
       "  )\n",
       "  (converter): Sequential(\n",
       "    (0): Conv2d(11, 3, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2))\n",
       "    (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Sigmoid()\n",
       "  )\n",
       "  (effnet): EfficientNet(\n",
       "    (_conv_stem): Conv2dStaticSamePadding(\n",
       "      3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "      (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "    )\n",
       "    (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_blocks): ModuleList(\n",
       "      (0): MBConvBlock(\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (1): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "          (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (2): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (3): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (4): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (5): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "          (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (6): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (7): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (8): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (9): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (10): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (11): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (12): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (13): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (14): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (15): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "    )\n",
       "    (_conv_head): Conv2dStaticSamePadding(\n",
       "      320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (static_padding): Identity()\n",
       "    )\n",
       "    (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "    (_dropout): Dropout(p=0.2, inplace=False)\n",
       "    (_fc): Sequential(\n",
       "      (0): Linear(in_features=1280, out_features=1, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (_swish): MemoryEfficientSwish()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0df9ae7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# import time\n",
    "\n",
    "# epoch_numbers = 100\n",
    "# k = 5\n",
    "\n",
    "# # Combine all datasets\n",
    "# combined_dataset = CombinedDataset(datasets)\n",
    "\n",
    "# # Initialize KFold\n",
    "# kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# average_accuracies = []\n",
    "\n",
    "# for train_idx, test_idx in kf.split(combined_dataset):\n",
    "#     # Split the data\n",
    "#     train_dataset = torch.utils.data.Subset(combined_dataset, train_idx)\n",
    "#     test_dataset = torch.utils.data.Subset(combined_dataset, test_idx)\n",
    "\n",
    "#     # Create data loaders for train and test sets\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    \n",
    "#     for epoch in range(epoch_numbers):\n",
    "#         for dataANDlabels in train_loader:\n",
    "#             t1 = time.time()\n",
    "#             out = model(dataANDlabels)\n",
    "#             lbl = dataANDlabels[0][1].float().to('cuda')\n",
    "        \n",
    "#             print(out.shape, lbl.shape)\n",
    "  \n",
    "#             print(time.time()-t1)\n",
    "#         break\n",
    "#     break\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b59398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3981e82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Batch loss: 0.6783\n",
      "Epoch [1/1000], Batch loss: 0.6826\n",
      "Epoch [1/1000], Batch loss: 0.7494\n",
      "Epoch [1/1000], Batch loss: 0.7637\n",
      "Epoch [1/1000], Batch loss: 0.7257\n",
      "Epoch [1/1000], Batch loss: 0.6109\n",
      "Epoch [1/1000], Batch loss: 0.8702\n",
      "Epoch [1/1000], Batch loss: 0.6903\n",
      "Epoch [1/1000], Batch loss: 0.7929\n",
      "Epoch [1/1000], Batch loss: 0.7033\n",
      "Epoch [1/1000], Batch loss: 0.8263\n",
      "Epoch [1/1000], Batch loss: 0.8003\n",
      "Epoch [1/1000], Batch loss: 0.8071\n",
      "Epoch [1/1000], Batch loss: 0.7417\n",
      "Epoch [1/1000], Batch loss: 0.7201\n",
      "Epoch [1/1000], Batch loss: 0.7764\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     55\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Reset gradients\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_numbers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Batch loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     60\u001b[0m predicted \u001b[38;5;241m=\u001b[39m (out \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     61\u001b[0m y_probs\u001b[38;5;241m.\u001b[39mextend(out\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()) \n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR  # Add this import\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have a model defined\n",
    "# model = YourModel()\n",
    "\n",
    "# Set up loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# Add a scheduler\n",
    "scheduler = MultiStepLR(optimizer, milestones=[200, 300, 400], gamma=0.3)  # Adjust milestones and gamma as needed\n",
    "\n",
    "epoch_numbers = 1000\n",
    "k = 5\n",
    "seed = 42\n",
    "\n",
    "# Combine all datasets\n",
    "combined_dataset = CombinedDataset(datasets)\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "average_accuracies = []\n",
    "ress = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(combined_dataset):\n",
    "    model.load_state_dict(torch.load('model_Prof_cetin.pth'))\n",
    "    best = -1\n",
    "    # Split the data\n",
    "    train_dataset = torch.utils.data.Subset(combined_dataset, train_idx)\n",
    "    test_dataset = torch.utils.data.Subset(combined_dataset, test_idx)\n",
    "\n",
    "    # Create data loaders for train and test sets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    for epoch in range(epoch_numbers):\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        y_probs = []\n",
    "        model.train()  # Set the model to training mode\n",
    "        for dataANDlabels in train_loader:\n",
    "            out = model(dataANDlabels)\n",
    "            lbl = dataANDlabels[0][1].float().to('cuda')\n",
    "\n",
    "            loss = criterion(out.squeeze(), lbl)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{epoch_numbers}], Batch loss: {loss.item():.4f}')\n",
    "\n",
    "            \n",
    "            predicted = (out >= 0.5).float()\n",
    "            y_probs.extend(out.data.cpu().numpy()) \n",
    "            y_true.extend(lbl.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        print(\"Train Accuracy\", accuracy)\n",
    "            \n",
    "            \n",
    "            \n",
    "        scheduler.step()  # Update learning rate\n",
    "\n",
    "        # Test the model\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        y_probs = []\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for dataANDlabels in test_loader:\n",
    "                out = model(dataANDlabels)\n",
    "                lbl = dataANDlabels[0][1].float().to('cuda')\n",
    "\n",
    "                # Apply threshold\n",
    "                predicted = (out >= 0.5).float()\n",
    "                y_probs.extend(out.data.cpu().numpy()) \n",
    "                y_true.extend(lbl.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            print(\"Test Accuracy\", accuracy)\n",
    "\n",
    "            if best < accuracy:\n",
    "                best = accuracy\n",
    "\n",
    "    ress.append(best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3911c2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79359979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bpradsad\\anaconda3\\envs\\torch1\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Bpradsad\\anaconda3\\envs\\torch1\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5481481481481482"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ress)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f166bcf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
