{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49888321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240731, 74)\n",
      "(240731, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "name = \"74_feature_Arousal_normed\"\n",
    "start = 48\n",
    "end = 53\n",
    "X_trr, y_trr, X_tee, y_tee = [], [], [], []\n",
    "X = np.load(\"C:/Features/\"+name+\".npy\")\n",
    "\n",
    "print(X.shape)\n",
    "X = X[:,start:end]\n",
    "print(X.shape)\n",
    "scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "###################################################################################################\n",
    "# lookingSection = \"Respiratory\"\n",
    "lookingSection = \"Arousal\"\n",
    "lookingSection\n",
    "\n",
    "aliginingwithArousal = [\n",
    " \"shhs1-200094_ds.npy\"\n",
    ",\"shhs1-200245_ds.npy\"\n",
    ",\"shhs1-200291_ds.npy\"\n",
    ",\"shhs1-200312_ds.npy\"\n",
    ",\"shhs1-200387_ds.npy\"\n",
    ",\"shhs1-200860_ds.npy\"\n",
    ",\"shhs1-200641_ds.npy\"\n",
    ",\"shhs1-200668_ds.npy\"\n",
    ",\"shhs1-200692_ds.npy\"\n",
    ",\"shhs1-200759_ds.npy\"\n",
    ",\"shhs1-201234_ds.npy\"\n",
    ",\"shhs1-201277_ds.npy\"\n",
    ",\"shhs1-201284_ds.npy\"\n",
    ",\"shhs1-201343_ds.npy\"\n",
    ",\"shhs1-201199_ds.npy\"\n",
    ",\"shhs1-201565_ds.npy\"\n",
    ",\"shhs1-201632_ds.npy\"\n",
    ",\"shhs1-201852_ds.npy\"\n",
    ",\"shhs1-201678_ds.npy\"\n",
    ",\"shhs1-201716_ds.npy\"\n",
    ",\"shhs1-201756_ds.npy\"\n",
    ",\"shhs1-202055_ds.npy\"\n",
    ",\"shhs1-202310_ds.npy\"\n",
    ",\"shhs1-202393_ds.npy\"\n",
    ",\"shhs1-202152_ds.npy\"\n",
    ",\"shhs1-202438_ds.npy\"\n",
    ",\"shhs1-202559_ds.npy\"\n",
    ",\"shhs1-202405_ds.npy\"\n",
    ",\"shhs1-202503_ds.npy\"\n",
    ",\"shhs1-202712_ds.npy\"\n",
    ",\"shhs1-202558_ds.npy\"\n",
    ",\"shhs1-202622_ds.npy\"\n",
    ",\"shhs1-202763_ds.npy\"\n",
    ",\"shhs1-202717_ds.npy\"\n",
    ",\"shhs1-202849_ds.npy\"\n",
    ",\"shhs1-202916_ds.npy\"\n",
    ",\"shhs1-202986_ds.npy\"\n",
    ",\"shhs1-203001_ds.npy\"\n",
    ",\"shhs1-203089_ds.npy\"\n",
    ",\"shhs1-203115_ds.npy\"\n",
    ",\"shhs1-203139_ds.npy\"\n",
    ",\"shhs1-203196_ds.npy\"\n",
    ",\"shhs1-203307_ds.npy\"\n",
    ",\"shhs1-203355_ds.npy\"\n",
    ",\"shhs1-203521_ds.npy\"\n",
    ",\"shhs1-203418_ds.npy\"\n",
    ",\"shhs1-203571_ds.npy\"\n",
    ",\"shhs1-203432_ds.npy\"\n",
    ",\"shhs1-203645_ds.npy\"\n",
    ",\"shhs1-203677_ds.npy\"\n",
    ",\"shhs1-203567_ds.npy\"\n",
    ",\"shhs1-203684_ds.npy\"\n",
    ",\"shhs1-203589_ds.npy\"\n",
    ",\"shhs1-203592_ds.npy\"\n",
    ",\"shhs1-203763_ds.npy\"\n",
    ",\"shhs1-203705_ds.npy\"\n",
    ",\"shhs1-203766_ds.npy\"\n",
    ",\"shhs1-203846_ds.npy\"\n",
    ",\"shhs1-203864_ds.npy\"\n",
    ",\"shhs1-203827_ds.npy\"\n",
    ",\"shhs1-204014_ds.npy\"\n",
    ",\"shhs1-204039_ds.npy\"\n",
    ",\"shhs1-204065_ds.npy\"\n",
    ",\"shhs1-204097_ds.npy\"\n",
    ",\"shhs1-204079_ds.npy\"\n",
    ",\"shhs1-204089_ds.npy\"\n",
    ",\"shhs1-204144_ds.npy\"\n",
    ",\"shhs1-204236_ds.npy\"\n",
    ",\"shhs1-204269_ds.npy\"\n",
    ",\"shhs1-204268_ds.npy\"\n",
    "                      ]\n",
    "\n",
    "import os\n",
    "# import pyedflib\n",
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import scipy.signal \n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "import scipy.interpolate\n",
    "\n",
    "import pandas as pd\n",
    "col_list = [\"a\"]\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/Bpradsad/Desktop/data/SaO2/norm_sparkremoverAVG_full50_STFT_n/htnderv_s2_n.csv\", usecols=[\"a\",\"b\"])\n",
    "## shhs1-200001\n",
    "file_n = []\n",
    "for i in df[\"a\"]:\n",
    "    file_n.append(f\"{i}\")\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/Bpradsad/Desktop/data/SaO2/norm_sparkremoverAVG_full50_STFT_n/htnderv_s2_n.csv\", usecols=[\"a\",\"b\"])\n",
    "## shhs1-200001\n",
    "label_n = []\n",
    "for i in df[\"b\"]:\n",
    "    label_n.append(f\"{i}\")\n",
    "\n",
    "import os.path\n",
    "file_nn = []\n",
    "label_nn = []\n",
    "\n",
    "for i, patient in enumerate(file_n):\n",
    "    file_exist = os.path.exists('D:/Directory_D_Desk_jup/Raw_npy/Signal_SaO2_OX_'+lookingSection+\"/\"+patient)\n",
    "    if file_exist == True and patient not in aliginingwithArousal:\n",
    "        file_nn.append(patient)\n",
    "        label_nn.append(int(label_n[i]))\n",
    "\n",
    "\n",
    "\n",
    "D_len = {}\n",
    "tempp = []\n",
    "for counter, patient in enumerate(file_nn, 0):\n",
    "    p = []\n",
    "    temp = np.load('D:/Directory_D_Desk_jup/Raw_npy/Signal_SaO2_OX_'+lookingSection+\"/\"+patient)\n",
    "    D_len[patient[:-7]] = len(temp)//30\n",
    "    tempp.append(len(temp)//30)\n",
    "np.cumsum(tempp)\n",
    "\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "df = pd.read_csv(\"C:/Features/time_6_\"+lookingSection+\"_segmented/htnderv_s2_nnn.csv\", usecols=[\"a\",\"b\"])\n",
    "df = df.dropna(axis=0)\n",
    "# ## shhs1-200001\n",
    "file_n = []\n",
    "label_n = []\n",
    "indexconsider = []\n",
    "cnt = 0\n",
    "for i,j in zip(df[\"a\"],df[\"b\"]):\n",
    "    if i[:12]+\"_ds.npy\" not in aliginingwithArousal:\n",
    "        file_n.append(f\"{i}\")\n",
    "        label_n.append(int(j))\n",
    "        indexconsider.append(cnt)\n",
    "    cnt += 1\n",
    "\n",
    "\n",
    "D_len = {}\n",
    "tempp = []\n",
    "for counter, patient in enumerate(file_nn, 0):\n",
    "    p = []\n",
    "    temp = np.load(\"D:/Directory_D_Desk_jup/Raw_npy/Signal_SaO2_OX_\"+lookingSection+\"/\"+patient)\n",
    "    D_len[patient[:-7]] = len(temp)//30\n",
    "    tempp.append(len(temp)//30)\n",
    "\n",
    "CUMSUM = np.cumsum(tempp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5b6d6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    86,    147,    249, ..., 240505, 240621, 240731], dtype=int32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUMSUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53480840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"C:/Features/Patient_Sleep_Duration.csv\", usecols=[\"a\",\"b\"])\n",
    "number_normalized = {}\n",
    "num_arsresp = []\n",
    "label_len = []\n",
    "cnt = 0\n",
    "for i in range(len(df[\"a\"])):\n",
    "    if i%2==0 and df[\"a\"][i]+\"_ds.npy\" in file_nn:\n",
    "#         duration[f'{df[\"a\"][i]}'] = df[\"b\"][i]   \n",
    "        num_arsresp.append(round((D_len[df[\"a\"][i]] / df[\"b\"][i]) * 3600, 3))\n",
    "#         print(cnt)\n",
    "        label_len.append(label_nn[cnt])\n",
    "        cnt += 1\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c6c02d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(num_arsresp).shape\n",
    "\n",
    "X_train = np.expand_dims(np.array(num_arsresp[:1400]),1)\n",
    "y_train = label_len[:1400]\n",
    "\n",
    "X_test = np.expand_dims(np.array(num_arsresp[1400:]),1)\n",
    "y_test = label_len[1400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a17e1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================\n",
      "The best parameters are {'C': 0.1, 'gamma': 0.1}\n",
      "=============================================================================\n",
      "The best parameters are {'C': 0.1, 'gamma': 0.1}\n",
      "acc on training 51.57000000000001 %\n",
      "acc on testing 51.03 %\n",
      "0.49785714285714283 0.5161290322580645\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "C_range = np.array([.1, 1, 10])\n",
    "gamma_range = np.array([.1, 1, 10])\n",
    "\n",
    "   \n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "cv = StratifiedShuffleSplit(n_splits=10, test_size=0.3, random_state=10)\n",
    "grid = GridSearchCV(SVC(kernel='rbf'), n_jobs = 3, param_grid = param_grid, cv = cv)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"=============================================================================\")\n",
    "# print(\"The best parameters are %s with a score of %0.2f\"% (grid.best_params_, grid.best_score_))\n",
    "print(\"The best parameters are %s\"% (grid.best_params_))\n",
    "\n",
    "\n",
    "clf = SVC(kernel='rbf', C=grid.best_params_[\"C\"], gamma=grid.best_params_[\"gamma\"])\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"=============================================================================\")\n",
    "# print(\"The best parameters are %s with a score of %0.2f\"% (grid.best_params_, grid.best_score_))\n",
    "print(\"The best parameters are %s\"% (grid.best_params_))\n",
    "\n",
    "\n",
    "clf = SVC(kernel='rbf', C=grid.best_params_[\"C\"], gamma=grid.best_params_[\"gamma\"])\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"acc on training\", round(sum(clf.predict(X_train) == y_train)/len(y_train), 4)*100, \"%\"  )\n",
    "print(\"acc on testing\", round(sum(clf.predict(X_test) == y_test)/len(y_test), 4)*100  ,\"%\")\n",
    "print(sum(y_train)/len(y_train), sum(y_test)/len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34ab53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0586de3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4e51bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "name = \"74_feature_Arousal_normed\"\n",
    "start = 48\n",
    "end = 53\n",
    "X_trr, y_trr, X_tee, y_tee = [], [], [], []\n",
    "X = np.load(\"C:/Features/\"+name+\".npy\")\n",
    "\n",
    "print(X.shape)\n",
    "X = X[:,start:end]\n",
    "print(X.shape)\n",
    "scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "###################################################################################################\n",
    "# lookingSection = \"Respiratory\"\n",
    "lookingSection = \"Arousal\"\n",
    "lookingSection\n",
    "\n",
    "aliginingwithArousal = [\n",
    " \"shhs1-200094_ds.npy\"\n",
    ",\"shhs1-200245_ds.npy\"\n",
    ",\"shhs1-200291_ds.npy\"\n",
    ",\"shhs1-200312_ds.npy\"\n",
    ",\"shhs1-200387_ds.npy\"\n",
    ",\"shhs1-200860_ds.npy\"\n",
    ",\"shhs1-200641_ds.npy\"\n",
    ",\"shhs1-200668_ds.npy\"\n",
    ",\"shhs1-200692_ds.npy\"\n",
    ",\"shhs1-200759_ds.npy\"\n",
    ",\"shhs1-201234_ds.npy\"\n",
    ",\"shhs1-201277_ds.npy\"\n",
    ",\"shhs1-201284_ds.npy\"\n",
    ",\"shhs1-201343_ds.npy\"\n",
    ",\"shhs1-201199_ds.npy\"\n",
    ",\"shhs1-201565_ds.npy\"\n",
    ",\"shhs1-201632_ds.npy\"\n",
    ",\"shhs1-201852_ds.npy\"\n",
    ",\"shhs1-201678_ds.npy\"\n",
    ",\"shhs1-201716_ds.npy\"\n",
    ",\"shhs1-201756_ds.npy\"\n",
    ",\"shhs1-202055_ds.npy\"\n",
    ",\"shhs1-202310_ds.npy\"\n",
    ",\"shhs1-202393_ds.npy\"\n",
    ",\"shhs1-202152_ds.npy\"\n",
    ",\"shhs1-202438_ds.npy\"\n",
    ",\"shhs1-202559_ds.npy\"\n",
    ",\"shhs1-202405_ds.npy\"\n",
    ",\"shhs1-202503_ds.npy\"\n",
    ",\"shhs1-202712_ds.npy\"\n",
    ",\"shhs1-202558_ds.npy\"\n",
    ",\"shhs1-202622_ds.npy\"\n",
    ",\"shhs1-202763_ds.npy\"\n",
    ",\"shhs1-202717_ds.npy\"\n",
    ",\"shhs1-202849_ds.npy\"\n",
    ",\"shhs1-202916_ds.npy\"\n",
    ",\"shhs1-202986_ds.npy\"\n",
    ",\"shhs1-203001_ds.npy\"\n",
    ",\"shhs1-203089_ds.npy\"\n",
    ",\"shhs1-203115_ds.npy\"\n",
    ",\"shhs1-203139_ds.npy\"\n",
    ",\"shhs1-203196_ds.npy\"\n",
    ",\"shhs1-203307_ds.npy\"\n",
    ",\"shhs1-203355_ds.npy\"\n",
    ",\"shhs1-203521_ds.npy\"\n",
    ",\"shhs1-203418_ds.npy\"\n",
    ",\"shhs1-203571_ds.npy\"\n",
    ",\"shhs1-203432_ds.npy\"\n",
    ",\"shhs1-203645_ds.npy\"\n",
    ",\"shhs1-203677_ds.npy\"\n",
    ",\"shhs1-203567_ds.npy\"\n",
    ",\"shhs1-203684_ds.npy\"\n",
    ",\"shhs1-203589_ds.npy\"\n",
    ",\"shhs1-203592_ds.npy\"\n",
    ",\"shhs1-203763_ds.npy\"\n",
    ",\"shhs1-203705_ds.npy\"\n",
    ",\"shhs1-203766_ds.npy\"\n",
    ",\"shhs1-203846_ds.npy\"\n",
    ",\"shhs1-203864_ds.npy\"\n",
    ",\"shhs1-203827_ds.npy\"\n",
    ",\"shhs1-204014_ds.npy\"\n",
    ",\"shhs1-204039_ds.npy\"\n",
    ",\"shhs1-204065_ds.npy\"\n",
    ",\"shhs1-204097_ds.npy\"\n",
    ",\"shhs1-204079_ds.npy\"\n",
    ",\"shhs1-204089_ds.npy\"\n",
    ",\"shhs1-204144_ds.npy\"\n",
    ",\"shhs1-204236_ds.npy\"\n",
    ",\"shhs1-204269_ds.npy\"\n",
    ",\"shhs1-204268_ds.npy\"\n",
    "                      ]\n",
    "\n",
    "import os\n",
    "# import pyedflib\n",
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import scipy.signal \n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "import scipy.interpolate\n",
    "\n",
    "import pandas as pd\n",
    "col_list = [\"a\"]\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/Bpradsad/Desktop/data/SaO2/norm_sparkremoverAVG_full50_STFT_n/htnderv_s2_n.csv\", usecols=[\"a\",\"b\"])\n",
    "## shhs1-200001\n",
    "file_n = []\n",
    "for i in df[\"a\"]:\n",
    "    file_n.append(f\"{i}\")\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/Bpradsad/Desktop/data/SaO2/norm_sparkremoverAVG_full50_STFT_n/htnderv_s2_n.csv\", usecols=[\"a\",\"b\"])\n",
    "## shhs1-200001\n",
    "label_n = []\n",
    "for i in df[\"b\"]:\n",
    "    label_n.append(f\"{i}\")\n",
    "\n",
    "import os.path\n",
    "file_nn = []\n",
    "label_nn = []\n",
    "\n",
    "for i, patient in enumerate(file_n):\n",
    "    file_exist = os.path.exists('D:/Directory_D_Desk_jup/Raw_npy/Signal_SaO2_OX_'+lookingSection+\"/\"+patient)\n",
    "    if file_exist == True and patient not in aliginingwithArousal:\n",
    "        file_nn.append(patient)\n",
    "        label_nn.append(int(label_n[i]))\n",
    "\n",
    "\n",
    "\n",
    "D_len = {}\n",
    "tempp = []\n",
    "for counter, patient in enumerate(file_nn, 0):\n",
    "    p = []\n",
    "    temp = np.load('D:/Directory_D_Desk_jup/Raw_npy/Signal_SaO2_OX_'+lookingSection+\"/\"+patient)\n",
    "    D_len[patient[:-7]] = len(temp)//30\n",
    "    tempp.append(len(temp)//30)\n",
    "np.cumsum(tempp)\n",
    "\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "df = pd.read_csv(\"C:/Features/time_6_\"+lookingSection+\"_segmented/htnderv_s2_nnn.csv\", usecols=[\"a\",\"b\"])\n",
    "df = df.dropna(axis=0)\n",
    "# ## shhs1-200001\n",
    "file_n = []\n",
    "label_n = []\n",
    "indexconsider = []\n",
    "cnt = 0\n",
    "for i,j in zip(df[\"a\"],df[\"b\"]):\n",
    "    if i[:12]+\"_ds.npy\" not in aliginingwithArousal:\n",
    "        file_n.append(f\"{i}\")\n",
    "        label_n.append(int(j))\n",
    "        indexconsider.append(cnt)\n",
    "    cnt += 1\n",
    "\n",
    "\n",
    "D_len = {}\n",
    "tempp = []\n",
    "for counter, patient in enumerate(file_nn, 0):\n",
    "    p = []\n",
    "    temp = np.load(\"D:/Directory_D_Desk_jup/Raw_npy/Signal_SaO2_OX_\"+lookingSection+\"/\"+patient)\n",
    "    D_len[patient[:-7]] = len(temp)//30\n",
    "    tempp.append(len(temp)//30)\n",
    "\n",
    "CUMSUM = np.cumsum(tempp)\n",
    "\n",
    "X_train, X_test, y_train, y_test = [], [], [], []\n",
    "X_train = X[CUMSUM[338]:, :]\n",
    "X_test = X[:CUMSUM[338], :]\n",
    "\n",
    "for i, patient in enumerate(file_n):\n",
    "    if i < CUMSUM[338]:\n",
    "        y_test.append(label_n[i])\n",
    "    else:\n",
    "        y_train.append(label_n[i])\n",
    "\n",
    "X_tr  = X_train   \n",
    "X_te = X_test \n",
    "y_tr = y_train \n",
    "y_te = y_test\n",
    "\n",
    "\n",
    "print(time.time() - t1)\n",
    "\n",
    "# CUMSUM_train = CUMSUM[:338]\n",
    "CUMSUM_new = CUMSUM - CUMSUM[338]\n",
    "CUMSUM_new = CUMSUM_new[338:]\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "\n",
    "SVM = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "SVM.fit(X_tr, y_tr)\n",
    "\n",
    "\n",
    "\n",
    "filename = name +\"_SVMf_\"+f\"{start}\"+\"_\"+f\"{end}\"\n",
    "pickle.dump(SVM, open(filename, 'wb'))\n",
    "\n",
    "# Predict on dataset which model has not seen before\n",
    "predict_res = SVM.predict(X_te)\n",
    "print(\"acc\", sum(predict_res == y_te)/len(y_te))\n",
    "# predict_res_train = SVM.predict(X_tr)\n",
    "# print(\"predict_res_train\", predict_res_train)\n",
    "print(\"predict_res\", predict_res)\n",
    "\n",
    "# predict_proba = SVM.predict_proba(X_te)\n",
    "#     print((time.time() - t1)/60)\n",
    "\n",
    "\n",
    "darsadofONE = sum(y_test)/len(y_test)\n",
    "print(time.time() - t1)\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "def Confusion_pic(actual,y_pred_test, Thereshold, Acc, darsadofONE):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc,roc_auc_score\n",
    "\n",
    "    # cf1 = confusion_matrix(y_train, y_pred_train)\n",
    "    \n",
    "    cf1 = confusion_matrix(actual, y_pred_test)\n",
    "    import seaborn as sns\n",
    "\n",
    "    ax = sns.heatmap(cf1, annot=True, cmap='Blues')\n",
    "\n",
    "    ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "    ax.set_xlabel('\\nPredicted Values')\n",
    "    ax.set_ylabel('Actual Values ');\n",
    "\n",
    "    ## Ticket labels - List must be in alphabetical order\n",
    "    ax.xaxis.set_ticklabels(['False','True'])\n",
    "    ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "    ## Display the visualization of the Confusion Matrix.\n",
    "    plt.show()\n",
    "    #cf2\n",
    "#     ax = sns.heatmap(cf2, annot=True, cmap='Blues')\n",
    "\n",
    "#     ax.set_title('Confusion Matrix \\n'+\"#\"+f\"{Thereshold}\"+\"_\"+\"Acc:\"+f\"{round(Acc, 2)}\"+\"percent_1\"+\"__\"+f\"{round(darsadofONE, 2)}\");\n",
    "#     ax.set_xlabel('\\nPredicted Values')\n",
    "#     ax.set_ylabel('Actual Values');\n",
    "\n",
    "#     ## Ticket labels - List must be in alphabetical order\n",
    "#     ax.xaxis.set_ticklabels(['False','True'])\n",
    "#     ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "#     ## Display the visualization of the Confusion Matrix.\n",
    "#     plt.savefig(\"C:/Features/figsonlyfeatures59_/\"+lookingSection+\"_confusion/\"+\"#\"+f\"{Thereshold}\"+\"__\"+\"Acc\"+f\"{round(Acc, 2)}\"+\"percent_1\"+\"__\"+f\"{round(darsadofONE, 2)}\"+\".png\")\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(2):\n",
    "        fpr[i], tpr[i], _ = roc_curve(actual, y_pred_test)\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    print(\"Accuracy\", roc_auc_score(actual, y_pred_test))\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[1], tpr[1])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Classification')\n",
    "    plt.savefig(\"C:/Features/figs_/\"+\"#\"+f\"{Thereshold}\"+\"auc\"+\".png\")\n",
    "    plt.show()\n",
    "###########################################################################################################################################################################\n",
    "\n",
    "#     if Acc_test >= best_acc:\n",
    "#         best_acc = Acc_test\n",
    "#         Confusion_pic(actual, y_pred_test, Thereshold, Acc, darsadofONE)\n",
    "\n",
    "best_acc = 0\n",
    "darsadofONE = sum(y_test)/len(y_test)\n",
    "\n",
    "for ther in np.arange(1,300):\n",
    "    Thereshold = ther\n",
    "    y_pred_test = []\n",
    "    y_pred_train = []\n",
    "    actual_test = []\n",
    "    actual_train = []\n",
    "    \n",
    "    NN=200\n",
    "    for i in range(0, len(CUMSUM[:338])-1):\n",
    " \n",
    "        if sum(predict_res[CUMSUM[i]:CUMSUM[i+1]]) >= ther:\n",
    "            predicted_label = 1\n",
    "        else:\n",
    "            predicted_label = 0\n",
    "\n",
    "        actual_label = int(sum(y_test[CUMSUM[i]:CUMSUM[i+1]]) /len(y_test[CUMSUM[i]:CUMSUM[i+1]]))\n",
    "        y_pred_test.append(predicted_label)\n",
    "        actual_test.append(actual_label)\n",
    "\n",
    "        \n",
    "#     for i in range(len(CUMSUM[:1400])-2):\n",
    "#         if sum(predict_res_train[CUMSUM[i]:CUMSUM[i+1]]) >= Thereshold:\n",
    "#             predicted_label = 1\n",
    "#         else:\n",
    "#             predicted_label = 0\n",
    "\n",
    "#         actual_label = int(sum(y_train[CUMSUM[i]:CUMSUM[i+1]]) /len(y_train[CUMSUM[i]:CUMSUM[i+1]]))\n",
    "\n",
    "#         y_pred_train.append(predicted_label)\n",
    "#         actual_train.append(actual_label) \n",
    "        \n",
    "        \n",
    "\n",
    "#         Confusion_pic(actual,y_pred_test, Thereshold, Acc, darsadofONE)\n",
    "        \n",
    "\n",
    "    Acc_test = sum(np.array(y_pred_test) == np.array(actual_test)) / len(actual_test)\n",
    "#     Acc_train = sum(np.array(y_pred_train) == np.array(actual_train)) / len(actual_train)\n",
    "\n",
    "    print(\">>>\", Acc_test, best_acc)\n",
    "    if Acc_test >= best_acc:\n",
    "        best_acc = Acc_test\n",
    "        Confusion_pic(actual_test, y_pred_test, Thereshold, Acc_test, darsadofONE)\n",
    "        print(\"Acc_test\", Acc_test, \"ther\", ther)\n",
    "        ttt = ther\n",
    "        y_final_best = y_pred_test\n",
    "    \n",
    "\n",
    "np.save(\"C:/Features/SVM_SUBFEATURE_RES/NewHalf/\"+name+\"__\"+f\"{start}\"+\"_\"+f\"{end}\"+\"_\"+f\"{ttt}\", y_final_best)\n",
    "print(\"time\", time.time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361e6dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(actual_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
